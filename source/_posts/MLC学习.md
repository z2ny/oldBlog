---
title: MLCå­¦ä¹ 
date: 2023-09-21 16:30:11
categories:
- work
tags:
- MLC
- TVM
- è¯¾ç¨‹ç¬”è®°
---

é™ˆå¤©å¥‡çš„MLCè¯¾ç¨‹ï¼Œå‚è€ƒ
https://github.com/BBuf/tvm_mlir_learn
https://mlc.ai/zh/index.html
https://space.bilibili.com/1663273796/channel/collectiondetail?sid=499979
<!-- more -->

## 1.æ¦‚è¿°

å®šä¹‰ï¼šå°†æœºå™¨å­¦ä¹ çš„ç®—æ³•ï¼ˆæ¨¡å‹ï¼‰ä»å¼€å‘å½¢å¼ï¼ˆå¦‚pytorchã€tfç­‰é€šç”¨æ¡†æ¶ç¼–å†™çš„æ¨¡å‹æè¿°ä»¥åŠç›¸å…³æƒé‡ï¼‰ï¼Œé€šè¿‡å˜æ¢å’Œä¼˜åŒ–ï¼Œè½¬åŒ–ä¸ºéƒ¨ç½²å½¢å¼ï¼ˆå¦‚æ¨¡å‹æ”¯æ’‘ä»£ç ã€å†…å­˜æ§åˆ¶ã€æ¥å£ç­‰ï¼‰
å³ï¼Œå°†ç¥ç»ç½‘ç»œæ¨¡å‹è½¬å˜æˆåœ¨ç‰¹å®šç¡¬ä»¶ä¸Šè¿è¡Œçš„å¼ é‡å‡½æ•°ä»£ç 

æœºå™¨å­¦ä¹ ç¼–è¯‘ç›®æ ‡ï¼š
1. é›†æˆå’Œæœ€å°åŒ–ä¾èµ–
2. åˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿï¼šåˆ©ç”¨åˆ°æ¯ä¸ªéƒ¨ç½²ç¯å¢ƒçš„åŸç”ŸåŠ é€ŸæŠ€æœ¯
3. é€šç”¨ä¼˜åŒ–

## 2. å¼ é‡ç¨‹åºæŠ½è±¡

å…ƒå¼ é‡å‡½æ•°ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹æ‰§è¡Œä¸­çš„æ¯ä¸€ä¸ªæ­¥éª¤ï¼ˆæˆ–è€…è¯´ç®—å­ï¼Ÿï¼‰ï¼Œå¦‚linearã€reluã€softmax

è®¸å¤šä¸åŒçš„æŠ½è±¡å¯ä»¥è¡¨è¾¾åŒä¸€ç§å…ƒå¼ é‡å‡½æ•°ï¼Œå¦‚torch.addå’Œnumpy.addï¼ŒåŒæ—¶ï¼Œæœ‰äº›æœºå™¨å­¦ä¹ æ¡†æ¶ä¹Ÿæä¾›æ¨¡å‹çš„ç¼–è¯‘è¿‡ç¨‹ä¼˜åŒ–ï¼Œå°†å…ƒå¼ é‡å‡½æ•°è½¬å˜æˆæ›´ä¸“é—¨çš„ã€é’ˆå¯¹æ€§çš„å‡½æ•°

å¼ é‡ç¨‹åºæŠ½è±¡ï¼šä¸€ä¸ªå…¸å‹çš„å…ƒå¼ é‡å‡½æ•°å®ç°åŒ…æ‹¬ï¼š
1. å­˜å‚¨æ•°æ®çš„å¤šç»´æ•°ç»„
2. é©±åŠ¨å¼ é‡è®¡ç®—çš„å¾ªç¯åµŒå¥—
3. è®¡ç®—è¯­å¥

æ ¹æ®æŠ½è±¡å‡ºæ¥çš„å…±åŒç‰¹å¾ï¼Œå…ƒå¼ é‡å‡½æ•°å› æ­¤å¯ä»¥è¢«ä¸€ç³»åˆ—æœ‰æ•ˆçš„ç¨‹åºå˜æ¢æ‰€æ”¹å˜ï¼Œå³ä¼˜åŒ–ã€‚
ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„å¤§éƒ¨åˆ†å…ƒå¼ é‡å‡½æ•°éƒ½å…·æœ‰è‰¯å¥½çš„å¯å˜æ¢å±æ€§ã€‚

### TensorIRï¼šTVMä½¿ç”¨çš„å¼ é‡ç¨‹åºæŠ½è±¡
å‰æï¼šå¤§å¤šæ•°çš„æœºå™¨å­¦ä¹ ç¼–è¯‘å¯ä»¥è§†ä¸ºå¼ é‡å‡½æ•°ä¹‹é—´çš„å˜æ¢

#### ç¤ºä¾‹ï¼šä¸€ä¸ªç»å…¸çš„ç‚¹ç§¯ + relu ç½‘ç»œ
```python
dtype = "float32"
a_np = np.random.rand(128, 128).astype(dtype)
b_np = np.random.rand(128, 128).astype(dtype)
c_mm_relu = np.maximum(a_np @ b_np, 0)
```

åœ¨åº•å±‚ï¼Œnumpyå¯èƒ½ä½¿ç”¨å¾ªç¯å’Œç®—æœ¯è¿ç®—å®ç°ä¸Šè¿°æ“ä½œï¼š
```python
def lnumpy_mm_relu(A: np.ndarray, B: np.ndarray, C: np.ndarray):
    # å­˜å‚¨æ•°æ®çš„å¤šç»´æ•°ç»„
    Y = np.empty((128, 128), dtype="float32")
    # é©±åŠ¨å¼ é‡è®¡ç®—çš„å¾ªç¯åµŒå¥—
    for i in range(128):
        for j in range(128):
            for k in range(128):
                if k == 0:
                    Y[i, j] = 0
                # è®¡ç®—è¯­å¥
                Y[i, j] = Y[i, j] + A[i, k] * B[k, j]
    # é©±åŠ¨å¼ é‡è®¡ç®—çš„å¾ªç¯åµŒå¥—
    for i in range(128):
        for j in range(128):
            # è®¡ç®—è¯­å¥
            C[i, j] = max(Y[i, j], 0)
```

#### TensorIRå®ç°ï¼š
```python
@tvm.script.ir_module
class MyModule:
    @T.prim_func
    def mm_relu(A: T.Buffer((128, 128), "float32"),
                B: T.Buffer((128, 128), "float32"),
                C: T.Buffer((128, 128), "float32")):
        T.func_attr({"global_symbol": "mm_relu", "tir.noalias": True})
        # å­˜å‚¨æ•°æ®çš„å¤šç»´æ•°ç»„ï¼ˆç¼“å†²åŒºï¼‰
        Y = T.alloc_buffer((128, 128), dtype="float32")
        # é©±åŠ¨å¼ é‡è®¡ç®—çš„å¾ªç¯åµŒå¥—
        for i, j, k in T.grid(128, 128, 128):
            # è®¡ç®—è¯­å¥
            with T.block("Y"):
                vi = T.axis.spatial(128, i)
                vj = T.axis.spatial(128, j)
                vk = T.axis.reduce(128, k)
                with T.init():
                    Y[vi, vj] = T.float32(0)
                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]
        # é©±åŠ¨å¼ é‡è®¡ç®—çš„å¾ªç¯åµŒå¥—
        for i, j in T.grid(128, 128):
            # è®¡ç®—è¯­å¥
            with T.block("C"):
                vi = T.axis.spatial(128, i)
                vj = T.axis.spatial(128, j)
                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))
```

å—æ˜¯tensorIRçš„åŸºæœ¬è®¡ç®—å•ä½ã€‚å®šä¹‰å¦‚ä¸‹ï¼š
```python
[block_axis] = T.axis.[axis_type]([axis_range], [mapped_value])
```
å¦‚` vi = T.axis.spatial(128, i) ` å³è¡¨ç¤ºviä¸ºiçš„æ˜ å°„ï¼ŒèŒƒå›´ä¸º(0,128)ï¼Œä¸”è¯¥å—è½´å±æ€§ä¸ºspatialï¼ˆç©ºé—´è½´ï¼‰ï¼Œè€Œvkçš„å±æ€§åˆ™ä¸ºreduceè§„çº¦è½´ã€‚ï¼ˆå¯ä»¥ç†è§£ä¸ºç©ºé—´è½´æ˜¯åŸæœ¬å°±åœ¨çš„ï¼Œè§„çº¦è½´æ˜¯åœ¨ä¸Šé¢åšæ»‘åŠ¨çš„ï¼‰

å—è½´åŠ å±æ€§çš„å¥½å¤„æ˜¯ä½¿å¾—viï¼Œvjï¼Œvkç‹¬ç«‹äºå¤–éƒ¨çš„å¾ªç¯åµŒå¥—iï¼Œjï¼Œkï¼ŒåŒæ—¶ä¹Ÿå¯¹å¤–éƒ¨å¾ªç¯æ­£ç¡®æ€§åšäº†äºŒæ¬¡éªŒè¯ã€‚åŒæ—¶è¿™äº›é™„åŠ ä¿¡æ¯ä¹Ÿæœ‰åŠ©äºæœºå™¨å­¦ä¹ ç¼–è¯‘åˆ†æï¼Œæ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬æ€»æ˜¯å¯ä»¥åœ¨ç©ºé—´è½´ä¸Šåšå¹¶è¡ŒåŒ–ï¼Œä½†åœ¨è§„çº¦è½´ä¸Šåšå¹¶è¡ŒåŒ–åˆ™éœ€è¦ç‰¹å®šçš„ç­–ç•¥

    å¦‚æœè§‰å¾—è‡ªå®šä¹‰å±æ€§æ¯”è¾ƒéº»çƒ¦ä¹Ÿå¯ä»¥ä¸€é”®ç»‘å®š
```python
# SSR means the properties of each axes are "spatial", "spatial", "reduce"
vi, vj, vk = T.axis.remap("SSR", [i, j, k])
```

#### tensorIRçš„å…ƒå¼ é‡å‡½æ•°å˜æ¢

tensorIRå¼•å…¥äº†åä¸ºScheduleçš„è¾…åŠ©ç»“æ„ï¼Œå…è®¸æˆ‘ä»¬è¿›è¡Œæ–¹ä¾¿çš„å…ƒå¼ é‡å‡½æ•°å˜æ¢

è¿™æ˜¯åŸæ¥çš„ï¼š
```python
import IPython
IPython.display.Code(MyModule.script(), language="python")

# from tvm.script import ir as I
# from tvm.script import tir as T
@I.ir_module
class Module:
    @T.prim_func
    def mm_relu(A: T.Buffer((128, 128), "float32"), B: T.Buffer((128, 128), "float32"), C: T.Buffer((128, 128), "float32")):
        T.func_attr({"global_symbol": "mm_relu", "tir.noalias": True})
        # with T.block("root"):
        Y = T.alloc_buffer((128, 128))
        for i, j, k in T.grid(128, 128, 128):
            with T.block("Y"):
                vi, vj, vk = T.axis.remap("SSR", [i, j, k])
                T.reads(A[vi, vk], B[vk, vj])
                T.writes(Y[vi, vj])
                with T.init():
                    Y[vi, vj] = T.float32(0)
                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]
        for i, j in T.grid(128, 128):
            with T.block("C"):
                vi, vj = T.axis.remap("SS", [i, j])
                T.reads(Y[vi, vj])
                T.writes(C[vi, vj])
                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))
```

ä½¿ç”¨Scheduleè¿›è¡Œå˜æ¢ï¼š
```python
## ä»¥ç»™å®šçš„moduleä½œä¸ºè¾“å…¥çš„è¾…åŠ©Scheduleç±»
sch = tvm.tir.Schedule(MyModule)
# è·å–å¯¹åº”çš„å—åŠç›¸åº”å¾ªç¯çš„å¼•ç”¨
block_Y = sch.get_block("Y", func_name="mm_relu")
i, j, k = sch.get_loops(block_Y)
# å˜æ¢ï¼šå°†åŸæœ‰çš„jå¾ªç¯æ‹†åˆ†æˆä¸¤ä¸ªå¾ªç¯ï¼ˆ4è¡¨ç¤ºå†…éƒ¨å¾ªç¯é•¿åº¦ï¼‰
j0, j1 = sch.split(j, factors=[None, 4])
# å†æ¬¡æ£€æŸ¥ç»“æœ
IPython.display.Code(sch.mod.script(), language="python")

@I.ir_module
class Module:
    @T.prim_func
    def mm_relu(A: T.Buffer((128, 128), "float32"), B: T.Buffer((128, 128), "float32"), C: T.Buffer((128, 128), "float32")):
        T.func_attr({"global_symbol": "mm_relu", "tir.noalias": True})
        # with T.block("root"):
        Y = T.alloc_buffer((128, 128))
        for i, j_0, j_1, k in T.grid(128, 32, 4, 128):
            with T.block("Y"):
                vi = T.axis.spatial(128, i)
                vj = T.axis.spatial(128, j_0 * 4 + j_1)
                vk = T.axis.reduce(128, k)
                T.reads(A[vi, vk], B[vk, vj])
                T.writes(Y[vi, vj])
                with T.init():
                    Y[vi, vj] = T.float32(0)
                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]
        for i, j in T.grid(128, 128):
            with T.block("C"):
                vi, vj = T.axis.remap("SS", [i, j])
                T.reads(Y[vi, vj])
                T.writes(C[vi, vj])
                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))

# è¿˜å¯ä»¥æ›´æ¢å¾ªç¯æ¬¡åº
# sch.reorder(j0, k, j1)
```

æ­¤å¤–ï¼Œå—ä¹‹é—´ä¹Ÿå¯ä»¥é€šè¿‡å˜æ¢å®Œæˆç»„åˆ

```python
# å°†å—Cæ”¾åˆ°Yçš„å†…å¾ªç¯ä¸­
block_C = sch.get_block("C", "mm_relu")
# æ„Ÿè§‰æ„æ€æ˜¯å°†å—Cä¸j0å¾ªç¯ç»‘å®šï¼ŒåŠj0è¿™ä¸ªç©ºé—´è½´å˜æ¢æ—¶ï¼ŒåŸæœ¬åªæœ‰Yæœ‰åŠ¨ä½œï¼Œç°åœ¨Cä¹Ÿæœ‰åŠ¨ä½œ
sch.reverse_compute_at(block_C, j0)
IPython.display.Code(sch.mod.script(), language="python")

@I.ir_module
class Module:
    @T.prim_func
    def mm_relu(A: T.Buffer((128, 128), "float32"), B: T.Buffer((128, 128), "float32"), C: T.Buffer((128, 128), "float32")):
        T.func_attr({"global_symbol": "mm_relu", "tir.noalias": True})
        # with T.block("root"):
        Y = T.alloc_buffer((128, 128))
        for i, j_0 in T.grid(128, 32):
            for k, j_1 in T.grid(128, 4):
                with T.block("Y"):
                    vi = T.axis.spatial(128, i)
                    vj = T.axis.spatial(128, j_0 * 4 + j_1)
                    vk = T.axis.reduce(128, k)
                    T.reads(A[vi, vk], B[vk, vj])
                    T.writes(Y[vi, vj])
                    with T.init():
                        Y[vi, vj] = T.float32(0)
                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]
            for ax0 in range(4):
                with T.block("C"):
                    vi = T.axis.spatial(128, i)
                    # æ³¨æ„è¿™é‡Œvjçš„å˜åŒ–ï¼ŒåŸæœ¬vj = j = j_0 * 4 + j_1ï¼Œç°åœ¨å˜æˆäº†j_0 * 4 + ax0
                    # æ„Ÿè§‰æ˜¯å› ä¸ºä¸Šé¢ reverse_compute_at åªæ˜¯å°†Cä¸j0ç»‘å®šï¼Œæ‰€ä»¥j_1è¿™ä¸ªå¾ªç¯è¿˜æ˜¯åœ¨Yä¸­ï¼ŒCé‡Œè¿˜éœ€è¦å•ç‹¬å¾ªç¯ax0
                    vj = T.axis.spatial(128, j_0 * 4 + ax0)
                    T.reads(Y[vi, vj])
                    T.writes(C[vi, vj])
                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))

```

æ­¤å¤–è¿˜ä»‹ç»äº†å¦ä¸€ç§åŸè¯­decompose_reductionï¼Œç”¨äºå°†è¯­å—ä¸­å…ƒç´ çš„åˆå§‹åŒ–ä¸è§„çº¦æ›´æ–°åˆ†å¼€ï¼š
è¿™ä¹Ÿæ˜¯ TVM åœ¨ä»¥åç¼–è¯‘çš„æ—¶å€™éšå¼åšçš„ï¼Œæ‰€ä»¥è¿™ä¸€æ­¥çš„ä¸»è¦ç›®çš„æ˜¯è®©å®ƒæ˜¾å¼ï¼Œçœ‹çœ‹æœ€ç»ˆæ•ˆæœ
```python
# å°†å—Yä¸­çš„åˆå§‹åŒ–ä¸å¾ªç¯kæ— å…³(kæ˜¯è§„çº¦è½´)
sch.decompose_reduction(block_Y, k)
IPython.display.Code(sch.mod.script(), language="python")

def lnumpy_mm_relu_v3(A: np.ndarray, B: np.ndarray, C: np.ndarray):
    Y = np.empty((128, 128), dtype="float32")
    for i in range(128):
        for j0 in range(32):
            # Y_init
            for j1 in range(4):
                j = j0 * 4 + j1
                # æ­¤æ—¶åˆå§‹åŒ–åœ¨kå¾ªç¯ä¹‹å‰å°±å·²ç»åšå¥½
                Y[i, j] = 0
            # Y_update
            for k in range(128):
                for j1 in range(4):
                    j = j0 * 4 + j1
                    Y[i, j] = Y[i, j] + A[i, k] * B[k, j]
            # C
            for j1 in range(4):
                j = j0 * 4 + j1
                C[i, j] = max(Y[i, j], 0)

c_np = np.empty((128, 128), dtype=dtype)
lnumpy_mm_relu_v3(a_np, b_np, c_np)
np.testing.assert_allclose(c_mm_relu, c_np, rtol=1e-5)
```

#### æ„å»ºä¸è¿è¡Œ

```python
# ä½¿ç”¨llvmå°†æ¨¡å‹ç¼–è¯‘åˆ°æœ¬æœºå¹³å°
rt_lib = tvm.build(MyModule, target="llvm")

# ç”¨äºå­˜å‚¨è¾“å…¥å’Œè¾“å‡ºçš„TVM NDArray
a_nd = tvm.nd.array(a_np)
b_nd = tvm.nd.array(b_np)
c_nd = tvm.nd.empty((128, 128), dtype="float32")

# è°ƒç”¨ç¼–è¯‘å¥½çš„å‡½æ•°
func_mm_relu = rt_lib["mm_relu"]
func_mm_relu(a_nd, b_nd, c_nd)
# å°†TVMä¸numpyçš„ç»“æœè¿›è¡Œæ¯”è¾ƒ
np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=1e-5)

# è°ƒç”¨TVMå˜æ¢åçš„å‡½æ•°ï¼Œç»§ç»­æ¯”è¾ƒ
rt_lib_after = tvm.build(sch.mod, target="llvm")
rt_lib_after["mm_relu"](a_nd, b_nd, c_nd)
np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=1e-5)
```

åœ¨æœ€åçš„ç»“æœä¸­ï¼ŒTVMå˜æ¢åçš„å‡½æ•°è¿è¡Œæ—¶é—´ç›¸æ¯”åŸå…ˆçš„TVMå‡½æ•°å¤§å¹…ç¼©çŸ­ï¼Œä¸ºä»€ä¹ˆä¸åŒçš„å¾ªç¯å˜ä½“ä¼šå¯¼è‡´ä¸åŒçš„æ—¶é—´æ€§èƒ½å‘¢ï¼Ÿ

å…³é”®åœ¨äºCPUçš„è®¿å­˜ç­–ç•¥ï¼Œç”±äºå±€éƒ¨æ€§åŸç†ï¼ŒCPUåœ¨è¯»å–å†…å­˜æŸå…ƒç´ æ—¶ä¼šå°è¯•å°†è¯¥å…ƒç´ é™„è¿‘çš„å…ƒç´ ä¸€èµ·è·å–åˆ°ç¼“å­˜ä¸­ï¼ˆcacheå—ï¼Ÿç‰¹ä¹ˆOSå¿«å¿˜å¹²å‡€äº†ğŸ˜…ï¼‰ã€‚å› æ­¤å…·æœ‰è¿ç»­å†…å­˜è®¿é—®çš„ä»£ç é€šå¸¸æ¯”éšæœºè®¿é—®å†…å­˜ä¸åŒéƒ¨åˆ†çš„ä»£ç æ›´å¿«ã€‚

## 3. ç«¯åˆ°ç«¯çš„æ¨¡å‹æ‰§è¡Œ

ç°åœ¨è€ƒè™‘ä¸€ä¸ªåŸºç¡€çš„ä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œç”±2ä¸ªMLPå’Œ1ä¸ªreluç»„æˆï¼ˆç®€åŒ–é—®é¢˜ï¼Œåˆ é™¤æœ€åçš„softmaxï¼‰

numpyå®ç°ï¼š
```python
def numpy_mlp(data, w0, b0, w1, b1):
    lv0 = data @ w0.T + b0
    lv1 = np.maximum(lv0, 0)
    lv2 = lv1 @ w1.T + b1
    return lv2

res = numpy_mlp(img.reshape(1, 784),
                mlp_params["w0"],
                mlp_params["b0"],
                mlp_params["w1"],
                mlp_params["b1"])
```

åº•å±‚å®ç°ï¼š
```python
def lnumpy_linear0(X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray):
    Y = np.empty((1, 128), dtype="float32")
    for i in range(1):
        for j in range(128):
            for k in range(784):
                if k == 0:
                    Y[i, j] = 0
                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]

    for i in range(1):
        for j in range(128):
            Z[i, j] = Y[i, j] + B[j]


def lnumpy_relu0(X: np.ndarray, Y: np.ndarray):
     for i in range(1):
        for j in range(128):
            Y[i, j] = np.maximum(X[i, j], 0)

def lnumpy_linear1(X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray):
    Y = np.empty((1, 10), dtype="float32")
    for i in range(1):
        for j in range(10):
            for k in range(128):
                if k == 0:
                    Y[i, j] = 0
                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]

    for i in range(1):
        for j in range(10):
            Z[i, j] = Y[i, j] + B[j]


def lnumpy_mlp(data, w0, b0, w1, b1):
    lv0 = np.empty((1, 128), dtype="float32")
    lnumpy_linear0(data, w0, b0, lv0)

    lv1 = np.empty((1, 128), dtype="float32")
    lnumpy_relu0(lv0, lv1)

    out = np.empty((1, 10), dtype="float32")
    lnumpy_linear1(lv1, w1, b1, out)
    return out

result =lnumpy_mlp(
    img.reshape(1, 784),
    mlp_params["w0"],
    mlp_params["b0"],
    mlp_params["w1"],
    mlp_params["b1"])

pred_kind = result.argmax(axis=1)
print("Low-level Numpy MLP Prediction:", class_names[pred_kind[0]])
```

è¯¥æ¨¡å‹çš„TVMScriptå®ç°ï¼š
```python
@tvm.script.ir_module
class MyModule:
    @T.prim_func
    def relu0(x: T.handle, y: T.handle):
        n = T.int64()
        X = T.match_buffer(x, (1, n), "float32")
        Y = T.match_buffer(y, (1, n), "float32")
        for i, j in T.grid(1, n):
            with T.block("Y"):
                vi, vj = T.axis.remap("SS", [i, j])
                Y[vi, vj] = T.max(X[vi, vj], T.float32(0))

    @T.prim_func
    def linear0(x: T.handle,
                w: T.handle,
                b: T.handle,
                z: T.handle):
        m, n, k = T.int64(), T.int64(), T.int64()
        X = T.match_buffer(x, (1, m), "float32")
        W = T.match_buffer(w, (n, m), "float32")
        B = T.match_buffer(b, (n, ), "float32")
        Z = T.match_buffer(z, (1, n), "float32")
        Y = T.alloc_buffer((1, n), "float32")
        for i, j, k in T.grid(1, n, m):
            with T.block("Y"):
                vi, vj, vk = T.axis.remap("SSR", [i, j, k])
                with T.init():
                    Y[vi, vj] = T.float32(0)
                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]
        for i, j in T.grid(1, n):
            with T.block("Z"):
                vi, vj = T.axis.remap("SS", [i, j])
                Z[vi, vj] = Y[vi, vj] + B[vj]

    @R.function
    def main(x: R.Tensor((1, "m"), "float32"),
             w0: R.Tensor(("n", "m"), "float32"),
             b0: R.Tensor(("n", ), "float32"),
             w1: R.Tensor(("k", "n"), "float32"),
             b1: R.Tensor(("k", ), "float32")):
        m, n, k = T.int64(), T.int64(), T.int64()
        with R.dataflow():
            lv0 = R.call_dps_packed("linear0", (x, w0, b0), R.Tensor((1, n), "float32"))
            lv1 = R.call_dps_packed("relu0", (lv0, ), R.Tensor((1, n), "float32"))
            out = R.call_dps_packed("linear0", (lv1, w1, b1), R.Tensor((1, k), "float32"))
            R.output(out)
        return out
```
å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ `@R.function` å³Relexå‡½æ•°ï¼Œæ˜¯ä¸€ç§è¡¨ç¤ºä¸Šå±‚ç¥ç»ç½‘ç»œæ‰§è¡Œçš„å…¨æ–°æŠ½è±¡

![Alt text](image.png)

æ³¨æ„åˆ°ï¼Œå…¶ä¸­`call_dps_packed`å°†æˆ‘ä»¬çš„å…ƒå‡½æ•°åµŒå…¥åˆ°è®¡ç®—å›¾ä¸­ï¼Œå…¶ä¸»è¦ä½œç”¨æ˜¯æ»¡è¶³**ç›®æ ‡ä¼ é€’**çš„è°ƒç”¨çº¦å®šï¼Œå³ pure æˆ– side-effect free ï¼Œå‡½æ•°åªä»å…¶è¾“å…¥ä¸­è¯»å–æ•°æ®å¹¶è¾“å‡ºè¿”å›ç»“æœï¼Œè€Œä¸æ”¹å˜ç¨‹åºçš„å…¶ä»–éƒ¨åˆ†ï¼Œè¿™å¯ä»¥æ–¹ä¾¿æˆ‘ä»¬éšè—è°ƒç”¨åº•å±‚å…ƒå‡½æ•°çš„ç»†èŠ‚

å¦‚æœåªæ˜¯åƒnumpyå®ç°ä¸­é‚£æ ·ï¼š
```python
    lnumpy_linear0(data, w0, b0, lv0)
    lnumpy_relu0(lv0, lv1)
    lnumpy_linear1(lv1, w1, b1, out)
```
è®¡ç®—å›¾å¯èƒ½ä¼šå˜æˆè¿™æ ·ï¼šlv0æ—¢æ˜¯`lnumpy_linear0`çš„å…¥å‚ï¼Œä¹Ÿæ˜¯`lnumpy_relu0`çš„å…¥å‚ï¼Œå…¶ä½™åŒç†
![Alt text](image-1.png)

> è®¡ç®—å›¾é€šå¸¸å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š
> - æ¡†çš„æ¯ä¸ªè¾“å…¥è¾¹å¯¹åº”äºæ“ä½œçš„è¾“å…¥
> - æ¯ä¸ªå‡ºè¾¹å¯¹åº”äºæ“ä½œçš„è¾“å‡º
> - æ¯ä¸ªæ“ä½œå¯ä»¥ä»»æ„é‡æ–°æ’åºï¼Œç›´åˆ°è¾¹ç¼˜çš„æ‹“æ‰‘é¡ºåº

å½“ç„¶ï¼Œnumpyçš„åº•å±‚åŒæ ·ä¹Ÿä½¿ç”¨äº†å¦‚`lnumpy_call_dps_packed`çš„ç±»ä¼¼è°ƒç”¨

æ­¤å¤–ï¼Œæ³¨æ„`with R.dataflow():` æ˜¯ä¸€ä¸ªå¸®åŠ©æˆ‘ä»¬æ ‡æ³¨ç¨‹åºè®¡ç®—å›¾èŒƒå›´çš„æ–¹å¼ï¼Œåé¢çš„æ„å»ºè¿è¡Œå°±ä¸å¤šè¯´äº†

## 4. è‡ªåŠ¨ç¨‹åºä¼˜åŒ–
