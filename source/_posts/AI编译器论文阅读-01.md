---
title: AI编译器论文阅读-01
date: 2023-11-13 14:52:54
categories:
- work
tags:
- AI
- TVM
---

TVM以及autoTVM的原文，一作都是陈天奇。

*TVM: An Automated End-to-End Optimizing Compiler for Deep Learning ——2018*

*Learning to Optimize Tensor Programs ——2019*

<!-- more -->

# *TVM: An Automated End-to-End Optimizing Compiler for Deep Learning*

## Abstract

人工智能的需求越来越高，机器学习的应用场景越来越广blabla

我们提供了一种AI编译器TVM。TVM向外界暴露出图级和算子级的优化工作，以在不同的硬件平台为深度学习工作负载提供性能可移植性。

TVM解决了深度学习特有的优化难题，如算子融合、硬件原语映射、内存延迟隐藏等。他还采用了一种新型的、基于继续学习的成本建模方法来探索自动调优问题。实验表明效果很好，已经有大公司在使用。

## 1. Intro

现有的DL框架如TensorFlow、Pytorch等都是依靠计算图的中间表示来优化，如自动微分和动态内存管理等。但是图级优化过于高级，无法看到和处理特定硬件后端的算子级别优化。大多数后端的优化都是依赖于高度定制化的算子库，当前，在各种DL框架中为各种硬件后端提供支持需要大量的工程努力。

> 即使对于已经支持的后端，框架也必须在一下两种方案之间做出艰难选择：1. 避免产生不在预定义算子库的算子而实现图优化 2. 使用包括新算子但尚未优化的图 （对高速发展的机器学习来说，新算子是不可避免的）

为了对不同的硬件后端启用图级和算子级的优化，作者采用了一种完全不同的端到端的方法。作者构建了 TVM，这是一个编译器，它从现有框架中获取深度学习程序的高级规范，并为各种硬件后端生成低级优化代码。

简单来说，TVM要解决的问题是**适配**。将模型部署到不同的硬件上依赖硬件厂商的定制化库，需要大量的手工优化。而TVM这种端到端的编译器方案解决了不同框架下的不同模型部署到不同硬件平台上的适配问题。

方案的核心包括三大问题：
1. 如何进行高级别的图优化
2. 如何进行硬件/算子级别的优化
3. 如何解决组合搜索空间的问题

