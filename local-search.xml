<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>秋招面经</title>
    <link href="/2024/07/08/%E7%A7%8B%E6%8B%9B%E9%9D%A2%E7%BB%8F/"/>
    <url>/2024/07/08/%E7%A7%8B%E6%8B%9B%E9%9D%A2%E7%BB%8F/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><h2 id="简历描述">简历描述</h2><p>面试官您好，我叫XXX，来自湖北省荆州市。本科毕业于武汉大学计算机系，研究生就读于中国科学技术大学软件学院，预计将于明年六月份毕业。</p><p>在校期间，我获得过一次二等学业奖学金。研一期间，我独立完成了MIT的6.S081课程项目，设计并实现了一个精简的类Unix操作系统中的部分核心功能，包括内存管理、进程管理和文件系统等。</p><p>在研二开始，也就是去年七八月份，我来到燧原科技进行日常实习，主要负责公司推理加速卡的性能优化，至今已有将近一年的时间。这段时间里，我主要做了以下几方面的工作：</p><p>第一，组内的日常性能优化。在收到针对某网络或算子的性能优化需求后，我们会使用组内的autotuner框架处理网络或算子，获得算子级别的切分数据，并进行上卡实跑。通过性能对比，得到网络上各类算子的最佳切分，完成一次调优工作。</p><p>第二，autotuner的重构工作，称为tuner2.0。原先的autotuner流程比较繁琐，展示了过多的末端细节，只适合开发人员调试和使用。我们希望这一系统可以交给用户，甚至直接融入加速卡推理运行的生命周期中。因此，我的工作是对tuner的核心runner模块进行重写，利用多线程的思路提高系统健壮性，并完善异常处理等功能。</p><p>第三，为autotuner引入基于深度学习的启发式搜索模型，这也是我毕业论文的研究方向。原先基于单一网络的自动调优虽然能提供极致的性能优化，但本质上是根据加速卡的各项硬件参数，对算子的每一种可能切分进行实跑，进而找到最佳结果，运行时间较长且应用场景有限。当前的想法是设计一个在线学习模型，在得到足够量级的实跑数据后，对后续进入的算子数据直接推理出性能结果，以节省调优时间。</p><p>以上是我目前的一些工作经历和成果。谢谢！</p><h3 id="autotuner">autotuner</h3><p>我们知道张量运算由于大量的数据并行和数据吞吐，怎么高效地组织张量中各个元素的访存和调度，是机器学习编译器性能提高的重点。<br>TVM这种深度学习编译器的工作流程就是IR进行流转的过程。从Pytorch、Tensorflow、ONNX这类框架中导入模型后，TVM首先会将模型翻译成他自己的一种模型语言RelayIR，经过图级别的优化以及模型切分、降级之后转化成一个个小的张量表达式，也就是TE。对应到我们的autotuning系统中，我们首先会提取出经过多层pass流转处理过后的最后一级IR，我们叫做lastIR，这一层包含了经过图级别优化、降级之后得到的模型全部计算节点信息，然后从里面提取出目前支持tune的算子种类，例如conv、dot等等，对于每个算子，经过我们的cost model计算出性能最佳的调度参数，对应到我们的系统里，目前还是使用的基于实际硬件的黑盒模型，也就是将调度参数上机实跑得到运行时间这种性能数据</p><h2 id="招聘情况">招聘情况</h2><h3 id="科大讯飞-提前批（飞星计划）-AI研究算法工程师-深度学习框架和平台方向">科大讯飞-提前批（飞星计划）-AI研究算法工程师-深度学习框架和平台方向</h3><p><img src="image1.png" alt="alt text"></p><h4 id="0628投递">0628投递</h4><h4 id="0708技术一面-40min">0708技术一面 40min</h4><p>虚函数，静态多态和动态多态，内存分页机制，LRU</p><p>怎么排查内存泄漏</p><p>快排复杂度，为什么最坏是n^2</p><p>一条绳子切m段，怎么切使得每段长度之积最大（动态规划）</p><p>项目创新点，autoTVM算法</p><p>表达的太垃圾了。。。表述要清晰</p><p>有点kpi，最后“如果没什么问题我们就结束了”。。。反问环节也没有</p><h3 id="科大讯飞-提前批（飞凡计划）-研发方向">科大讯飞-提前批（飞凡计划）-研发方向</h3><p><img src="image2.png" alt="alt text"></p><h4 id="0706投递">0706投递</h4>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT 6.S081 复习</title>
    <link href="/2024/06/25/MIT-6-S081-%E5%A4%8D%E4%B9%A0/"/>
    <url>/2024/06/25/MIT-6-S081-%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><p>参考<br><a href="http://xv6.dgs.zone/">某课程笔记</a><br><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-s081">课程翻译</a></p><h2 id="操作系统接口">操作系统接口</h2><h3 id="进程和内存">进程和内存</h3><blockquote><p>当我们在shell中运行某个指令时，shell通常会创建一个新的进程来执行，即fork一个子进程，然后在子进程中调用<code>exec</code>来执行指令</p></blockquote><h4 id="fork调用">fork调用</h4><p>进程可以使用<code>fork</code>复制创建一个新进程，其内存内容与父进程完全一致（严格来说，父进程只是C程序被编译之后存在内存中的指令，因此是可以被100%复制的）<br>fork函数在父进程中返回子进程的PID，在子进程中返回0</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> pid = fork();<br><span class="hljs-keyword">if</span>(pid &gt; <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;parent: child=%d\n&quot;</span>, pid);<br>    <span class="hljs-comment">// 等待直到任一子进程结束，返回pid</span><br>    pid = wait((<span class="hljs-type">int</span> *) <span class="hljs-number">0</span>);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;child %d is done\n&quot;</span>, pid);<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(pid == <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;child: exiting\n&quot;</span>);<br>    <span class="hljs-comment">// 退出，并返回状态码0 （一般0表正常，1表异常）</span><br>    <span class="hljs-built_in">exit</span>(<span class="hljs-number">0</span>);<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;fork error\n&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>在fork函数执行时，父子进程的内存内容完全相同，但由于身份的不同后续可能执行不同的代码</p><h4 id="exec调用">exec调用</h4><p>使用<code>exec</code>从文件系统中找到指定的可执行文件，将当前进程的内存内容替换为新的可执行文件的内容<br><code>exec</code>通常不会返回，毕竟<code>exec</code>会完全替换当前进程的内存，除非执行错误（比如找不到该文件）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">char</span>* argv[<span class="hljs-number">3</span>];<br>argv[<span class="hljs-number">0</span>] = <span class="hljs-string">&quot;echo&quot;</span>;<br>argv[<span class="hljs-number">1</span>] = <span class="hljs-string">&quot;hello&quot;</span>;<br>argv[<span class="hljs-number">2</span>] = <span class="hljs-number">0</span>;<br><span class="hljs-comment">// exec认为第一个参数是程序名并忽略，所以此处的输出只有hello</span><br><span class="hljs-comment">// argv[2] = 0 标志了数组的结尾。值为0的指针对C来说表示结束</span><br>exec(<span class="hljs-string">&quot;/bin/echo&quot;</span>, argv);<br><span class="hljs-comment">// 因此这里可以直接print error，因为exec成功的话就不会再执行后面代码</span><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;exec error\n&quot;</span>);<br></code></pre></td></tr></table></figure><h3 id="I-O和文件描述符">I/O和文件描述符</h3><p>文件描述符（file descriptor）是一个小整数，用于表示进程可以读或写的对象，如文件、目录、设备、管道等。这种方式将文件、管道、设备之间的差异抽象出来，使他们看起来都像字节流</p><p>xv6内核使用文件描述符作为每个进程表的索引。每个进程都有一个从0开始的文件描述符表，每个表项指向一个“文件”，按照惯例，进程的文件描述符0，1，2分别指向标准输入、标准输出和标准错误</p><p>下面是一个进程使用系统调用<code>read</code>和<code>write</code>将数据从标准输入复制到标准输出的过程，如果失败，将错误信息写到标准错误：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">char</span> buf[<span class="hljs-number">512</span>];<br><span class="hljs-type">int</span> n;<br><span class="hljs-keyword">for</span> (;;) &#123;<br>    <span class="hljs-comment">// read 从fd0（标准输入）读取数据到buf中，并返回读取的字节数</span><br>    n = read(<span class="hljs-number">0</span>, buf, <span class="hljs-keyword">sizeof</span> buf);<br>    <span class="hljs-comment">// read从fd0中读数据，每个文件会自带一个文件偏移量，记录上次读取的位置，并从该位置往下读</span><br>    <span class="hljs-comment">// 也就是说，如果标准输入只给了512字节，那从第二个循环开始，n将为0，代表读取完成</span><br>    <span class="hljs-keyword">if</span> (n == <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">break</span>;<br>    <span class="hljs-keyword">if</span> (n &lt; <span class="hljs-number">0</span>) &#123;<br>        <span class="hljs-comment">// 在fd2（标准错误）中输出错误信息</span><br>        <span class="hljs-built_in">fprintf</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;read error\n&quot;</span>);<br>        <span class="hljs-built_in">exit</span>(<span class="hljs-number">1</span>);<br>    &#125;<br>    <span class="hljs-comment">// write 将buf中长度为n的数据写入文fd1（标准输出），并返回写入的字节数</span><br>    <span class="hljs-keyword">if</span> (write(<span class="hljs-number">1</span>, buf, n) != n) &#123;<br>        <span class="hljs-built_in">fprintf</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;write error\n&quot;</span>);<br>        <span class="hljs-built_in">exit</span>(<span class="hljs-number">1</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这其实也是一个简单的cat程序，从标准输入读取数据，然后写到标准输出</p><p>系统调用<code>close</code>会释放一个fd，使其可被重用。在新分配fd时，总会分配当前进程中标号最小的未使用描述符</p><p><code>fork</code>恢复至父进程的文件描述符表和内存，因此父子进程在开始拥有完全相同的打开文件。下面时利用fork进行I/O重定向的例子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">char</span>* argv[<span class="hljs-number">2</span>];<br>argv[<span class="hljs-number">0</span>] = <span class="hljs-string">&quot;cat&quot;</span>;<br>argv[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">if</span> (fork() == <span class="hljs-number">0</span>) &#123;<br>    close(<span class="hljs-number">0</span>);<br>    <span class="hljs-comment">// 子进程关闭fd0之后，open保证新打开的input.txt分配最小的可用fd，也就是0</span><br>    <span class="hljs-comment">// 即子进程将input.txt重定向为其标准输入</span><br>    open(<span class="hljs-string">&quot;input.txt&quot;</span>, O_RDONLY);<br>    <span class="hljs-comment">// cat会读取fd0的内容并输出到fd1，即读input.txt的内容并输出到标准输出</span><br>    exec(<span class="hljs-string">&quot;cat&quot;</span>, argv);<br>&#125;<br></code></pre></td></tr></table></figure><p>需要注意的是，<code>fork</code>虽然会复制文件描述符表，并且这个表在之后不会共享，但是每个基础文件偏移量是共享的：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">if</span> (fork() == <span class="hljs-number">0</span>) &#123;<br>    write(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;hello &quot;</span>, <span class="hljs-number">6</span>);<br>    <span class="hljs-built_in">exit</span>(<span class="hljs-number">0</span>);<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>    wait(<span class="hljs-number">0</span>);<br>    write(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;world\n&quot;</span>, <span class="hljs-number">6</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>这段代码会输出<code>hello world</code>，因为子进程和父进程共享文件偏移量，所以子进程写完<code>hello</code>之后，标准输出（视为基础文件）的文件偏移量也会被移动到<code>hello</code>之后，所以父进程接着写<code>world</code>，而不是从头开始写</p><blockquote><p>文件偏移量与文件描述符绑定，而不是文件本身</p></blockquote><p><code>dup</code> 系统调用会复制一个现有的文件描述符，返回一个新的文件描述符，指向同一个文件。和<code>fork</code>一样，<code>dup</code>出来的fd也和原来的fd共享偏移量（可以理解成引用）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c">fd = dup(<span class="hljs-number">1</span>);<br>write(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;hello &quot;</span>, <span class="hljs-number">6</span>);<br>write(fd, <span class="hljs-string">&quot;world\n&quot;</span>, <span class="hljs-number">6</span>);<br></code></pre></td></tr></table></figure><h3 id="管道">管道</h3><p>管道是一对用于读和写的缓冲区，用于进程间通信。</p><p>如果没有可用数据，对于管道读操作会进入等待，直到有新数据写入，或者所有指向管道写的fd都被关闭</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span> p[<span class="hljs-number">2</span>];<br><span class="hljs-type">char</span> *argv[<span class="hljs-number">2</span>];<br>argv[<span class="hljs-number">0</span>] = <span class="hljs-string">&quot;wc&quot;</span>;<br>argv[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br><span class="hljs-comment">// 创建管道，p[0]记录从管道读fd，p[1]记录从管道写fd</span><br>pipe(p);<br><span class="hljs-keyword">if</span> (fork() == <span class="hljs-number">0</span>) &#123;<br>    close(<span class="hljs-number">0</span>);<br>    dup(p[<span class="hljs-number">0</span>]);<br>    close(p[<span class="hljs-number">0</span>]);<br>    close(p[<span class="hljs-number">1</span>]);<br>    <span class="hljs-comment">// 子进程将fd0指向管道读取端，并执行wc</span><br>    exec(<span class="hljs-string">&quot;/bin/wc&quot;</span>, argv);<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>    close(p[<span class="hljs-number">0</span>]);<br>    <span class="hljs-comment">// 父进程将数据写入管道写入端</span><br>    write(p[<span class="hljs-number">1</span>], <span class="hljs-string">&quot;hello world\n&quot;</span>, <span class="hljs-number">12</span>);<br>    <span class="hljs-comment">// 这里关闭管道写入端，否则子进程的wc会一直等待</span><br>    close(p[<span class="hljs-number">1</span>]);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="操作系统架构——xv6">操作系统架构——xv6</h2><p>操作系统必须满足的是三个要求：多路复用、隔离、交互</p><h3 id="抽象系统资源">抽象系统资源</h3><p>操作系统需要协调多个应用程序的运行，比起每个应用程序直接操作硬件的同时还需要互相协调硬件使用时间，操作系统提供了一种抽象的硬件接口，将资源抽象为服务，实现不同应用之间的隔离</p><h3 id="系统权限分层">系统权限分层</h3><p>RISC-V为CPU提供了三种CPU执行指令的模式：机器模式、管理模式（管态）、用户模式（目态）</p><p>CPU在机器模式下启动，执行少量代码完成初始化配置，然后切换管态，将控制权交给操作系统。操作系统在管态下运行，可以访问所有硬件资源，包括内存、I/O设备、时钟等。用户程序在目态下运行，只能访问有限的硬件资源，不能直接访问内存、I/O设备等，必须通过系统调用来访问</p><h3 id="内核组织">内核组织</h3><p>一个关键问题是判断操作系统的哪些部分需要在管态运行，哪些部分需要在目态运行。</p><p>宏内核：整个操作系统都驻留在内核，所有系统调用的实现都以管态运行<br>缺点在于，OS不同部分的接口通常很复杂，内核难以维护，同时出错可能性较大</p><p>微内核：只有少量的核心功能驻留在内核，其他功能以用户态进程的形式运行。<br>比如说，微内核的文件系统作为用户级进程运行（被称为server），进程通过消息传递内核转发来调用文件系统服务。这样做的好处是，内核的代码量减少，内核更加稳定，同时用户态进程的代码可以更容易地被替换、升级，但坏处是一次交互可能需要多次目态/管态的跳转，影响性能；同时，将os的功能隔离开将更难实现内存共享</p><p>和大多数类Unix一样，xv6使用宏内核设计，尽管他的内核比很多微内核还要小</p><h3 id="xv6进程概述">xv6进程概述</h3><p>xv6以进程为单位做隔离。这种抽象可以防止进程破坏或监视其他进程，或者破坏内核。</p><p>xv6使用由硬件实现的页表来为每个进程提供自己的地址空间，这样每个进程都有自己的专用空间。虚拟地址将每个进程所使用的空间与其他进程隔离开来。</p><h2 id="页表">页表</h2><p>页表是OS为每个进程提供私有地址空间和内存的机制。</p><h3 id="分页硬件">分页硬件</h3><p>将虚拟地址映射到物理地址的硬件。每个进程都有一个自己的页表，并存储在物理内存中，这个页表的物理地址由内核保存。</p><p>当某个进程上CPU后，某个名为SATP的寄存器会装入该进程页表的物理地址。随后内存管理单元（MMU）根据地址找到该进程的页表，开始该进程虚拟内存到物理内存的映射</p><p>xv6基于Sv39 RISC-V，只是用64位虚拟地址中的低39位。页表硬件利用这39位找到一个56位的物理地址<br><img src="image1.png" alt="alt text"></p><p>页表在逻辑上是一个由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>27</mn></msup></mrow><annotation encoding="application/x-tex">2^{27}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">27</span></span></span></span></span></span></span></span></span></span></span></span> 个页表条目（Page Table Entries）组成的数组，每个页表条目对应一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>12</mn></msup></mrow><annotation encoding="application/x-tex">2^{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span></span></span></span></span></span></span></span></span> 大小的页框，每个PTE包含一个44位的物理页号（Physical Page Number）和一些标志位，如可读、可写、可执行等</p><blockquote><p>在上述系统中，虚拟内存空间大小为39位，而物理内存空间大小为56位，但并不是说物理内存空间一定会大于虚拟内存空间</p></blockquote><p>分页硬件利用虚拟地址中的27位索引PTE，找到物理页框号，在结合虚拟地址的低12位偏移量，找到最终的物理地址</p><p>对于多级页表来说，仅仅是将PTE的物理页号替换为下一级页表的逻辑页号，然后再次索引，直到找到最终的物理页号而已，具体的原理和标志位含义等不再赘述</p><h3 id="内核地址空间">内核地址空间</h3><p>xv6为每个进程维护一个页表，用来描述每个进程的用户地址空间，此外，还需要维护一个单独描述内核地址空间的页表</p><p>内核配置其地址空间的布局，以允许自己用虚拟地址访问物理内存和各种硬件资源<br><img src="image2.png" alt="alt text"></p><p>这一部分结构大多由硬件设计者决定，当主板上电之后，主板做的第一件事就是运行存储在0x1000，即boot ROM中的代码，随后会从地址0x8000000000开始运行，启动操作系统。设计者规定，当完成了虚拟到物理地址的映射后，得到的地址如果大于0x8000000000，那么这个地址就是内核地址，走向物理内存；如果小于，就走向不同的I/O设备</p><p>进入系统后，首先需要配置内核使用的虚拟地址空间，也就是左图的地址分布，为了尽可能简单易懂，xv6的内核地址空间大部分与物理地址相等。</p><p>注意上图中有一部分page在虚拟内存的位置很靠后，如kernel stack，此外，两个Kstack后面都跟随了一个未被映射到物理内存的Guard Page。为了防止栈溢出，Guard Page的PTE中，其flags中的valid标志位会保持为空，这样当发生栈溢出时，MMU在访问溢出地址的PTE时会发现其valid位为空，从而触发异常，内核可以处理这个异常，而不是让程序继续执行。这样设置的同时还节省了物理内存</p><p>尽管kernel stack在Kernel data中被重复映射了一次，在实际使用Kstack时还是使用上面的这部分地址，因为由Guard page更加安全</p><p>实验还没看，先笼统回顾一遍吧</p><h2 id="陷阱和系统调用">陷阱和系统调用</h2><p>有三类事件会导致CPU搁置当前普通指令的执行，并将控制权转移给处理该事件的特殊代码：</p><ol><li>系统调用：用户程序执行<code>ecall</code>来要求内核执行某行动时</li><li>异常：指令（用户或内核都可能）做了一些非法的事情</li><li>设备中断：某设备向系统发出某种信号，表明它需要被处理</li></ol><p>这三类被统称为trap，通常情况下，我们希望trap是透明的，正在cpu的代码只需要稍后恢复，而意识不到发生了任何特殊的事情，一般来说，处理顺序为：<br>trap强制将cpu控制权转移到内核–&gt;内核保存寄存器和其他状态，以便稍后恢复–&gt;内核执行适当的trap处理程序–&gt;内核之前保存的状态</p><h2 id="中断和设备驱动">中断和设备驱动</h2>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>interview</tag>
      
      <tag>课程笔记</tag>
      
      <tag>OS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024秋招准备</title>
    <link href="/2024/05/12/2024%E7%A7%8B%E6%8B%9B%E5%87%86%E5%A4%87/"/>
    <url>/2024/05/12/2024%E7%A7%8B%E6%8B%9B%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><h2 id="基础">基础</h2><p>参考</p><ul><li><p>八股:</p><ul><li><a href="/papers/leet/%E5%85%AB%E8%82%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80-Kdf7458k.pdf">计算机基础 Kdf7458k</a></li><li><a href="/papers/leet/%E5%85%AB%E8%82%A1C++-Kdf7458k.pdf">C++ Kdf7458k</a></li><li><a href="/papers/leet/%E5%85%AB%E8%82%A1%E7%AE%97%E6%B3%95-Kdf7458k.pdf">算法 Kdf7458k</a></li><li><a href="/papers/leet/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.pdf">设计模式</a></li></ul></li><li><p>网络、操作系统、数据库：<a href="https://www.xiaolincoding.com/">小林coding</a></p></li><li><p>反问：<a href="https://github.com/yifeikong/reverse-interview-zh">reverse-interview-zh</a></p></li></ul><h2 id="项目经历">项目经历</h2><h3 id="webservercl">webservercl</h3><p>参考</p><ul><li><a href="/papers/leet/webserver-dmsxlwb0624.pdf">webserver</a></li><li><a href="https://github.com/qinguoyi/TinyWebServer">tinywebserver</a></li></ul><h3 id="操作系统">操作系统</h3><p>参考</p><ul><li><a href="%5Cpapers%5Cleet%5C6.s081-dldue234L.pdf">6.s081</a></li></ul><h2 id="实习经历">实习经历</h2><p>参考</p><ul><li><a href="https://github.com/BBuf/tvm_mlir_learn">TVM</a></li></ul><h2 id="笔试">笔试</h2><p>参考：</p><ul><li><a href="https://www.hello-algo.com/">hello-algo</a></li><li><a href="https://programmercarl.com/">代码随想录</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图解GPT</title>
    <link href="/2024/05/11/%E5%9B%BE%E8%A7%A3GPT/"/>
    <url>/2024/05/11/%E5%9B%BE%E8%A7%A3GPT/</url>
    
    <content type="html"><![CDATA[<p>最近看了3Blue1Brown的深度学习，有些新理解，做下记录，参考</p><p><a href="https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528929">3Blue1Brown</a></p><span id="more"></span><h2 id="GPT：Generative-Pre-trained-Transformer">GPT：Generative Pre-trained Transformer</h2><p>对GPT来说，一次完整的文本生成过程中的数据流转大致分为以下几个阶段：</p><p>首先，输入内容会被切分为许多小片段，称为token，token可以是单词、单词片段、字符等。</p><p>每个token都会被映射到一个高维语义空间中的向量，这个过程称为embedding。此时的token是上下文无关的，向量只包含token本身的信息。</p><p>理所应当的，在这个高维语义空间中，语义接近的token对应的向量也会比较接近</p><p>这些向量在经过注意力机制处理后，通过相互传递信息来更新自己的值，使原先的上下文无关的token向量变得上下文相关。</p><p>经过层层处理，整段输入文字的所有关键信息都将融入到这段文字的token序列的最后一个向量中，该向量经过解码，输出为下一个token的概率分布。GPT据此产生下一个token。</p><p>GPT3总共约有1750亿个参数，绝大部分以矩阵形式存在</p><p>嵌入矩阵：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>e</mi></msub><mo>=</mo><msub><mi>n</mi><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>×</mo><msub><mi>n</mi><mrow><mi>d</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub><mo>=</mo><mn>50257</mn><mo>×</mo><mn>12288</mn><mo>≈</mo><mn>6</mn><mtext>亿</mtext></mrow><annotation encoding="application/x-tex">W_{e} = n_{token} \times n_{dimension} = 50257 \times 12288 \approx 6亿</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">im</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">50257</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">12288</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">6</span><span class="mord cjk_fallback">亿</span></span></span></span></p><h3 id="Embedding">Embedding</h3><p><img src="image.png" alt="alt text"></p><p>嵌入矩阵包含了所有token的初始向量，GPT3中共设置50257种token，每个token拥有12288个维度。</p><p><img src="image-1.png" alt="alt text"></p><p>空间中的方向，将能够承载不同的语义信息</p><p>需要注意的是，GPT中的token在经注意力模块处理前还融合了该token在文本中的位置信息，即位置编码</p><h3 id="Attention">Attention</h3><p>初始化的token携带的是原始的、上下文无关的信息，下一步就是让这些token有效的结合上下文信息，使这些向量获得比单个词汇更丰富、更具体的含义<br><img src="image-5.png" alt="alt text"></p><p>第一步是如何让每个token注意到其他token，尤其是与自己相关的token，比如名词前的形容词，指代词指代的前面的名词等等</p><p>可以理解成，初始的token包含了该token的不同含义，现在需要对这个向量做一些处理，使它能够更好的偏移到文中含义的方向上，即对token精细化，具体化</p><p>下面是处理的具体步骤：</p><ol><li><p>分别使用一个(128,12288)的权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_{Q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_{K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，对每一个(12288，1)的token进行左乘，得到两个(128,1)的新向量,分别为该token的query向量和key向量</p><blockquote><p>可以把query向量理解为该token需要了解的信息，key向量理解为该token的信息，token1的query和token2的key在空间上越接近（也就是点积越大），说明这两个向量的信息越相关</p></blockquote></li><li><p>每个token的query分别于上下文中其他token的key做点积，对同一个query，将其与所有key的点积结果组成一个向量，进行归一化，即代表了该token对其他token的注意力分布</p><blockquote><p>这就是为什么上下文token长度会成为大模型的瓶颈，这个点积计算的复杂度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，n为token长度，GPT3中的上下文长度为2048<br><img src="image-6.png" alt="alt text"></p></blockquote></li><li><p>需要注意的是，我们不希望token注意到自己之后的token，因此在计算注意力分布时，将下半部分对应的点积结果设为负无穷，这样在softmax后，该token对后面token的注意力分布就会为0<br><img src="image-7.png" alt="alt text"></p></li><li><p>在得到token的注意力分布后，需要根据该分布调整目前的token，以得到更准确的token值。我们需要使用一个新的(12288,12288)的权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_{V}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_{V}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>左乘每个(12288，1)的token，得到一个(122888,1)的新向量，这个向量就是该token的value向量，可以理解为，value向量表示了该token对其他token的影响，当token1需要根据token2改变自己的语义时，就将token1的值加上token2的value，完成token1在语义空间上的偏移<br><img src="image-8.png" alt="alt text"></p></li><li><p>每一个token，都根据自己的注意力分布，去加上其他token的value值，完成自身的偏移，即完成一次注意力处理<br><img src="image-9.png" alt="alt text"></p></li><li><p>以上是一个单头自注意层的工作流程，注意到所需要的权重矩阵中，value矩阵的参数量远大于query和key矩阵，因此，我们一般将value矩阵低秩分解为两个小矩阵相乘，比如(12288,12288)分解为(12288,128)和(128,12288)，这样可以减少参数量，同时便于三种矩阵的存储和计算</p></li><li><p>在GPT3中，一个多头自注意层拥有96个注意力头，意味着96组不同的K,Q,V矩阵，以产生96种不同的注意力模式，对每一个token来说，每一个自注意头都产生一种偏移，将这96个偏移都加到原始token上，就得到该多头自注意层的结果</p></li></ol><blockquote><p>GPT3的上下文长度限制在2048个token，这意味着GPT3在处理长文本时，只能看到前2048个token的信息</p></blockquote><h3 id="UnEmbedding">UnEmbedding</h3><p><img src="image-2.png" alt="alt text"></p><h3 id="Softmax">Softmax</h3><p>在对最后的token向量解码时，GPT3使用了一个softmax函数，将向量转化为下一个token的概率分布，注意到此处用到了带温度系数的softmax，用于控制输出的多样性</p><p>给指数加一个分母T，通常取值在0-10之间，显而易见当T取大值时，输出概率会给低值赋予更高的权重，使得输出多样化，反之，T取小值时，输出概率会更加集中，使得输出更加确定。T=0意味着输出概率中只有一个token的概率为1，其他token的概率为0</p><p><img src="image-3.png" alt="alt text"></p><p><img src="image-4.png" alt="alt text"></p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning Systems: Design and Implementation</title>
    <link href="/2024/04/01/Machine-Learning-Systems-Design-and-Implementation/"/>
    <url>/2024/04/01/Machine-Learning-Systems-Design-and-Implementation/</url>
    
    <content type="html"><![CDATA[<p>all in hpc</p><span id="more"></span><h2 id="导论">导论</h2><h3 id="机器学习框架的设计目标">机器学习框架的设计目标</h3><p>广义来说，一个机器学习框架有以下的设计目标：</p><ul><li>神经网络编程：根据各式需求定制不同的神经网络</li><li>自动微分：自动计算梯度以训练网络</li><li>数据管理和处理</li><li>模型训练和部署</li><li>与硬件加速器结合</li><li>分布式执行</li></ul><p>在框架设计之初，开发者们曾尝试建立传统的<strong>神经网络开发库</strong>（如Theano和Caffe），以及<strong>数据处理框架</strong>（如Spark和Google Pregel）等方式达到以上设计目标，但各有不足：</p><table><thead><tr><th style="text-align:center">方式</th><th style="text-align:center">神经网络编程</th><th style="text-align:center">自动微分</th><th style="text-align:center">数据管理</th><th style="text-align:center">训练和部署</th><th style="text-align:center">硬件加速</th><th style="text-align:center">分布式执行</th></tr></thead><tbody><tr><td style="text-align:center">传统神经网络库</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">×</td><td style="text-align:center">×</td><td style="text-align:center">√</td><td style="text-align:center">×</td></tr><tr><td style="text-align:center">数据处理框架</td><td style="text-align:center">×</td><td style="text-align:center">×</td><td style="text-align:center">√</td><td style="text-align:center">×</td><td style="text-align:center">×</td><td style="text-align:center">√</td></tr><tr><td style="text-align:center">机器学习框架</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td></tr></tbody></table><h3 id="机器学习框架的基本组成">机器学习框架的基本组成</h3><p><img src="image.png" alt="alt text"></p><ul><li><p>编程接口: 考虑到机器学习开发人员背景的多样性，机器学习框架首先需要提供以高层次编程语言（如Python）为主的编程接口。同时，机器学习框架为了优化运行性能，需要支持以低层次编程语言（如C和C++）为主的系统实现，从而实现操作系统（如线程管理和网络通讯等）和各类型硬件加速器的高效使用。</p></li><li><p>计算图： 利用不同编程接口实现的机器学习程序需要共享一个运行后端。实现这一后端的关键技术是计算图技术。计算图定义了用户的机器学习程序，其包含大量表达计算操作的算子节点（Operator Node），以及表达算子之间计算依赖的边（Edge）。</p></li><li><p>编译器前端： 机器学习框架往往具有AI编译器来构建计算图，并将计算图转换为硬件可以执行的程序。这个编译器首先会利用一系列编译器前端技术实现对程序的分析和优化。编译器前端的关键功能包括实现中间表示、自动微分、类型推导和静态分析等。</p></li><li><p>编译器后端和运行时： 完成计算图的分析和优化后，机器学习框架进一步利用编译器后端和运行时实现针对不同底层硬件的优化。常见的优化技术包括分析硬件的L2/L3缓存大小和指令流水线长度，优化算子的选择或者调度顺序。</p></li><li><p>异构处理器： 机器学习应用的执行由中央处理器（Central Processing Unit，CPU）和硬件加速器（如英伟达GPU、华为Ascend和谷歌TPU）共同完成。其中，非矩阵操作（如复杂的数据预处理和计算图的调度执行）由中央处理器完成。矩阵操作和部分频繁使用的机器学习算子（如Transformer算子和Convolution算子）由硬件加速器完成。</p></li><li><p>数据处理： 机器学习应用需要对原始数据进行复杂预处理，同时也需要管理大量的训练数据集、验证数据集和测试数据集。这一系列以数据为核心的操作由数据处理模块（例如TensorFlow的tf.data和PyTorch的DataLoader）完成。</p></li><li><p>模型部署： 在完成模型训练后，机器学习框架下一个需要支持的关键功能是模型部署。为了确保模型可以在内存有限的硬件上执行，会使用模型转换、量化、蒸馏等模型压缩技术。同时，也需要实现针对推理硬件平台（例如英伟达Orin）的模型算子优化。最后，为了保证模型的安全（如拒绝未经授权的用户读取），还会对模型进行混淆设计。</p></li><li><p>分布式训练： 机器学习模型的训练往往需要分布式的计算节点并行完成。其中，常见的并行训练方法包括数据并行、模型并行、混合并行和流水线并行。这些并行训练方法通常由远端程序调用（Remote Procedure Call, RPC）、集合通信（Collective Communication）或者参数服务器（Parameter Server）实现。</p></li></ul><h2 id="编程接口">编程接口</h2><h3 id="机器学习系统编程模型的演进">机器学习系统编程模型的演进</h3><p><img src="image1.png" alt="alt text"></p><p>机器学习系统的诞生初期，Lua（torch）或python（Theano）是主流编程语言，适用于小型和科研为导向的机器学习应用</p><p>神经网络在2011年快速崛起。而训练深度神经网络需要大量的算力，Torch和Theano无法为这些算力发挥做有力支撑。同时，计算加速卡如Nvidia 的通用API接口（如CUDA C）日趋成熟，且构建在CPU多核技术之上的多线程库也逐渐被广大开发者接受。许多开发者希望在C++的基础上开发高性能的深度学习应用。这一类需求被Caffe等一系列C++之上框架所满足</p><p>然而，机器学习模型往往需要在部署场景、数据类型、任务细节等需求上进行深度定制，这类定制任务涉及的背景多样，相关开发者往往无法基于C++熟练的完成任务。因此与C++的深度绑定，也成为了框架推广的制约。</p><p>2015年底Google率先推出tensorflow，相比传统的torch，TF采取前后端分离的设计理念，使用Python作为面向用户的主要前端语言，后端则使用相对高性能的C和C++实现。大量基于Python的前端API确保了TF可以被大量的数据/机器学习科学家接受，同时帮助TF快速融入以Python为主导的大数据生态。使得TF在保持后端高性能的同时，还兼具Python的灵活性和生态。这种设计理念也引领了后续Pytorch、MindSpore等机器学习框架的研发</p><h3 id="机器学习工作流">机器学习工作流</h3><ol><li>数据处理：首先，用户需要使用<strong>数据处理API</strong>来从磁盘中导入数据集，并完成数据的预处理</li><li>模型定义：完成预处理后，用户需要<strong>模型定义API</strong>来定义机器学习模型，并需要提供特定的模型参数以供自定义</li><li>优化器：模型的输出需要与用户的标签对比差异，一般是通过损失函数来评估效果。因此框架需要<strong>优化器API</strong>来支持用户定义自己的损失函数，并定义各种优化算法来计算梯度，完成模型参数的更新</li><li>训练：用户可以使用<strong>训练API</strong>来定义循环、批量大小等以完成模型的训练，框架会自动完成前向传播、反向传播、梯度计算、参数更新等操作</li><li>测试与调试：<strong>测试API</strong>用于对当前模型的精度进行评估</li></ol><h3 id="定义深度神经网络">定义深度神经网络</h3><p><strong>以层为核心，将提供的神经网络层组件按照网络结构进行堆叠和连接</strong></p><p>机器学习框架将神经网络层抽象为一个基类，如Pytorch中的torch.nn.Module，基于该基类延伸出如torch.nn.Conv2d，torch.nn.MaxPool2d的子类，用以实现特定的层的功能</p><h3 id="C-C-编程接口">C/C++编程接口</h3><p>现代机器学习框架主要依赖Pybind11来使大量的底层C/C++函数自动生成对应的Python接口，这一过程一般称为Python绑定</p><p><strong>使用C/C++自定义算子</strong></p><p>当框架自带的算子（即低级API）无法满足实际需要时，就需要手写算子，在机器学习框架中实现一个手写算子需要以下步骤：</p><ol><li>注册算子原语：定义算子信息，包括名字、类型约束、算子属性（如conv的stride、padding）等等，这些信息可以被框架用来进行优化，用户能够在后续直接使用这个算子</li><li>GPU Kernel实现：编写跑在kernel上的代码，包括声明Tensor大小，输入输出字节数、工作区大小等等，并实现线程调度、内存访问等操作</li><li>GPU Kernel注册：框架根据注册的算子原语以及I/O数据类型等信息，调用CUDA的内置数据类型对算子进行实例化</li></ol><h3 id="机器学习框架的编程范式">机器学习框架的编程范式</h3><p>函数式编程：将计算视为函数的求值，而非改变状态和数据的操作。在函数式编程中，函数是一等公民，可以作为参数传递给其他函数，也可以作为返回值返回给调用者。同时，函数相互隔离也使并发行和并行性更容易管理</p><h2 id="计算图">计算图</h2><p>基本构成：基本数据结构——张量 + 基本运算单元——算子</p><h3 id="静态生成">静态生成</h3><p>先编译后执行，将计算图定义与执行分离</p><p>使用前端语言定义模型形成完整的程序表达后，框架会首先对模型进行分析，得到网络层之前的拓扑关系以及参数变量设置、损失函数等信息，并将完整的模型描述编译为可被后端计算硬件调用执行的固定代码文本，这种固定的代码文本称为静态图</p><p>后端直接通过相应硬件调度执行图中的算子，也可以通过优化图结构（如融合算子）提高计算效率</p><p>静态编译并不读取输入数据，此时就需要一种特殊张量——占位符来辅助构建完整图</p><p>条件控制在静态图编译阶段不会完成判断，因此条件控制算子下的所有分支计算图都会纳入总计算图中</p><p>静态图具有两大优势：计算性能和直接部署。依靠静态图，框架可以掌握整个计算过程的全部信息，因此更容易制定优化策略，同时静态图由于只需编译一次，也支持序列化保存，在模型推理阶段直接执行保存的静态图，无需前端再次编译</p><p>劣势：开发繁琐，代码调试难度大，无法及时打印中间结果</p><h3 id="动态生成">动态生成</h3><p>编译与执行同时发生。使用前端对代码完成模型构建后，动态生成并不利用编译器生成完整的静态计算图，而是利用前端的PythonAPI调用机器学习框架，利用框架自身的算子分发功能，将Python调用的算子上机进行计算，再将结果返回前端，继续下一个算子的计算。</p><p>因此，对于动态图，框架每个时间点都只处理当前的计算节点，无法获取到完整的模型结构，也较难进行优化</p><p>静态图可以很方便的构建出完整的前向计算图和反向计算图。而动态图由于边解析边执行，反向梯度计算的构建需要随着前向计算的调用同步执行。在前向执行的过程中，框架会根据算子的调用信息，记录对应的反向算子信息以及参与梯度计算的张量信息。前向计算完毕时，反向计算也完成记录</p><p>动态图模式更偏向于命令时编程范式，使用前端语言构建网络模型，也更加编程友好</p><table><thead><tr><th style="text-align:center">特性</th><th style="text-align:center">静态图</th><th style="text-align:center">动态图</th></tr></thead><tbody><tr><td style="text-align:center">即时获取中间结果</td><td style="text-align:center">否</td><td style="text-align:center">是</td></tr><tr><td style="text-align:center">代码调试难度</td><td style="text-align:center">难</td><td style="text-align:center">易</td></tr><tr><td style="text-align:center">控制流实现方式</td><td style="text-align:center">特定语法</td><td style="text-align:center">前端语言语法</td></tr><tr><td style="text-align:center">性能</td><td style="text-align:center">优化策略多，性能佳</td><td style="text-align:center">优化受限，性能一般</td></tr><tr><td style="text-align:center">内存占用</td><td style="text-align:center">少</td><td style="text-align:center">多</td></tr><tr><td style="text-align:center">部署</td><td style="text-align:center">直接部署</td><td style="text-align:center">不可直接部署</td></tr></tbody></table><h3 id="计算图调度">计算图调度</h3><p>根据计算图可以找到相互独立的算子进行并发调度，提高计算的并行性。而存在依赖关系的算子则必须依次调度执行</p><h2 id="AI编译器与前端">AI编译器与前端</h2><p><img src="image2.png" alt="alt text"></p><p>前端图级别优化（硬件无关），后端硬件相关优化</p><p><img src="image3.png" alt="alt text"></p><p>AI编译器前端与传统编译器前端的最大不同：需要满足自动微分</p><h3 id="中间表示-IR">中间表示 IR</h3><h4 id="分类">分类</h4><p><strong>线性IR</strong><br>Linear IR是一种简单直接的中间表示，将前端的计算图直接转换为一系列的线性计算操作，每个操作都是一个算子，包括输入输出张量、算子类型、算子参数等信息</p><p><strong>图IR</strong><br>Graphical IR将编译信息保存到图中，通过图中的节点、边、树等表述算法，如AST、CFG、DAG等。图IR的优势在于可以更好的表达算法的结构信息，同时也更容易进行优化</p><p><strong>混合IR</strong><br>如LLVM IR，将线性IR和图IR结合，使用线性IR表示基本块，使用图IR表示块之间的控制流</p><h4 id="机器学习框架的IR">机器学习框架的IR</h4><ol><li>张量表达：机器学习框架主要处理张量数据，IR必须可以支持张量的正确表达</li><li>自动微分：IR需要考虑自动微分实现的简洁性、性能以及高阶微分的扩展能力</li><li>计算图模式：需要同时支持静态图和动态图</li><li>支持高阶函数和闭包：函数式编程的重要特性</li><li>编译优化</li><li>JIT：满足实时编译</li></ol><p><strong>PyTorch</strong><br>PyTorch基于动态图，使用TorchScript作为IR，支持Python和C++两种前端语言，支持JIT编译，支持自动微分，支持高阶函数和闭包</p><p>PyTorch采用命令式编程方式，通过JIT将Python代码即时编译为TorchScript</p><p><strong>Jax</strong><br>Jaxpr IR</p><h3 id="自动微分">自动微分</h3><p>常见计算机程序求导的方法：</p><ol><li>手动微分：手工求解导数表达式，再由计算机代入求解</li><li>数值微分：由导数定义，使用差分近似导数结果</li><li>符号微分：计算机使用数学规则对函数表达式进行递归变换，但容易产生表达式膨胀</li><li>自动微分：运用链式法则，将运算操作分解为一个有限的基本操作集合，在完成每一个基本操作的求导后，将结果组合得到整体程序的求导结果</li></ol><p>对于某函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>n</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>−</mo><mi>s</mi><mi>i</mi><mi>n</mi><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">y = f(x_{1},x_{2}) = ln(x_{1}) + x_{1}x_{2} - sin{x_{2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">in</span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> 计算其在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>2</mn><mo separator="true">,</mo><mn>5</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_{1},x_{2})=(2,5)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">5</span><span class="mclose">)</span></span></span></span> 处的导数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">d</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">d</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\mathrm{d} y}{\mathrm{d} x_{1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3773em;vertical-align:-0.4451em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">d</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">d</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><p>首先，框架会为其转出对应计算图：<br><img src="image4.png" alt="alt text"></p><p>前向模式：顺着计算图方向计算，依次计算算子对值以及算子对x1的梯度<br>反向模式：依次计算y对上一个算子的梯度，以此类推</p><p>在机器学习的应用中，输入的数量一般远大于输出数量，所以普遍采用反向模式的自动微分，即反向传播。（正向传播从输入开始，每个输入都要进行一次传播，而反向传播从输出开始，大部分情况只需要进行一次传播）</p><h3 id="类型系统和静态分析">类型系统和静态分析</h3><p><strong>类型系统</strong><br>类型的集合，以及使用类型来规定程序行为的规则。</p><p>机器学习框架一般使用Python作为描述模型结构的前端语言，Python作为动态强类型的语言，易上手且开发简洁高效，但由于其解释执行的方式，运行往往较慢。因此要想生成高效的后端代码，后端框架需要优化友好的静态强类型中间表示，就需要一种高效可靠的静态分析方法作为桥梁，比如Hindley–Milner（HM）类型系统</p><p><strong>静态分析</strong><br>过</p><h3 id="常见前端编译优化方法">常见前端编译优化方法</h3><ol><li>无用和不可达代码消除：无用即输出不被其他任何代码使用；不可达指没有有效的控制流路径包含该代码。这类代码消除可使中间表示减负，提高程序编译运行速度</li><li>常量传播和常量折叠：在编译时使用常量替换或直接运算出某些常量计算的结果</li><li>公共子表达式消除</li></ol><h2 id="编译器后端和运行时">编译器后端和运行时</h2>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>课程笔记</tag>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++ 杂记</title>
    <link href="/2024/03/11/C-%E6%9D%82%E8%AE%B0/"/>
    <url>/2024/03/11/C-%E6%9D%82%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><h2 id="基础">基础</h2><h3 id="内存分区">内存分区</h3><ol><li>代码区：函数体的二进制代码</li><li>全局区：全局变量、静态变量、常量</li><li>栈区：函数调用、局部变量，编译器自动分配</li><li>堆区：程序员动态分配内存，如new和delete</li></ol><h3 id="枚举">枚举</h3><p><code>enum enum_name&#123;a,b,c..&#125; var_name;</code><br>枚举标识符可赋值，默认从0开始+1</p><h3 id="const">const</h3><p>常类型的变量或对象的值无法被更新</p><h4 id="修饰变量">修饰变量</h4><p>必须赋初始值，且赋值之后无法修改</p><p>与#define相比，const定义的常量有数据类型，编译器可以进行类型安全检查，而#define只是单纯的字符串替换</p><p>可以防止修改，提高代码健壮性；同时节省空间</p><p>const变量默认为文件作用域，如果想在其他文件中使用，需要加extern</p><h4 id="修饰指针">修饰指针</h4><p><code>int const *p;</code> 或 <code>const int *p;</code>: const在*左边，无法通过指针修改这个变量的值<strong>指向常量的指针</strong></p><blockquote><p>无法通过这个指针修改，但仍然可以通过变量名、引用，或者其他指针修改</p></blockquote><p><code>int * const p</code>: const在*右边，指针指向的地址不能被修改 <strong>常指针</strong></p><blockquote><p>定义时必须初始化，常指针无法修改指向的地址，即使两个变量指向同一块地址</p></blockquote><h4 id="修饰引用">修饰引用</h4><p>同理，不能通过引用修改值，但仍可以通过变量名修改</p><h4 id="修饰函数返回类型">修饰函数返回类型</h4><p>函数返回值时，加const无意义，因为本身返回的值也会赋给其他变量，该值就可以通过其他变量修改</p><p>返回指针时同上</p><p><strong>修饰函数形参同上</strong></p><h4 id="修饰类、对象、成员函数">修饰类、对象、成员函数</h4><p>const对象只能访问const成员函数，const成员函数可以访问所有成员变量和其他const成员函数，但无法修改</p><h3 id="static">static</h3><p>为什么引入static：函数内部定义的变量，程序执行到时才分配内存到栈区，函数运行结束即释放。但有时候想要保存该变量的值到下一次调用，同时又不想改变该变量的访问范围</p><h4 id="修饰成员变量">修饰成员变量</h4><p>在程序启动时就被创建，不依赖类的对象，可以使用类名来直接调用</p><h4 id="修饰成员函数">修饰成员函数</h4><p>同样不依赖于类的对象，无法使用this指针，只能访问静态成员变量/函数</p><h3 id="struct和class">struct和class</h3><p>区别：struct的默认访问和继承权限是public，而class的默认访问和继承权限是private</p><h3 id="define和typedef">define和typedef</h3><p>define只做单纯字符串替换，没有类型检查，在预处理阶段起作用；typedef相当于类型别名，在编译运行时起作用</p><h3 id="new和malloc">new和malloc</h3><p>new是一个操作符，使用时会初始化一个对象，调用构造函数，返回一个指向新分配空间的对象实例的指针，因此无需指定内存大小，使用完毕指针销毁时并不会自动回收内存，因此需要手动delete来调用对象的析构函数释放</p><p>malloc是一个库函数，使用时仅仅显式的在堆上分配指定大小的内存</p><h2 id="面向对象">面向对象</h2><h3 id="构造函数">构造函数</h3><p>编译器默认为每个对象提供空的构造函数和析构函数，以提供对象初始化和清理功能，也可以自定义</p><p><code>class_name()&#123;&#125;</code> 可以有参数，可以重载，程序调用对象前自动执行</p><p>编译器默认会给一个类添加三个函数：默认构造函数、拷贝构造函数、析构函数</p><p>如果定义了有参构造，编译器不再提供默认构造，需要手动定义；同理，如果要自定义拷贝构造函数，编译器也不会再自动生成其他构造函数</p><h3 id="析构函数">析构函数</h3><p><code>~class_name()&#123;&#125;</code> 不能有参数，无返回值，程序结束时自动调用</p><h3 id="深拷贝和浅拷贝">深拷贝和浅拷贝</h3><p>浅拷贝：只是简单的拷贝，指针指向的地址相同，两个对象指向同一块内存，释放一个对象的内存会导致另一个对象的内存无效</p><p>深拷贝：重新在堆区分配一块内存，将原对象的值拷贝到新的内存中，两个对象指向不同的内存</p><p>对于拷贝构造来说，如果类中有属性是在堆区开辟的（比如说被拷贝的对象中有指针，指针指向堆区new出来的一块内存），那么在拷贝时也需要重新在堆区开辟内存，并将原对象中的值拷贝到新的内存中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Person</span> &#123;<br><span class="hljs-keyword">public</span>:<br><span class="hljs-built_in">Person</span>() &#123;&#125;<br><br>    <span class="hljs-built_in">Person</span>(<span class="hljs-type">int</span> age ,<span class="hljs-type">int</span> height) &#123;<br>m_age = age;<br>m_height = <span class="hljs-keyword">new</span> <span class="hljs-built_in">int</span>(height);<br>&#125;<br><br><span class="hljs-built_in">Person</span>(<span class="hljs-type">const</span> Person&amp; p) &#123;<br><span class="hljs-comment">//如果不利用深拷贝在堆区创建新内存，会导致浅拷贝带来的重复释放堆区问题</span><br>m_age = p.m_age;<br>m_height = <span class="hljs-keyword">new</span> <span class="hljs-built_in">int</span>(*p.m_height);<br>&#125;<br><br>~<span class="hljs-built_in">Person</span>() &#123;<br><span class="hljs-keyword">if</span> (m_height != <span class="hljs-literal">NULL</span>)&#123;<span class="hljs-keyword">delete</span> m_height;&#125;<br>&#125;<br><span class="hljs-keyword">public</span>:<br><span class="hljs-type">int</span> m_age;<br><span class="hljs-type">int</span>* m_height;<br>&#125;;<br></code></pre></td></tr></table></figure><p>如上，m_height本身是指针，构造时在堆区创建一块内存并指向该内存。如果在拷贝时仅仅做m_height = p.m_height，会导致两个对象指向同一块内存，释放一个对象的内存会导致另一个对象的内存无效</p><h3 id="构造与析构顺序">构造与析构顺序</h3><h4 id="继承">继承</h4><p>基类构造-&gt;派生类构造-&gt;派生类析构-&gt;基类析构</p><h4 id="成员对象">成员对象</h4><p>成员对象构造-&gt;外层对象构造-&gt;外层对象析构-&gt;成员对象析构</p><h3 id="this指针">this指针</h3><p>类的每个成员函数只会诞生一份函数实例放入代码区，多个同类型的对象会共用这份代码<br>在调用成员函数时，C++内置this指针，用于指向调用该函数的对象</p><h3 id="友元friend">友元friend</h3><h4 id="修饰全局函数">修饰全局函数</h4><p>将全局函数在类中声明为友元，就可以访问该类的私有成员</p><h4 id="修饰类">修饰类</h4><p>将类A在类B中声明为友元，A就可以访问B的私有成员</p><h4 id="修饰成员函数-2">修饰成员函数</h4><p>将类A的成员函数在类B中声明为友元，A的成员函数就可以访问B的私有成员</p><h3 id="继承-2">继承</h3><p>无论怎么继承，都无法访问从父类继承的私有成员。<br>私有成员还是会继承，但只是被隐藏了</p><h4 id="公共继承-class-A-public-B">公共继承 class A : public B</h4><p>父类中的公共和保护成员类型不变</p><h4 id="保护继承-class-A-protected-B">保护继承 class A : protected B</h4><p>父类中的公共和保护成员变为保护成员</p><h4 id="私有继承-class-A-private-B">私有继承 class A : private B</h4><p>父类中的公共和保护成员变为私有成员</p><h3 id="多态">多态</h3><p>如果有多个派生类且需要给他们分别实现一个同名的行为，比如说一个动物类，有狗、猫、猪等派生类，都有一个叫的行为，但是叫的实现不同，这时候就可以使用多态</p><p>多态的必须条件：父类指针指向子类对象，其实就是该指针将这个子类对象当作一个其父类，因此可以调用父类中的虚函数，同时由于指向的是子类对象，所以调用的是子类中重写的函数</p><h3 id="虚函数-virtual">虚函数 virtual</h3><p>在基类中，将需要多态的函数以virtual关键字声明，并在派生类中重写该函数。此后，若是<strong>父类指针指向子类对象</strong>时，并使用该指针调用多态函数，就会调用子类的函数</p><p>virtual的意思类似“这个函数可能会被子类重写”，告诉编译器先不急着确定函数地址，编译器会根据指针指向的对象来动态调用对应的函数</p><h4 id="纯虚函数">纯虚函数</h4><p>在需要使用多态的场景中，往往基类中的虚函数只是为了让派生类重写，并无实际意义，此时可将虚函数改为纯虚函数</p><p><code>virtual void func() = 0</code>，纯虚函数没有函数体，只是一个接口，派生类必须重写该函数</p><p>有纯虚函数意味着这个类仅仅用来声明派生类的接口，称为抽象类，无法被实例化</p><h4 id="虚析构">虚析构</h4><p>在父类指针指向子类对象时，如果释放该指针，只会调用父类的析构函数。这是因为在编译时，编译器只知道指针的<br>类型是父类，而不知道指针指向的是子类对象。</p><p>因此，如果想要调用子类的析构函数，需要将父类的析构函数声明为虚析构<code>virtual ~类名()&#123;&#125;</code></p><p>同理纯虚析构<code>virtual ~类名() = 0</code> 和纯虚函数一样，纯虚析构意味着该类是抽象类，除此之外和虚析构没啥区别</p><h3 id="构造-析构能否设置为虚函数">构造/析构能否设置为虚函数</h3><p>构造函数不能：虚函数的原意是只知道接口不用知道对象完整信息的情况下完成某个工作，构造函数在创建对象时显然需要对象的完整信息，因此不应该定义成虚函数</p><p>析构函数需要：在派生类对象中有额外的内存在析构时需要回收时，如果析构函数不是虚函数，就不会触发动态绑定，派生类析构只会调用基类析构，造成内存泄漏</p><h3 id="智能指针">智能指针</h3><p>智能指针是一种用于自动管理内存的工具，在原先的new分配内存时，由于指针并没有自动回收内存的功能，会存在因为忘记delete或者多次delete造成内存泄漏的隐患</p><p>而智能指针是一个模板类，在他的生命周期结束时会调用本身的析构函数，自动进行空间回收</p><p>std::unique_ptr：独占管理分配的对象，确保只有一个指针拥有指定的内存资源<br>std::shared_ptr：共享指针，内部会使用一个引用计数器来跟踪对象被共享的次数，只有在计数为0的时候才释放内存<br>std::weak_ptr：弱引用指针，不增加引用计数器，必须配合共享指针使用</p><h3 id="模板-template-class-typename-T">模板 template&lt;class/typename T&gt;</h3><p>在下面的函数或者类中定义一种通用数据类型，在调用时可以指定具体的数据类型，提高代码复用</p><h3 id="STL：标准模板库">STL：标准模板库</h3><p>三大组件：容器、算法、迭代器<br>容器和算法之间通过迭代器无缝衔接</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>interview</tag>
      
      <tag>C++</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux &amp;&amp; docker 杂记</title>
    <link href="/2024/02/29/linux-&amp;&amp;-docker-%E6%9D%82%E8%AE%B0/"/>
    <url>/2024/02/29/linux-&amp;&amp;-docker-%E6%9D%82%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[ <span id="more"></span><h2 id="Linux">Linux</h2><h3 id="系统启动过程">系统启动过程</h3><h4 id="内核引导">内核引导</h4><p>当计算机打开电源后，首先是BIOS开机自检，按照BIOS中设置的启动设备（通常是硬盘）来启动。操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。</p><h4 id="init">init</h4><p>nit 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。init 程序首先是需要读取配置文件 /etc/inittab</p><p><strong>运行级别</strong><br>许多程序需要开机启动。它们在Windows叫做&quot;服务&quot;（service），在Linux就叫做&quot;守护进程&quot;（daemon）。</p><p>init进程的一大任务，就是去运行这些开机启动的程序。</p><p>但是，不同的场合需要启动不同的程序，比如用作服务器时，需要启动Apache，用作桌面就不需要。</p><p>Linux允许为不同的场合，分配不同的开机启动程序，这就叫做&quot;运行级别&quot;（runlevel）。也就是说，启动时根据&quot;运行级别&quot;，确定要运行哪些程序。</p><p>Linux系统有7个运行级别(runlevel)：</p><ul><li>0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动</li><li>1：单用户工作状态，root权限，用于系统维护，禁止远程登录</li><li>2：多用户状态(没有NFS)</li><li>3：完全的多用户状态(有NFS)，登录后进入控制台命令行模式</li><li>4：系统未使用，保留</li><li>5：X11控制台，登录后进入图形GUI模式</li><li>6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动</li></ul><h4 id="系统初始化">系统初始化</h4><p>在init的配置文件中有这么一行： si::sysinit:/etc/rc.d/rc.sysinit　它调用执行了/etc/rc.d/rc.sysinit。</p><p>rc.sysinit是一个bash shell的脚本，它主要是完成一些系统初始化的工作，是每一个运行级别都要首先运行的重要脚本。</p><p>它主要完成的工作有：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务。</p><p>如在/etc/inittab可能会看到某条目<code>l5:5:wait:/etc/rc.d/rc 5</code> 这个条目的格式是 id:runlevels:action:process，代表在运行级别 5 下，init 应该运行 /etc/rc.d/rc 5 脚本（该脚本意思是启动运行级别5下的所有守护进程），并等待它结束</p><h4 id="建立终端">建立终端</h4><p>在init执行完inittab中启动守护进程的条目后，接下来是打开终端以方便用户交互<br>如<code>1:2345:respawn:/sbin/mingetty tty1</code> 表示运行级别2、3、4、5下，init应该运行/sbin/mingetty tty1，而且当它结束时，init应该重新运行它</p><h3 id="目录结构">目录结构</h3><ul><li><p>/bin：<br>bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。</p></li><li><p>/boot：<br>这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。</p></li><li><p>/dev ：<br>dev 是 Device(设备) 的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。</p></li><li><p>/etc：<br>etc 是 Etcetera(等等) 的缩写,这个目录用来存放所有的系统管理所需要的配置文件和子目录。</p></li><li><p>/home：<br>用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的，如上图中的 alice、bob 和 eve。</p></li><li><p>/lib：<br>lib 是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。</p></li><li><p>/lost+found：<br>这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。</p></li><li><p>/media：<br>linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux 会把识别的设备挂载到这个目录下。</p></li><li><p>/mnt：<br>系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在 /mnt/ 上，然后进入该目录就可以查看光驱里的内容了。</p></li><li><p>/opt：<br>opt 是 optional(可选) 的缩写，这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。</p></li><li><p>/proc：<br>proc 是 Processes(进程) 的缩写，/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。<br>这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：<br>echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all</p></li><li><p>/root：<br>该目录为系统管理员，也称作超级权限者的用户主目录。</p></li><li><p>/sbin：<br>s 就是 Super User 的意思，是 Superuser Binaries (超级用户的二进制文件) 的缩写，这里存放的是系统管理员使用的系统管理程序。</p></li><li><p>/selinux：<br>这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。</p></li><li><p>/srv：<br>该目录存放一些服务启动之后需要提取的数据。</p></li><li><p>/sys：<br>这是 Linux2.6 内核的一个很大的变化。该目录下安装了 2.6 内核中新出现的一个文件系统 sysfs 。</p></li></ul><p>sysfs 文件系统集成了下面3种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的 devpts 文件系统。</p><p>该文件系统是内核设备树的一个直观反映。</p><p>当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。</p><ul><li><p>/tmp：<br>tmp 是 temporary(临时) 的缩写这个目录是用来存放一些临时文件的。</p></li><li><p>/usr：<br>usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。</p></li><li><p>/usr/bin：<br>用户使用的应用程序。</p></li><li><p>/usr/sbin：<br>超级用户使用的比较高级的管理程序和系统守护程序。</p></li><li><p>/usr/src：<br>内核源代码默认的放置目录。</p></li><li><p>/var：<br>var 是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。</p></li><li><p>/run：<br>是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。</p></li></ul><h3 id="文件基本属性">文件基本属性</h3><p>chown 修改所属用户及用户组 chmod修改文件权限<br><img src="image.png" alt="alt text"><br>把每一组rwx看作二进制计数，即chmod 777代表了文件所有者、文件所有者同组用户、其他用户都有rwx权限</p><h2 id="Docker">Docker</h2><p>Docker依赖于已存在并运行的Linux内核环境，因此windows上的Docker需要在虚拟机中运行</p><p>docker run：用于从一个 Docker 镜像创建并启动一个新的容器<br><code>docker run -it ubuntu:latest /bin/bash</code><br>-i代表容器的标准输入保持开启，即可以向容器发送输入<br>-t代表为容器分配一个伪终端，可以看到命令行</p><p>start：启动一个已经存在的容器</p><p>exec：对一个正在运行的容器执行一个命令<br><code>docker exec -it my_container /bin/bash</code><br>在exec之前容器就已经有一个或多个进程，因此exec的命令会在容器中启动一个新的进程，此时exit容器由于还有别的进程所以并不会直接停止</p><p>attach：为正在运行的容器启动一个shell，同样退出时不会停止容器</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git cheat sheet</title>
    <link href="/2023/12/31/Git-cheat-sheet/"/>
    <url>/2023/12/31/Git-cheat-sheet/</url>
    
    <content type="html"><![CDATA[<p><a href="https://git-scm.com/book/zh/v2">Pro Git Book</a></p><span id="more"></span><h2 id="起步">起步</h2><h3 id="Git特色">Git特色</h3><p><strong>直接记录快照，而非差异比较</strong><br>与其他版本控制系统基于差异的版本控制不同，Git直接记录文件快照，每次提交或保存都会基于当前的全部文件创建一个快照并保存这个快照的索引。为了高效，如果文件没有变化，Git不会再次保存，而是只保留一个链接指向之前存储的文件。</p><p><strong>近乎所有操作都在本地执行</strong></p><p><strong>保证完整性</strong><br>Git中所有数据在存储前都会计算校验和（SHA-1），并以校验和的哈希值作为引用和索引。</p><p><strong>只添加数据</strong><br>几乎所有操作都只是添加，几乎没有导致文件不可恢复的操作。</p><p><strong>三种状态</strong></p><ul><li>modified：已修改，文件在工作区被修改，但还未保存到数据库中</li><li>staged：已暂存，修改的文件已经被做了标记，将被包含在下次提交中</li><li>commited：已提交，已经安全的保存在本地数据库中</li></ul><h3 id="运行前配置-git-config">运行前配置 git config</h3><p>配置文件优先级由低到高</p><ul><li>–system:针对该系统上所有用户的所有仓库的通用配置，一般在Git安装目录</li><li>–global:当前用户的所有仓库，一般在用户目录</li><li>–local:当前仓库，一般在当前仓库目录</li></ul><blockquote><p>git config --global <a href="http://user.name">user.name</a> “John Doe”<br>git config --global user.email <a href="mailto:johndoe@example.com">johndoe@example.com</a><br>git config --list  # 可能会有重复，因为Git会依次读取系统、全局、当前仓库的config，后面的会覆盖前面的同名配置</p></blockquote><h2 id="Cheat-Sheet">Cheat Sheet</h2><h3 id="Remote">Remote</h3><p>查看所有分支：<code>branch</code> -r远程，-a所有，-v附带详细信息</p><p>将本地的分支推送至远程仓库：<code>push &lt;remote&gt; &lt;local_branch&gt;:&lt;remote_branch&gt;</code></p><p>删除某远程分支：<code>push &lt;remote&gt; --delete &lt;branch_name&gt;</code></p><h3 id="Local">Local</h3><p>查看提交记录：<code>log</code> --pretty=oneline指定以一行的形式显示</p><p>比较差异：<code>diff &lt;hash1&gt; &lt;hash2&gt;</code></p><p>增删分支：<code>branch &lt;branch_name&gt;</code> -d删除 -t追踪远程分支</p><p>获取远程仓库数据：<code>fetch</code> 从远程仓库下载数据，但不会自动合并，后续需要merge或pull</p><p>撤销提交：<code>reset</code> --soft仅仅撤销提交，–mixed撤销提交并取消暂存区，–hard撤销提交并取消暂存区和工作区 HEAD~n表示前n个版本</p><h3 id="Index-Stage">Index/Stage</h3><p>提交到本地仓库：<code>commit</code> -a自动将所有已跟踪文件暂存起来并提交，相当于先执行add -u ，-m指定提交信息，–amend修改最后一次提交</p><p>比较当前index与某次提交的差异：<code>diff --cached &lt;hash&gt;</code></p><h3 id="Workspace">Workspace</h3><p>检查状态：<code>status</code> -s简洁</p><p>克隆远程仓库：<code>clone</code> 创建一个全新副本</p><p>从远程仓库拉取最新文件：<code>pull</code> 相当于fetch+merge</p><p>添加文件至Index：<code>add</code> -u更新已跟踪文件（即不包括新增文件），-A更新所有改动（包括新增文件），.当前目录所有文件</p><p>签出某个分支或提交或文件：<code>checkout</code> -b创建并签出新分支。如果签出的是一个特定提交，会处于&quot;detached HEAD&quot;状态，可以随意修改并提交，但这个提交不属于任何分支（但仍然存在，只要记得提交的哈希值），可以通过创建分支来保存</p><p>将某一分支的所有更改合并到当前分支：<code>merge &lt;branch_name&gt;</code>，Git默认使用Fast forward模式，即如果被合并分支是当前分支的直接后继，Git会将当前分支指向被合并分支的最新提交并不产生新提交；这时用–no-ff禁用Fast forward模式，仍然会产生一次新提交。如果并非直接前后继关系，Git会创建一个新的合并提交，该提交有两个父提交，即两个分支的最新提交。</p><p>变基：<code>rebase</code> 一般不用。</p><p>将其他分支的某个提交应用到当前分支：<code>cherry-pick &lt;hash&gt;</code>，会产生一个新的提交，提交内容（记得git是保存修改后文件的快照）与原提交相同。</p><p>回滚某提交：<code>revert &lt;hash&gt;</code>，会产生一个新的提交，提交内容与原提交相反。</p><p>清理未被跟踪的文件：<code>clean</code> -n列出将要被清理的文件，-f清理文件，-d清理文件夹，-x清理忽略的文件，-X清理忽略的文件夹</p><p>临时保存工作区和暂存区，将这些改动进栈：<code>stash</code> -u保存未被跟踪的文件，-a保存所有改动，-p交互式保存，-k不保存暂存区的改动</p><p>恢复工作区和暂存区：<code>stash pop</code> 恢复并删除栈顶的stash，<code>stash apply</code> 恢复但不删除栈顶的stash</p><h3 id="Stash">Stash</h3><p>显示栈：<code>stash list</code></p><p>入栈：<code>stash push</code> 默认入栈顶</p><p>显示栈的细节：<code>stash show</code> 默认栈顶</p><p>删除当个栈：<code>stash drop</code> 默认栈顶</p><p>清空整个stash：<code>stash clear</code></p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TVM源码-00</title>
    <link href="/2023/11/16/TVM%E6%BA%90%E7%A0%81-00/"/>
    <url>/2023/11/16/TVM%E6%BA%90%E7%A0%81-00/</url>
    
    <content type="html"><![CDATA[<p>TVM v0.8 源码学习</p><span id="more"></span><h2 id="代码拉取">代码拉取</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --recursive https://github.com/apache/tvm.git<br>git checkout v0.8<br></code></pre></td></tr></table></figure><h2 id="编译安装">编译安装</h2><h3 id="编译常识">编译常识</h3><h4 id="GCC">GCC</h4><p>对于小项目来说，文件数量较少，使用GCC直接进行编译即可</p><ol><li>g++:将源文件编译成.out可执行文件</li><li>g++ -c:将源文件编译成中间文件 而不进行链接，即编译成.o文件</li><li>g++ -o:将源文件或者.o文件进行编译+链接或链接，生成.out文件</li><li>对于包含多个源文件的项目，可以将源文件分别gcc -c，再将产生的文件链接到一起，如<code>g++ -o myprogram file1.o file2.o</code></li></ol><h4 id="make-makefile">make &amp; makefile</h4><p>然而随着计算机的发展，一个软件工程包含的源文件越来越多，手动逐个编译完全不可行，于是有个make和makefile。</p><p>Make 是一个批处理工具，它根据 Makefile 文件中的规则来构建项目。Make 可以确定哪些文件需要重新编译，哪些文件已经是最新的，从而只编译需要编译的文件。</p><p>Makefile：Makefile 是 Make 的配置文件，它包含了一系列的规则，用于指定如何构建项目。Make 通过读取 Makefile 文件来构建项目。</p><p>在这一阶段，工程师可以手写项目的makefile文件，再使用make指令统一构建整个项目</p><h4 id="Cmake-CMakeLists">Cmake &amp; CMakeLists</h4><p>makefile在一些简单的工程下，完全可以人工手写，但是问题又来了，工程非常大的时候，连 makefile 的手写也非常麻烦，这时就需要一个工具可以自动生成 makefile ，这个工具就是cmake。</p><p>CMakeLists 是 Cmake 的配置文件。还是需要手写。</p><p>Cmake 会根据 CMakeLists 自动生成项目的 makefile 文件，然后再使用 make 构建项目。</p><p>Cmake 有不同的生成器，可以生成不同平台下的 makefile 文件，比如 Unix Makefile、Visual Studio、Ninja、Nmake等等，可以生成不同平台下的makefile，生成后再进行make就可以将项目构建在不同的平台下。</p><p>ninja是一种注重速度的生成器，使用ninja生成会产生一个build.ninja文件，然后使用ninja而非make进行构建</p><h3 id="编译TVM">编译TVM</h3><p>首先在 <a href="https://winlibs.com/">https://winlibs.com/</a> 拿到带 LLVM 库的 GCC 包，安装并加入环境变量，在cmd中可以使用 <code>llvm-config --libdir</code> 验证</p><p>创建build目录并<code>cp cmake/config.cmake build</code>并自定义配置，把USE LLVM打开</p><p>Conda创建tvm的虚拟环境，可以直接 <code>conda env create --file conda/build-environment.yaml</code> 但是使用该环境 git 会莫名奇妙出bug，不如手动创环境下载该文件中提到的依赖包</p><p>安装visual studio，把桌面C++开发组件勾上，安装完成后在cmd中使用<code>cl</code>验证（有没有可能除了装一个vs上述步骤全部不需要，但是不管了，官方文档安装部分相当混乱）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> build<br><span class="hljs-built_in">cd</span> build<br>cmake -A x64 -Thost=x64 ..<br><span class="hljs-built_in">cd</span> ..<br>cmake --build build --config Release -- /m<br></code></pre></td></tr></table></figure><h3 id="安装python包">安装python包</h3><p>在import tvm时如果未找到包，vscode会自动在工作区下创建配置帮你把tvm的路径加到 <code>python.analysis.extraPaths</code> 但实测虽然变绿且鼠标可以左键跳转了，但解释器还是找不到，可能需要改全局的python路径啥的</p><p>不如直接安装，环境变量引入包的好处在于源码更改后，引入可以立即感知；而install的包在每次源码更改后要重新install才能生效。但是我使用的v0.8的源码，已经没有更新了，所以直接安装也ok</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> python<br>python setup.py install<br></code></pre></td></tr></table></figure><h2 id="用户手册">用户手册</h2><p>tvm/gallery</p><h3 id="introduction-py"><a href="http://introduction.py">introduction.py</a></h3><p><img src="image.png" alt="Alt text"></p><p>TVM编译步骤：</p><ol><li><p>从 TensorFlow、PyTorch 或 ONNX 等框架导入模型。在导入阶段中，TVM 可以从其他框架（如 TensorFlow、PyTorch 或 ONNX）中提取模型。 TVM 为前端提供的支持水平会随着我们不断改进这个开源项目而变化。如果在将模型导入 TVM 时遇到问题，可以将其转换为 ONNX。</p></li><li><p>翻译成 TVM 的高级模型语言 Relay。已导入 TVM 的模型在 Relay 中表示。Relay 是神经网络的功能语言和中间表示（IR）。Relay 应用图级优化 pass 来优化模型。</p></li><li><p>降级为张量表达式（TE）表示。降级是指将较高级的表示转换为较低级的表示。应用了高级优化之后，Relay 通过运行 FuseOps pass，把模型划分为许多小的子图，并将子图降级为 TE 表示。张量表达式（TE）是一种用于描述张量计算的领域特定语言。 TE 还提供了几个 schedule 原语来指定底层循环优化，例如循环切分、矢量化、并行化、循环展开和融合。为将 Relay 表示转换为 TE 表示，TVM 包含了一个张量算子清单（TOPI），其中包含常用张量算子的预定义模板（例如，conv2d、transpose）。</p></li><li><p>使用 auto-tuning 模块 AutoTVM 或 AutoScheduler 搜索最佳 schedule。schedule 为 TE 中定义的算子或子图指定底层循环优化。auto-tuning 模块搜索最佳 schedule，并将其与 cost model 和设备上的测量值进行比较。TVM 中有两个 auto-tuning 模块，AutoTVM（有模板）和Ansor（无模板）。</p></li><li><p>为模型编译选择最佳配置。调优后，auto-tuning 模块会生成 JSON 格式的调优记录。此步骤为每个子图选择最佳 schedule。</p></li><li><p>降级为张量中间表示（TIR，TVM 的底层中间表示）。基于调优步骤选择最佳配置后，所有 TE 子图降级为 TIR 并通过底层优化 pass 进行优化。接下来，优化的 TIR 降级为硬件平台的目标编译器。这是生成可部署到生产的优化模型的最终代码生成阶段。</p></li><li><p>编译成机器码。compiler-specific 的生成代码最终可降级为机器码。 TVM 可将模型编译为可链接对象模块，然后轻量级 TVM runtime 可以用 C 语言的 API 来动态加载模型，也可以为 Python 和 Rust 等其他语言提供入口点。或将 runtime 和模型放在同一个 package 里时，TVM 可以对其构建捆绑部署。</p></li></ol><h3 id="autotvm-relay-x86-py">autotvm_relay_x86.py</h3><p>使用TVM的Python API编译、训练和调优与训练的模型。</p><h4 id="导入依赖，并下载和加载ONNX模型">导入依赖，并下载和加载ONNX模型</h4><p>使用ResNet-50 v2，一个50层的用于图像分类的卷积神经网络，可以使用<a href="https://netron.app/">Netron</a>检查模型结构。输入图像为224×224，模型已经预训练好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> onnx<br><span class="hljs-keyword">from</span> tvm.contrib.download <span class="hljs-keyword">import</span> download_testdata<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tvm.relay <span class="hljs-keyword">as</span> relay<br><span class="hljs-keyword">import</span> tvm<br><span class="hljs-keyword">from</span> tvm.contrib <span class="hljs-keyword">import</span> graph_executor<br><br>model_url = (<br>    <span class="hljs-string">&quot;https://github.com/onnx/models/raw/main/&quot;</span><br>    <span class="hljs-string">&quot;vision/classification/resnet/model/&quot;</span><br>    <span class="hljs-string">&quot;resnet50-v2-7.onnx&quot;</span><br>)<br><br>model_path = download_testdata(model_url, <span class="hljs-string">&quot;resnet50-v2-7.onnx&quot;</span>, module=<span class="hljs-string">&quot;onnx&quot;</span>)<br>onnx_model = onnx.load(model_path)<br><br><span class="hljs-comment"># 为 numpy 的 RNG 设置 seed，保证每次复现得到一致的结果</span><br>np.random.seed(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="下载和预处理图像">下载和预处理图像</h4><p>将图像转为Numpy数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">img_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&quot;</span><br>img_path = download_testdata(img_url, <span class="hljs-string">&quot;imagenet_cat.png&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-comment"># 重设大小为 224x224</span><br>resized_image = Image.<span class="hljs-built_in">open</span>(img_path).resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br>img_data = np.asarray(resized_image).astype(<span class="hljs-string">&quot;float32&quot;</span>)<br><br><span class="hljs-comment"># 输入图像是 HWC 布局，而 ONNX 需要 CHW 输入，所以转换数组</span><br>img_data = np.transpose(img_data, (<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br><br><span class="hljs-comment"># 根据 ImageNet 输入规范进行归一化</span><br>imagenet_mean = np.array([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>]).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>imagenet_stddev = np.array([<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>norm_img_data = (img_data / <span class="hljs-number">255</span> - imagenet_mean) / imagenet_stddev<br><br><span class="hljs-comment"># 添加 batch 维度，期望 4 维输入：NCHW。</span><br><span class="hljs-comment"># N:number of samples，即batch size C:channels H:height W:width</span><br>img_data = np.expand_dims(norm_img_data, axis=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="使用Relay编译模型">使用Relay编译模型</h4><p>将模型转为Relay中间表示（IR）。首先用 from_onnx 导入器将模型导入到 Relay 中。然后，用标准优化，将模型构建到 TVM 库中，最后从库中创建一个 TVM 计算图 runtime 模块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入名称可能因模型类型而异</span><br><span class="hljs-comment"># 可用 Netron 工具检查输入名称</span><br>input_name = <span class="hljs-string">&quot;data&quot;</span><br>target = <span class="hljs-string">&quot;llvm&quot;</span><br><br><span class="hljs-comment"># 定义输入的形状字典，键是模型固定的输入名称，值是输入形状，用于指定ONNX模型的输入形状</span><br>shape_dict = &#123;input_name: img_data.shape&#125;<br><br><span class="hljs-comment"># 将ONNX模型转换为Relay中间表示（IR）</span><br>mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)<br><br><span class="hljs-comment"># 构建和编译模型</span><br><span class="hljs-keyword">with</span> tvm.transform.PassContext(opt_level=<span class="hljs-number">3</span>):<br>    <span class="hljs-comment"># 使用Relay构建模型并编译为目标为&quot;llvm&quot;的库</span><br>    lib = relay.build(mod, target=target, params=params)<br><br><span class="hljs-comment"># 创建&quot;llvm&quot;设备</span><br>dev = tvm.device(target, <span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 创建GraphModule并加载编译好的模块</span><br>module = graph_executor.GraphModule(lib[<span class="hljs-string">&quot;default&quot;</span>](dev))<br></code></pre></td></tr></table></figure><p><code>relay.frontend.from_onnx</code>接受一个ONNX模型和一个形状字典作为输入，返回一个Relay模块（mod）和一个参数字典（params）</p><p>mod是整个模型的计算图，params是模型的参数字典，包含了模型的权重和偏置等参数</p><p><code>tvm.transform.PassContext</code>是TVM控制优化过程的上下文，<code>opt_level=3</code>代表启用所有推荐优化<br>lib是编译后生成的模块库，包含模型计算图、参数和编译后的函数，它将可以被加载到一个GraphModule中，并在指定设备上执行</p><p><code>dev = tvm.device(target, 0)</code>和<code>module = graph_executor.GraphModule(lib[&quot;default&quot;](dev))</code> 分别创建了一个device对象和一个GraphModule对象（图执行模块）<br>可以在上面运行上面编译好的lib库</p><h4 id="使用模型进行推理">使用模型进行推理</h4><p>直接运行模型进行预测，并将输出转为可读形式。并收集此时还未优化过的模型基本性能数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 运行模型</span><br>dtype = <span class="hljs-string">&quot;float32&quot;</span><br>module.set_input(input_name, img_data)<br>module.run()<br>output_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br>tvm_output = module.get_output(<span class="hljs-number">0</span>, tvm.nd.empty(output_shape)).numpy()<br><br><span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> softmax<br><br><span class="hljs-comment"># 下载标签列表</span><br>labels_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;</span><br>labels_path = download_testdata(labels_url, <span class="hljs-string">&quot;synset.txt&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(labels_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    labels = [l.rstrip() <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> f]<br><br><span class="hljs-comment"># 打开输出文件并读取输出张量</span><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br><br><span class="hljs-comment"># 运行时间评估</span><br><span class="hljs-keyword">import</span> timeit<br>timing_number = <span class="hljs-number">10</span><br>timing_repeat = <span class="hljs-number">10</span><br>unoptimized = (<br>    np.array(timeit.Timer(<span class="hljs-keyword">lambda</span>: module.run()).repeat(repeat=timing_repeat, number=timing_number))<br>    * <span class="hljs-number">1000</span><br>    / timing_number<br>)<br>unoptimized = &#123;<br>    <span class="hljs-string">&quot;mean&quot;</span>: np.mean(unoptimized),<br>    <span class="hljs-string">&quot;median&quot;</span>: np.median(unoptimized),<br>    <span class="hljs-string">&quot;std&quot;</span>: np.std(unoptimized),<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="输出后处理">输出后处理</h4><p>用专为该模型提供的查找表，运行一些后处理（post-processing），从而使得 ResNet-50 v2 的输出形式更具有可读性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> softmax<br><br><span class="hljs-comment"># 下载标签列表</span><br>labels_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;</span><br>labels_path = download_testdata(labels_url, <span class="hljs-string">&quot;synset.txt&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(labels_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    labels = [l.rstrip() <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> f]<br><br><span class="hljs-comment"># 打开输出文件并读取输出张量</span><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br></code></pre></td></tr></table></figure><h4 id="进行模型调优">进行模型调优</h4><p>用编译的模块推理，有时可能无法获得预期的性能。在这种情况下，可用自动调优器更好地配置模型，从而提高性能。 TVM 中的调优是指，在给定 target 上优化模型，使其运行得更快。与训练或微调不同，它不会影响模型的准确性，而只会影响 runtime 性能。作为调优过程的一部分，TVM 实现并运行许多不同算子的变体，以查看哪个性能最佳。这些运行的结果存储在调优记录文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tvm.auto_scheduler <span class="hljs-keyword">as</span> auto_scheduler<br><span class="hljs-keyword">from</span> tvm.autotvm.tuner <span class="hljs-keyword">import</span> XGBTuner<br><span class="hljs-keyword">from</span> tvm <span class="hljs-keyword">import</span> autotvm<br><br>number = <span class="hljs-number">10</span><br>repeat = <span class="hljs-number">1</span><br>min_repeat_ms = <span class="hljs-number">0</span>  <span class="hljs-comment"># 调优 CPU 时设置为 0</span><br>timeout = <span class="hljs-number">10</span>  <span class="hljs-comment"># 秒</span><br><br><span class="hljs-comment"># 创建 TVM 运行器</span><br>runner = autotvm.LocalRunner(<br>    <span class="hljs-comment"># 将要测试的不同配置的数量</span><br>    number=number,<br>    <span class="hljs-comment"># 每个配置重复次数</span><br>    repeat=repeat,<br>    <span class="hljs-comment"># 每次测试运行时间上限</span><br>    timeout=timeout,<br>    <span class="hljs-comment"># 指定运行配置测试需要多长时间，如果重复次数低于此时间，则增加其值</span><br>    min_repeat_ms=min_repeat_ms,<br>    enable_cpu_cache_flush=<span class="hljs-literal">True</span>,<br>)<br><br>tuning_option = &#123;<br>    <span class="hljs-string">&quot;tuner&quot;</span>: <span class="hljs-string">&quot;xgb&quot;</span>,<br>    <span class="hljs-comment"># 试验次数，CPU上推荐1500，GPU推荐3000-4000，此处仅作展示用</span><br>    <span class="hljs-string">&quot;trials&quot;</span>: <span class="hljs-number">10</span>,<br>    <span class="hljs-comment"># 使搜索提前停止的实验最小值</span><br>    <span class="hljs-string">&quot;early_stopping&quot;</span>: <span class="hljs-number">100</span>,<br><br>    <span class="hljs-string">&quot;measure_option&quot;</span>: autotvm.measure_option(<br>        builder=autotvm.LocalBuilder(build_func=<span class="hljs-string">&quot;default&quot;</span>),<br>        runner=runner<br>    ),<br>    <span class="hljs-comment"># 调优数据保存的文件名</span><br>    <span class="hljs-string">&quot;tuning_records&quot;</span>: <span class="hljs-string">&quot;resnet-50-v2-autotuning.json&quot;</span>,<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先从 onnx 模型中提取任务</span><br>tasks = autotvm.task.extract_from_program(mod[<span class="hljs-string">&quot;main&quot;</span>], target=target, params=params)<br><br><span class="hljs-keyword">for</span> i, task <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tasks):<br>    prefix = <span class="hljs-string">&quot;[Task %2d/%2d] &quot;</span> % (i + <span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(tasks))<br>    tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank&quot;</span>)<br>    tuner_obj.tune(<br>        n_trial=<span class="hljs-built_in">min</span>(tuning_option[<span class="hljs-string">&quot;trials&quot;</span>], <span class="hljs-built_in">len</span>(task.config_space)),<br>        early_stopping=tuning_option[<span class="hljs-string">&quot;early_stopping&quot;</span>],<br>        measure_option=tuning_option[<span class="hljs-string">&quot;measure_option&quot;</span>],<br>        callbacks=[<br>            autotvm.callback.progress_bar(tuning_option[<span class="hljs-string">&quot;trials&quot;</span>], prefix=prefix),<br>            autotvm.callback.log_to_file(tuning_option[<span class="hljs-string">&quot;tuning_records&quot;</span>]),<br>        ],<br>    )<br></code></pre></td></tr></table></figure><h4 id="使用生成的调优数据优化编译模型">使用生成的调优数据优化编译模型</h4><p>获取上述存储在<code>resnet-50-v2-autotuning.json</code> 中的调优记录，并使用该结果为指定target上的模型生成高性能代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> autotvm.apply_history_best(tuning_option[<span class="hljs-string">&quot;tuning_records&quot;</span>]):<br>    <span class="hljs-keyword">with</span> tvm.transform.PassContext(opt_level=<span class="hljs-number">3</span>, config=&#123;&#125;):<br>        lib = relay.build(mod, target=target, params=params)<br><br>dev = tvm.device(<span class="hljs-built_in">str</span>(target), <span class="hljs-number">0</span>)<br>module = graph_executor.GraphModule(lib[<span class="hljs-string">&quot;default&quot;</span>](dev))<br><br><span class="hljs-comment"># 验证模型是否产生相同的结果</span><br>dtype = <span class="hljs-string">&quot;float32&quot;</span><br>module.set_input(input_name, img_data)<br>module.run()<br>output_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br>tvm_output = module.get_output(<span class="hljs-number">0</span>, tvm.nd.empty(output_shape)).numpy()<br><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br></code></pre></td></tr></table></figure><h4 id="比较调优前后的模型性能">比较调优前后的模型性能</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> timeit<br><br>timing_number = <span class="hljs-number">10</span><br>timing_repeat = <span class="hljs-number">10</span><br>optimized = (<br>    np.array(timeit.Timer(<span class="hljs-keyword">lambda</span>: module.run()).repeat(repeat=timing_repeat, number=timing_number))<br>    * <span class="hljs-number">1000</span><br>    / timing_number<br>)<br>optimized = &#123;<span class="hljs-string">&quot;mean&quot;</span>: np.mean(optimized), <span class="hljs-string">&quot;median&quot;</span>: np.median(optimized), <span class="hljs-string">&quot;std&quot;</span>: np.std(optimized)&#125;<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;optimized: %s&quot;</span> % (optimized))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;unoptimized: %s&quot;</span> % (unoptimized))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TVM</tag>
      
      <tag>编译</tag>
      
      <tag>cmake</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Deep Learning Compiler: A Comprehensive Survey</title>
    <link href="/2023/10/31/The-Deep-Learning-Compiler-A-Comprehensive-Survey/"/>
    <url>/2023/10/31/The-Deep-Learning-Compiler-A-Comprehensive-Survey/</url>
    
    <content type="html"><![CDATA[<p>事已至此，先看论文吧</p><p><em>The Deep Learning Compiler: A Comprehensive Survey</em></p><p>2020的一篇有关AI编译器的综述，来自北航和清华</p><span id="more"></span><h1><em>The Deep Learning Compiler: A Comprehensive Survey</em></h1><h2 id="Abstract">Abstract</h2><p>由于AI芯片的高度定制化，使得在不同硬件上部署各种深度学习模型变得十分困难，这也推动了深度学习编译器的研究。业界因此推出了一些深度学习编译器，如Tensorflow XLA和TVM。与传统编译器类似的是，深度学习编译器将不同的深度学习框架中描述的深度学习模型作为输入，然后为不同的硬件生成优化后的代码作为输出。</p><p>然而现有的文章都没有全面分析深度学习编译器独特的设计架构。本文将对现有DL编译器做全面剖析，重点在DL的多级IR及前/后端优化。</p><p>这是第一篇关于DL编译器设计体系结构的综述性论文。</p><h2 id="1-Introduction">1. Introduction</h2><p>先讲深度学习对于各个领域的深远影响blablabla</p><p>然后说当前业界的几种主流框架，如TensorFlow、PyTorch，MXNet和CNTK等，同时提出这些框架如果需要支持新的模型，interoperability（互操作性，复用性？）会变得非常重要。为了提供复用性，我们又提出了ONNX，定义了一种DL模型的统一格式，以促进不同框架间模型的相互转换。</p><p>然后说各大公司对开发DL专用硬件的巨大投入。在可预见的未来，深度学习芯片的设计会变得越来越多样化。</p><p>要包容硬件的多样性，就必须将计算高效的映射到各种硬件上。在通用硬件上，一些专用的依赖库可以实现DL模型的高效计算，在许多专用的DL芯片上也有类似的库。但是依赖库的缺点是，库的开发和更新通常跟不上DL模型的飞速发展，无法真正有效的利用DL芯片。</p><p>为了解决DL库和其他工具的类似缺点，并减轻DL芯片上每个模型都需要手动优化的负担，DL社区开始发展DL编译器。针对不同的模型和硬件架构，DL编译器对模型定义到特定代码实现之间的转换进行的高度优化。同时还利用通用编译器如LLVM的成熟工具链，在各种硬件架构间提供更好的可移植性。与传统编译器类似，DL编译器也采用分层设计，包括前端、多级IR和后端。</p><p>本文贡献：blablabla</p><h2 id="2-Background">2. Background</h2><h3 id="2-1-深度学习框架">2.1 深度学习框架</h3><p>介绍目前流行的几种框架</p><h3 id="2-2-深度学习硬件">2.2 深度学习硬件</h3><p>当前AI芯片的粗略分类：</p><ol><li>通用AI硬件：GPGPU，如Nvidia的Volta架构，辅以深度学习加速库如cuDNN，已经一些TensorRT之类的库</li><li>专用AI硬件：如Google TPU</li><li>Neuromorphic Hardware：略略略</li></ol><h3 id="2-3-硬件相关的DL代码生成器">2.3 硬件相关的DL代码生成器</h3><p>讲FPGA，略略略</p><h2 id="3-COMMON-DESIGN-ARCHITECTURE-OF-DL-COMPILERS">3. COMMON DESIGN ARCHITECTURE OF DL COMPILERS</h2><p><img src="image.png" alt="Alt text"></p><p>DL编译器的通用架构一般包含两部分：编译器前端和后端，中间表示IR在前后端之间，处理优化工作。其中IR都分多级，高级IR用于前端，偏向硬件无关的转换和优化；低级IR用于后端，偏向硬件相关的转换和优化、代码生成、编译等</p><p>高阶IR即图IR，用于表示硬件无关的计算和控制流，设计难点在于计算和控制流的抽象。有了这种抽象能力就可以捕获和表示各种DL模型。其目标在于建立控制流以及算子与数据之间的依赖关系，并为图级优化提供接口。此外高阶IR还需要包含编译所需的语义信息，并为自定义算子提供可扩展性。</p><p>低阶IR用于硬件相关的优化和代码生成。因此，低阶IR更注重细节，反映硬件特性，准确表示硬件相关优化。并且应该能够在后端使用成熟的第三方工具链。</p><p>前端从DL框架获取模型作为输入，然后将其转化为计算图表示形式。为了支持不同的框架，前端需要支持并实现不同格式的相互转换。计算图的优化可以分为节点级别（消除nop和零维张量）、块级别（代数简化、算子融合）、数据流级别（CSE、DCE、静态内存规划）。生成的优化计算图会传递给后端。</p><p>后端收到高阶IR（计算图）后，将高阶IR转换为LLVM IR等第三方工具链，这样就可以利用已有工具完成通用优化和代码生成。此外，后端还可以利用DL模型和硬件特性的先验知识来优化代码生成，如硬件固有映射、内存分配、内存延迟隐藏、并行化、循环优化等。为了在大的优化空间中确定最佳参数，现有的DL编译器广泛采用两种方法，<strong>Auto-Tuing（如AutoTVM）和Auto-Scheduling（如AutoScheduler）</strong>。优化后的低阶IR再经过JIT或AOT编译，生成面向不同硬件目标的机器码。</p><h2 id="4-KEY-COMPONENTS-OF-DL-COMPILERS">4. KEY COMPONENTS OF DL COMPILERS</h2><h3 id="4-1-高阶IR">4.1 高阶IR</h3><p>传统编译器中采用的IR表示能力限制了DL模型中复杂计算的表达，现在的DL编译器都会采用高阶IR（称为图IR）和一些特殊设计，以求达到高效的代码优化。为了更好地理解DL编译器中使用的图IR，以下描述了图IR的表示和实现。</p><h4 id="4-1-1-图IR的表示形式">4.1.1 图IR的表示形式</h4><blockquote><p>图IR的表示方式决定了DL编译器分析图IR的方式</p></blockquote><p><strong>基于DAG的IR</strong></p><p>DAG即有向无环图，在DL编译器中，DAG的节点表示算子，边表示张量。通用编译器一般使用DDG即数据依赖图完成如公共子表达式消除CSE和死代码消除DCE之类的优化，而借助DAG，DL编译器同样可以实现这些。</p><p>基于DAG的IR由于表达方式的简单，便于编程和编译，但由于基于DAG的IR缺少计算范围定义，因而存在诸如语义二义性这类缺陷。</p><p><strong>基于Let-binding的IR</strong></p><p>基于Let绑定的IR-Let绑定是一种解决语义歧义的方法，它为Javascript等许多高级编程语言使用的某些范围有限的函数提供Let表达式。当使用let关键字定义表达式时，会生成一个let节点，然后它指向表达式中的运算符和变量，而不仅仅是将变量之间的计算关系构建为DAG。在基于DAG的编译器中，当进程需要获得一个表达式的返回值时，它首先访问相应的节点并搜索相关的节点，也称为递归下降技术。相反，基于let绑定的编译器计算出let表达式中变量的所有结果，并构建变量映射。当需要特定的结果时，编译器会查找此映射来决定表达式的结果。在DL编译器中，TVM的Relay IR同时采用了基于DAG的IR和基于let绑定的IR，以获得两者的好处。</p><p><strong>张量的计算表示</strong></p><p>不同的图IR表示张量计算的方式也不同</p><ol><li>基于函数的表示：XLA、nGraraph</li><li>Lambda表示：TVM</li><li>爱因斯坦符号表示</li></ol><h4 id="4-1-2-图IR的实现">4.1.2 图IR的实现</h4><p>DL编译器中的数据通常以张量的形式进行组织，也即多维数组。编译器可以通过内存指针访问张量，或通过更灵活的占位符方式表示。占位符需要包含张量每个维度的大小，有时也可以标记成未知。此外，出于优化的原因，DL编译器需要数据布局信息，也应该可以根据占位符推断迭代器的边界。</p><p><strong>占位符</strong></p><p>占位符广泛应用于符号编程，如Tensorflow。占位符用于提供数据给计算图，是一种具有明确形状信息的变量。无初始值，声明只分配必要内存。有助于计算与编译器执行的分离。</p><p><strong>未知形状表示</strong></p><p>与Tensorflow类似，TVM使用any表示未知维度。未知形状的表示对动态模型必不可少，但是要完全支持动态模型，还要放宽边界推断和维度检查，以及额外的机制来确保内存有效性。</p><p><strong>数据布局</strong></p><p>张量都是放在特定数据布局和形状下。数据布局用来描述张量在内存中的组织方式，一般是一个从逻辑索引到内存索引的映射方式。合适的数据布局对性能提升非常关键，特别是对于深度学习模型之类内存密集型的应用而言。数据布局通常包括维度顺序（NCHW or NHWC）、平铺（tiling）、填充（padding）、跨距（striding）等。</p><p>TVM和GLOW将数据布局当作一种算子的参数，因为计算和优化需要这些参数信息。</p><p><strong>边界推断</strong></p><p>尽管DL编译器使用张量的数据表示可以很方便的描述输入和输出，但这种方式在推断迭代器边界时会有困难。边界推断通常根据计算图和已知占位符，以迭代或递归的方式执行</p><p><strong>算子支持</strong></p><p>算子就是计算图中的节点，通常包含代数算子（加减乘除等）、神经网络算子（卷积、池化等）、张量算子（reshape、resize等）、广播\规约算子（min、argmin等），以及控制流算子（条件和循环等），下面对三个特定的算子进行说明：</p><ol><li>广播算子：。。。</li><li>控制流算子：任何控制流都可以通过递归和模式来实现，正因为这一点，Relay可通过函数式编程来描述复杂的深度神经网络。</li><li>Derivative算子：略略略 看不明白</li></ol><p><strong>算子定制</strong></p><p>程序员可以定制算子。</p><h4 id="4-1-3-讨论">4.1.3 讨论</h4><p>几乎所有的DL编译器都有其独特的高阶IR，重要的是高阶IR与硬件无关。</p><h3 id="4-2-低阶IR">4.2 低阶IR</h3><h4 id="4-2-1-实现">4.2.1 实现</h4><p>与高阶IR相比，低阶IR用更细的粒度描述了DL模型的计算，并提供计算调优和内存访问接口，实现目标相关的优化。本节将低阶IR分为三类：基于Halide的IR，基于多面体的IR和其他IR。</p><p><strong>基于Halide的IR</strong></p><p>TVM将Halide IR改进为独立的符号IR，主要的两点：首先，TVM消除了对LLVM的依赖，并重构了项目模块和Halide IR设计的结构，优化了代码组织，使得图IR和前端语言（如Python）更易于理解，并且通过运行时分发机制，可以方便地添加自定义算子。如此一来，可重用性也得到了改善。 其次，TVM还简化了从字符串匹配到指针匹配的变量定义，确保每个变量都具有一个定义位置，即，静态单赋值（Static Single-Assignment, SSA）。</p><p><strong>基于多面体的IR</strong></p><p>多面体（Polyhedral-based）模型是在某些DL编译器采用的一项重要技术。多面体模型使用线性编程、仿射变换和其它数学方法来优化基于loop循环的代码，代码可以具有边界和分支的静态控制流。与Halide相比，内存引用和循环嵌套的边界可以是多面体模型中具有任何形状的多面体。这种灵活性使多面体模型在通用编译器中得到广泛使用。但是，这种灵活性也妨碍了多面体模型与调优机制的集成。但是，由于能够处理深度嵌套的循环，很多DL编译器（例如TC和PlaidML）都采用了多面体模型作为其低阶IR。基于多面体的IR可以很容易地应用于各种多面体转换（例如融合、平铺、下沉和映射），不论是设备相关还是设备不相关的优化。基于多面体的编译器兼容很多其它工具链，例如isl、Omega、PIP、Polylib和PPL。</p><p>TC的低阶IR设计较为独特，其设计结合了Halide和多面体模型，也就是用基于Halide的IR表示计算，用基于多面体的IR表示loop结构。 TC通过抽象实例表示表达式，并引入了特定的节点类型。简而言之，TC用域节点来指定索引变量的范围，用上下文节点描述与硬件相关的、新的迭代变量，用band节点确定迭代顺序，用过滤器节点表示与语句实例结合的迭代器，用Set和sequence是指定过滤器的执行类型（并行和串行执行），用扩展节点来描述代码生成所需的其它必要指令，例如内存搬移。</p><p>PlaidML使用基于多面体的IR（称为Stripe）来表示张量操作，并且通过将并行多面体块的嵌套挤结构扩展到多个级别，创建可并行化代码的层次结构。PlaidML可以将嵌套的多面体分配给嵌套的内存单元，做到计算与存储层次结构的匹配。Stripe中的硬件配置独立于内核代码。 Stripe中的标记tag（在其他编译器中称为pass）不会更改内核结构，但会为优化pass提供硬件目标的其它信息。 Stripe将DL算子拆分为适合本地硬件资源的块（tile）。</p><p><strong>其他IR</strong></p><p>有些DL编译器无需使用Halide和多面体模型就可实现定制低阶IR。定制低阶IR将硬件相关的优化和降级操作应用到LLVM IR上。</p><p>Glow中的低阶IR是基于指令的表达式，可对通过地址引用的张量进行操作。Glow低阶IR中有两种基于指令的函数：声明和程序。声明用于表示在程序的整个生命周期都存在的常量内存区域数量（例如，输入、权重、偏置等）。程序是一系列本地分配的区域，包括函数（比如conv和pool）和临时变量。指令可以在全局内存区域或本地分配的区域上运行。此外，每个操作数都用一个限定符修饰。@in表示操作数从缓冲区中读取；@out表示操作数写入缓冲区；@inout表示操作数读取和写入缓冲区。这些指令和操作数限定符可帮助Glow确定在什么时候可以执行什么样的内存优化。</p><p>MLIR受LLVM的影响很大，但MLIR是一个比LLVM更纯粹的编译器基础结构。MLIR复用了LLVM中的许多想法和接口，其定位在模型表示和代码生成之间。MLIR有灵活的类型系统和多层抽象级别，并引入了dialect来表示这些抽象级别。每个dialect都由一组定义好的不可变操作组成。MLIR的当前dialect包括TensorFlow IR、XLA HLO IR、多面体IR、LLVM IR和TensorFlow Lite，并支持dialect之间的转换。此外，MLIR可以创建新的dialect连接到低阶编译器。</p><p>XLA的HLO IR既可以被视为高阶IR，也可以被视为低阶IR，因为HLO的粒度足以表示硬件信息。此外，HLO支持硬件相关优化，并可用于生成LLVM IR。</p><h4 id="4-2-2-基于低阶IR的代码生成">4.2.2 基于低阶IR的代码生成</h4><p>大多数DL编译器采用的低阶IR最终可以降级到LLVM IR，并利用LLVM成熟的优化器和代码生成器产生机器码。此外，LLVM可以从0开始，为专用加速器设计定制指令集。但是，如果直接将低阶IR转换为LLVM IR，传统的编译器生成的代码质量可能较差。为了避免这种情况，DL编译器采用了两种方法来实现硬件相关优化：</p><ol><li>在LLVM的上层IR（例如，基于Halide的IR和基于多面体的IR）中执行目标相关的loop循环转换</li><li>为优化pass提供更多硬件目标相关信息，帮助改善优化效果。</li></ol><p>大多数DL编译器都应用这两种方法，但是侧重点有所不同。通常，偏向前端用户的DL编译器（例如TC、TVM、XLA和nGraph）可能专注于第一点，而偏向后端开发人员的DL编译器（例如Glow、PlaidML和MLIR）可能专注于第二点。</p><p>DL编译器中的编译方案主要可分为两类：JIT（Just-In-Time）和AOT（Ahead-Of-Time）。JIT编译器可以即时生成可执行代码，并且可以利用运行时信息优化代码。 AOT编译器首先生成所有可执行二进制文件，然后再执行。因此， AOT编译器的静态分析范围比JIT编译器更大。此外，AOT方法可以用于嵌入式平台的交叉编译器，并可以在远程计算机和定制加速器上执行。</p><h4 id="4-2-3-讨论">4.2.3 讨论</h4><p>在DL编译器中，低阶IR是DL模型的细粒度表示，主要描述了DL模型移植到各种不同硬件上所需的详细信息。低阶IR包括基于Halide IR、基于多面体IR和其它IR。尽管这些低阶IR在设计上有所不同，但是都通过已有的成熟编译器工具链和基础结构，提供硬件相关的优化和代码生成接口。低阶IR的设计也会影响到DL加速器的设计（例如TVM Halide IR和Inferentia，以及XLA HLO和TPU）。</p><h3 id="4-3-前端优化">4.3 前端优化</h3><p>构造计算图后，编译器前端就可以开始执行图级优化。由于计算图提供了计算的全局视图，因此很容易在图这个级别识别判断并执行各种优化方法。这些优化方法与硬件无关，仅适用于计算图，不适用于后端实现。</p><p>前端优化通常通过pass来定义，用于遍历计算图的节点并执行图转换。前端功能主要包括两个：第一，从计算图捕获特定特征；第二，重写图以便进行优化。除了预定义pass，开发者也可以在前端定制pass。DL模型导入并转换为计算图后，大多数DL编译器就可以确定操作的输入输出张量的形状</p><p>在本节中，我们将前端优化分为三类：节点级优化、块级（窥孔、局部）优化和数据流级（全局）优化。</p><h4 id="4-3-1-节点级">4.3.1 节点级</h4><p>节点级优化包括消除不必要的节点（即节点消除），以及将节点替换为其它低成本节点（即节点替换），如消除只有一个输入的求和节点，或某个输入为零维张量的求和节点，以及一些零填充宽度的填充节点。</p><h4 id="4-3-2-块级优化">4.3.2 块级优化</h4><p><strong>代数简化</strong></p><p>包括代数识别、强度消减和常量折叠。</p><p>代数识别：利用不同类型节点的交换律、结合律和分配律来简化计算</p><p>强度消减：用便宜的算子代替更昂贵的算子</p><p>常量折叠：使用常量代替常量表达式</p><p><strong>算子融合</strong></p><p>核心思想是将多个算子合并为一个内核，这样无需将中间结果写回内存，减少了中间变量的分配时间。</p><p><strong>算子下沉</strong></p><p>这种优化方法将诸如转置之类的运算下沉到批标准化（batch normalization）、ReLU、Sigmoid和通道混洗（shuffle）之类的运算之下。通过这种优化，许多彼此相似的操作相互靠近，从而为代数简化优化创造更多机会。</p><h4 id="4-3-3-数据流级优化">4.3.3 数据流级优化</h4><p><strong>CSE：公共子表达式消除</strong></p><p>如果某表达式的值已经得到，并且在后续计算中不会再改变，就称为公共子表达式。DL编译器会在整个计算图中搜索该公共子表达式，并使用已经计算好的值替换，避免重复计算。</p><p><strong>DCE：死代码消除</strong></p><p>如果代码的计算结果未被使用，或代码无法执行和访问，则可将这些代码视为死代码。死代码消除是一种常见编译优化技术，目的是移除对程序运行结果没有影响的死代码，减少最终生成代码的大小。</p><p>传统编译器基于活跃变量分析进行死代码消除优化。DL计算图中的死代码通常是由其它图优化造成的。因此，在其它图优化之后还应执行死代码消除和公共子表达式消除。因为不同于传统编译器的低阶IR，DL编译器中的死代码消除的操作对象是高级图IR。因此DL编译器中的死代码消除会有一些传统编译器不具备的操作。例如，如果存储操作的目的张量在后续计算中不再使用，则存储操作可以被删除。这也属于死代码消除。</p><p><strong>静态内存规划</strong></p><p>静态内存规划的目的是尽可能重用内存缓冲区。静态内存规划通常有两种：就地（in-place）内存共享和标准内存共享。对于采用就地内存共享的操作，其输入和输出占用相同的内存，并在计算之前仅分配一个内存副本。标准内存共享可重复使用先前操作的内存，而不会出现重叠。静态内存规划离线执行，这样就可以采用更复杂的规划算法。</p><p><strong>布局转换</strong></p><p>传统编译器中有成熟且使用广泛的结构数据布局优化（Structure Data Layout Optimizations），目的是提高数据局部性，减少缓存未命中率。</p><p>AI编译器的布局转换，其目的是优化计算图中张量的数据布局，并在途中出插入布局转换节点。注意这时尚未执行实际的转换，而是待到编译器后端评估计算图时才执行实际的转换。</p><p>同一种算子在不同的数据布局上性能是不同的，而不同硬件的最佳数据布局也不同.例如，在GPU上，NCHW格式的操作通常运行速度更快，因而其它格式的张量可以先转换为NCHW格式。有些DL编译器依赖于硬件相关的库来实现更高的性能，而这些库可能对布局有要求。此外，边缘设备通常配备异构计算单元，不同类型的单元可能要求不同的数据布局。因此，AI芯片编译器需要提供在各种硬件之间做布局转换的优化方法。</p><p>布局转换本身的开销也很可观。</p><h4 id="4-3-4-讨论">4.3.4 讨论</h4><p>前端是DL编译器中最重要的组件之一，负责从DL模型到计算图高​​级IR的转换，以及基于高阶IR的硬件无关优化。虽然不同AI芯片编译器前端实现在高阶IR的数据表示和算子定义上有所不同，但与硬件无关的优化都可以分为三个层次：节点级、块级和数据流级。每个层次的优化方法都利用了DL特有方法和常规的编译优化技术，从而减少了计算冗余，提高了DL模型在计算图层次的性能。</p><h3 id="4-4-后端优化">4.4 后端优化</h3><p>DL编译器后端通常包括各种硬件相关优化、自动调优技术和优化的内核库。硬件相关优化可以针对不同硬件目标实现高效的代码生成，自动调优技术可以极大减轻推导最佳参数配置的手动工作量，高度优化的内核库广泛用于通用处理器和定制DL加速器。</p><p>在代码生成方面，传统编译器的目标是生成优化的通用代码，<strong>而AI编译器的目标是为特定算子（如卷积，矩阵乘等）生成性能达到或超过手动调优算子的代码实现。作为代价，AI编译器可能要牺牲编译时间去搜索最优配置。</strong></p><h4 id="4-4-1-硬件相关优化">4.4.1 硬件相关优化</h4><p>硬件相关优化（或目标相关优化）用于获得针对特定硬件的高性能代码。某一个途径是将低阶IR转为LLVM IR，以利用LLVM来生成优化的CPU/GPU代码；另一个途径是利用DL知识来设计定制的优化。</p><p>文章介绍了几个比较经典的硬件相关优化方法：</p><p><strong>硬件固有映射</strong></p><p>硬件固有映射可以将某组低级IR指令转换为已经在硬件上高度优化的内核。</p><p>在TVM中，硬件固有映射是用可扩展张量化的方法实现的，它可以声明硬件固有映射的行为和固有映射的下降规则。这种方法使编译器后端能够将硬件实现以及高度优化的手工微内核应用于特定的操作模式，从而显著提高性能。</p><p><strong>内存分配和获取</strong></p><p>对于GPU和定制加速器，内存分配是代码生成中的一个主要难点。例如，GPU主要包含共享内存空间（容量小但访问延迟较低）和本地内存空间（大容量但访问延迟较高）（3global，2shared，1local，0reg）。这样的内存层次结构需要高效的内存分配和获取技术才能改善数据局部性。为了实现内存分配和获取优化，TVM引入了内存作用域调度（TVM小节中应增加这部分内容？）的概念。内存作用域调度原语可以将某个计算阶段标记为共享或线程本地（thread-local）。对于标记为共享的计算阶段，TVM通过共享内存分配和协作数据获取方式生成代码，这种方式会在适当的代码位置插入内存栅栏（barrier）以保证正确性。而TC通过扩展PPCG编译器提供了类似的功能（称为内存提升）。但是，TC仅支持有限的预定义规则。特别是，TVM可以通过内存作用域调度原语在加速器中实现了特殊缓冲功能。</p><p><strong>内存延迟隐藏</strong></p><p>不论是何种处理器，延迟都包括两种：计算延迟和内存访问延迟。与计算速度相比，内存访问速度要慢得多，并且能耗要大得多。为了提高性能，处理器要花费大量资源来隐藏和减少内存延迟，内存延迟隐藏通过重叠内存计算操作，使内存利用率和计算效率最大化，是后端中使用的一项重要技术。</p><p>为了隐藏等待时间，处理器使用乱序执行，将内存访问与其它工作重叠起来，并使用指令投机调度（speculative instruction scheduling）执行相关指令。为了减少延迟，处理器使用了多级缓存，使常用数据更靠近处理器。但是，这些优化措施各有代价。乱序执行需要昂贵的处理器资源，指令投机调度必须在推测错误时重新执行指令，而多级缓存则需要额外的计算和延迟来搜索缓存。</p><p>由于大多数DL编译器都支持CPU和GPU上的并行化执行，因此可以通过芯片硬件实现内存延迟隐藏。但是，针对不同的硬件，需要采用不同的延迟隐藏策略。例如，GPU通过线程束调度器调度线程束参与指令（流水线）的执行，通过快速的切换线程束，最大化利用功能单元。如果有足够的并发活跃线程束，线程束调度器可以让GPU在每个流水线阶段都处于忙碌状态，GPU的内存指令延迟就可以被其它线程束的计算隐藏。但是对于具有解耦访问执行（DAE, Decoupled Access-Execute）架构的类似TPU的加速器，编译器后端需要执行调度和细粒度的同步才能生成正确且高效的代码。为了获得更好的性能，减少编程负担，TVM引入了虚拟线程调度原语，用户可以在虚拟化多线程体系结构上指定数据并行性。然后，TVM通过插入必要的内存栅栏指令来降低虚拟并行线程的数量，并将这些线程的操作交织到一个指令流中，形成更合理的线程执行流水线，这样就可以隐藏内存访问延迟。</p><p><strong>面向循环的优化</strong></p><p><strong>并行化</strong></p><p>并行化是提高深度学习模型中计算密集操作效率的关键。由于现代处理器通常都支持多线程和SIMD并行性，因此，编译器后端可以利用并行性来提高硬件利用率，实现性能最优化。 Halide使用并行调度原语来并行化计算任务，为线程级并行指定循环的并行化维度。其中的每个并行任务可以进一步递归地细分为子任务，以便充分利用目标体系结构上的多级线程层次结构。Halide可以用向量语句替换循环，然后通过硬件intrinsic映射将向量语句映射到硬件相关的SIMD操作码。 Stripe将多面体模型扩展为嵌套多面体模型，引入了并行多面体块这个概念作为迭代的基本执行元素。扩展之后，嵌套多面体模型可以检测平铺和跨度级别之间的层次结构并行化。另外，有些DL编译器依赖于手工库，例如Glow或由硬件厂商提供的优化数学库（在第4.4.3节中讨论）。同时，Glow将向量化处理放到LLVM中完成，因为只要有张量尺寸和循环轮次信息，LLVM自动向量化功能就完全可以正常工作。</p><h4 id="4-4-2-自动调优">4.4.2 自动调优</h4><p>由于在硬件相关优化中用于参数调优的搜索空间巨大，因此有必要利用自动调优来确定最佳参数配置。在常用DL编译器中，TVM、TC和XLA支持自动调优。</p><p>自动调优实现通常包括四个关键部分：参数化、成本模型、搜索技术和加速。</p><p><strong>参数化</strong></p><p>数据和目标：数据参数描述数据的规格，例如输入形状。目标参数描述了在优化调度和代码生成过程中要考虑的硬件相关的特性和约束。对于GPU，需要指定硬件参数，如共享内存和寄存器大小。</p><p>优化选项：优化选项包括优化调度方法和相应的参数，例如面向循环的优化和图块大小。在TVM中，考虑了预定义调度和用户自定义调度以及其参数。</p><p><strong>成本模型</strong></p><ol><li>黑盒模型：TC采用了这种模型。此模型仅考虑最终执行时间，而不考虑编译任务的特征。建立黑匣子模型很容易，但是在没有任务特征指导的情况下，最终得到的可能是次优的解决方案。</li><li>基于机器学习的成本模型：基于机器学习的成本模型是一种使用机器学习方法预测性能的统计方法。这种模型可以在探索新配置时进行更新，从而有助于实现更高的预测精度。TVM和XLA采用了这种模型。</li><li>预定义成本模型：预定义成本模型是基于编译任务特征的模型，可以用于评估编译任务的整体性能。与基于ML的模型相比，预定义的模型在实际应用中产生的计算开销较小，但是需要大量的工作量才能在每个新DL模型和硬件上重建模型。</li></ol><p><strong>搜索技术</strong></p><ol><li>初始化和确定搜索空间：初始选项可以随机设置，或者是根据经验配置。一般情况下，应在自动调优之前就指定搜索空间。TVM中，开发人员使用其特定域知识指定搜索空间，并基于计算描述提取每个硬件目标的自动搜索空间。而TC依赖于编译缓存和预定义规则获得搜索空间。</li><li>遗传算法（Genetic Algorithm）：遗传算法是受自然选择过程启发的一种元启发法，属于自然进化算法大类。遗传算法通过一组候选解（称为个体，生物或表型）的交叉、变异和选择来解决优化问题。在搜索技术中，遗传算法将每个调优参数视为基因，并将每个配置视为候选解。根据适应度值，通过交叉、变异和选择来迭代生成新的候选配置。最后，得出最佳候选解。交叉，变异和选择的速率用于控制探索和开发之间的权衡。TC的自动调整技术采用了遗传算法。</li><li>模拟退火算法（Simulate Anneal，SA）：模拟退火算法是一种通用概率算法，也是受物理退火过程启发的元启发法，用来在一个大的搜寻空间内寻找问题的最优解 ，并能够以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。TVM的自动调整技术采用了模拟退火算法。</li><li>强化学习（Reinforcement Learning, RL）：强化学习算法是机器学习的一个分支，算法的目的是在不断尝试的过程中，学习到在特定的情境下选择哪种行为可以得到最大的回报。强化学习的学习过程是在探索与开发之间的权衡取舍，在给定环境的情况下使回报最大化。基于TVM构建的Chameleon在其自动调整技术中采用了强化学习。</li></ol><p><strong>加速</strong></p><ol><li>并行化：并行化是加速自动调优的方向之一。考虑到遗传算法需要评估每一代中的所有候选配置，TC提出了一种多线程、多GPU的并行化策略。首先，将候选配置入队，并在多CPU线程上对其进行编译。生成的代码在GPU上并行评估，并且每个候选者都有其父选择步骤使用的适应度。完成整个评估后，将生成新的候选配置，并将新的编译任务入队，等待在CPU上进行编译。同样，TVM支持交叉编译和RPC，允许用户在本地计算机上编译，并在多个目标上以不同的自动调优配置运行程序。</li><li>配置重用：加速自动调优的另一个方向是重用以前的自动调优配置。 TC会在编译缓存中存储某个配置的最快生成代码版本。在编译过程中，每次内核优化之前，编译器都会先检索编译缓存，如果编译缓存未命中，才会触发自动调优。与此类似，TVM会生成一个日志文件，其中存储了所有调度算子的最佳配置，并在编译过程中，从日志文件检索最佳配置。TVM在Halide IR中为每个算子做自动调优，并为每个算子确定各自的最佳配置。</li></ol><h4 id="4-4-3-优化的内核库">4.4.3 优化的内核库</h4><h4 id="4-4-4-讨论">4.4.4 讨论</h4><h2 id="5-DL编译器的分类">5. DL编译器的分类</h2><p>后面实验不用看了</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>TVM</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI编译架构</title>
    <link href="/2023/10/16/AI%E7%BC%96%E8%AF%91%E6%9E%B6%E6%9E%84/"/>
    <url>/2023/10/16/AI%E7%BC%96%E8%AF%91%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<p>参考:</p><p><a href="https://github.com/BBuf/tvm_mlir_learn">TVM学习仓库</a></p><p><a href="https://space.bilibili.com/517221395/channel/collectiondetail?sid=857162">B站-zomi酱</a></p><p><a href="https://zhuanlan.zhihu.com/p/450022851">MLIR入门理解</a></p><span id="more"></span><h2 id="1-编译器相关">1. 编译器相关</h2><p>编译器（Compiler）和解释器（Interpreter）：</p><ul><li>编译器：将源代码整体编译为可执行文件（机器码），（可能经过预编译、编译、汇编、链接等环节，统一视作编译器的流程）最后由机器执行，会产生可以重复使用的中间文件和可执行文件</li><li>解释器：将源代码逐行解释成字节码并直接交由机器执行，不产生其他文件</li></ul><p>编译器编译方式：JIT 和 AOT</p><ul><li>AOT：AheadOfTime 即静态编译，源代码先统一编译成机器码，再执行</li><li>JIT：JustInTime 即动态编译，相比于传统 AOT，JIT 可以在程序运行过程中变运行边编译，具体流程可以参考 java。注意 JIT 与解释器的区别，解释器的粒度为一行源代码，而 JIT 的粒度为一个函数，JIT 编译的函数可以重复使用，而解释器每次都要重新解释一遍。</li></ul><p>一个 GCC 的标准编译流程：</p><ol><li>预处理：处理宏定义、文件包含、条件编译等信息，生成 .i 文件</li><li>编译：对 .i 文件进行语法分析，优化后生成 .s 汇编文件</li><li>汇编：将 .s 汇编文件汇编为机器码 .o 文件</li><li>链接：将程序运行所需要的目标文件、依赖库文件等统一打包链接成一个可执行文件</li></ol><p>LLVM 在 GCC 的基础上发展而来，早期苹果使用 GCC ，后来由于 GCC 证书以及苹果的商用需要，只能放弃 GCC 而单独发展出 LLVM，LLVM 本质算一个编译器的框架系统，使用模块化的方式，将编译器的前端、优化器、后端等模块分开，可以根据需要进行组合，比如目前主流的 Clang 就是 LLVM 的前端，而 LLVM 的后端可以生成多种平台的机器码，LLVM 的优化器也可以单独使用，这样就可以根据需要进行组合，而不是像 GCC 那样，前端、优化器、后端都是一体的，不可分割。</p><blockquote><p><em>LLVM is a sort of abstracted assembly language that compiler developers can target as a backend, and then LLVM itself comes packaged with a host of optimizations and “real” backend targets that can be compiled to. If you’re, say, the Rust programming language and you want to compile to x86, ARM, and WebAssembly without having to do all that work, you can just output LLVM code and then run LLVM’s compilation suite.</em></p></blockquote><p>PASS：编译器对源代码进行完整的扫描，进行优化和分析的步骤<br>IR：Intermediate Representation 中间表达</p><p>编译器基本结构（主要是 LLVM，GCC 分的没有这么明确）</p><ul><li>Front End：词法分析、语法分析，将源代码转换为抽象语法树（AST），LLVM 使用 Clang 作为前端</li><li>Optimizer：优化，将 IR 进行优化，使代码更高效（PASS 在这个地方）</li><li>Back End：代码生成，将 IR 转换为目标代码（机器码）</li></ul><p><img src="image.png" alt="Alt text"></p><blockquote><p>相关 <strong>Chris Lattner：The Golden Age of Compilers</strong></p></blockquote><p>AI 编译器是介于机器学习框架与硬件中间的一层，用于解决众多框架与多种硬件之间的适配问题，主要架构</p><ul><li>Front-end：计算图转换，将不同框架下的源代码输出为 Graph IR 等高阶 IR（HLIR），重点在于抽象出硬件无关的计算和控制流程，以及数据张量、算子的支持</li><li>Optimizer：对计算图进行一些算子融合、自动微分、并行切分、剪枝量化等优化，IR 间的相互转化，将高阶 IR 转换为低阶 IR（LLIR）</li><li>Back-end：针对特定的机器，将低级 IR 转换为 LLVM IR，再利用 LLVM 基础结构生成优化的机器码</li></ul><h2 id="2-TVM">2. TVM</h2><p>参考：</p><p><a href="https://tvm.hyper.ai/docs/arch/">TVM官方文档</a></p><p><a href="https://zhuanlan.zhihu.com/p/560210215">TVM学习指南</a></p><p>为什么使用 TVM：在模型部署时，众多的机器学习框架（Pytorch、TF、ONNX）与众多的平台（x86、arm、GPU）产生了众多不同的部署场景，而同一个模型在这些不同的场景之间是无法无缝切换的。TVM 的目标就是将这些不同的框架与平台进行统一，使得模型部署更加简单。</p><p>TVM 想要解决的问题：模型部署的可移植性问题、特定平台的硬件优化问题、软件栈的支持问题</p><h3 id="编译流程">编译流程</h3><p><img src="image0.png" alt="alt text"></p><p><img src="image1.png" alt="Alt text"></p><ol><li>TVM前端将如ONNX、Pytorch下的模型引入到IRModule中，将其翻译为relay（此时IRModule由一种高级表示relay.Function组成，一个relay.Function通常对应一个端到端的模型，可将其视为额外支持控制流、递归和复杂数据结构的计算图）</li><li>Relay经过第一次转换，即Relay Passes，主要是与硬件无关的转换（常量折叠、死码消除等）</li><li>在Relay优化的后期，TVM会运行一系列pass，以FuseOps（融合操作）开始，加上设备注解、布局重写、存储重写、图到函数转换等，将relay逐步转成te</li></ol><blockquote><p><em>重要的是，张量表达式te本身并不是一个可以存储到 IRModule 中的自包含函数（self-contained function）。相反，它是 IR 的一个片段，可以拼接起来构建一个 IRModule。</em></p></blockquote><ol start="4"><li>将te转为tir，使用autoTVM</li><li>tir经过一系列的tirPasses</li><li>IRMoudle转换成对应设备的runtime.Module由PackedFunc组成</li></ol><p>Relay IR：如 relay.Function，TVM 为了兼容上层的机器学习框架而引入的中间表达，一种高阶的图结构，包含了计算图和控制流的信息，这样的设计使得 TVM 可以对模型进行更加全面的优化。Relax 是下一代 Relay（Relay Next）</p><p>Tensor IR：如 tir.PrinFunc，TVM 为了兼容不同的硬件而引入的中间表达，一种低阶的图结构，包含了数据张量和算子的信息，这样的设计使得 TVM 可以对硬件进行更加全面的优化。</p><p>IRModule：是TVM堆栈中的主要数据结构，也是TVM编译的最小完整单元，在上层一般由一个或多个relay.Function组成。一个 RelayFunc 通常对应一个端到端的模型（可见MLC）。经过 TIR Pass 后一个 RelayFunc 可降级为多个 tir.PrimFunc 即元张量函数，这些函数可以被 TVM 优化器进行优化，最后转化为机器码。</p><h4 id="Pass-转换">Pass 转换</h4><p>TVM转换流程的目的：优化（如常量折叠、死码消除，针对特定张量的布局转换、scale因子折叠），以及降级（将代码逐渐转化成更接近硬件的低级表示。</p><p>在 relay/transform 流程的后期，FuseOps 将端到端的函数（即 relay.Function）转化为一个个的算子（即 tir.PrinFunc），这个过程帮助将原始的编译问题分为了两个子问题：</p><ol><li>算子的编译和优化</li><li>整体的执行流程：对生成的算子进行的调用</li></ol><p>tir/transform 流程主要处理 tir.PrimFunc 的降级，例如有些 pass 将多维访问展平为一维指针访问，将内联函数扩展至特定硬件的函数等。也有一些pass的目的仍是优化，如访问索引简化和死码消除。</p><h4 id="AutoTVM：搜索空间和基于学习的转换">AutoTVM：搜索空间和基于学习的转换</h4><p>上述的转换都是确定且基于某一规则的。TVM的目标之一是支持不同硬件平台的高性能代码优化，因此往往要研究尽可能多的优化选择，包括多维张量访问、循环分块策略、特殊加速器内存。</p><p>首先定义一组用来转换程序的操作，包括循环转换、内联、向量化等，称为调度原语，这种原语组成的集合定义了可用于程序优化的搜索空间。接下来，系统搜索不同的可能调度序列，找到最佳（极佳）的调度组合。</p><p>AutoTVM和AutoScheduler是TVM中的两个自动调度器，AutoTVM是基于遗传算法的调度器，AutoScheduler是基于机器学习的调度器。在官方文档中似乎统一为AutoTVM介绍了。</p><blockquote><p><em>使用基于搜索的优化来处理初始 tir 函数生成问题。</em></p></blockquote><p>AutoTVM是在tirPass之前进行的，经过AutoTVM后生成优化的PrinFunc，可以理解成到tirPass之后就不再进行高层优化了，只是针对硬件做一些特殊处理？</p><h4 id="Target-转换">Target 转换</h4><p>这一阶段将 tir 的 IRModule 转换为相应硬件的可执行形式。对于 x86 和 ARM 等后端，使用 LLVM IRBuilder 来构建内存中的 LLVM IR。还可以生成源代码级语言，例如 CUDA C 和 OpenCL。最后，还支持通过外部代码生成器将 Relay 函数（子图）直接转换为特定 target 。</p><p><strong>重要的是，这一阶段的转换要尽可能轻量级，因为绝大多数转换和降级都在之前的阶段完成</strong></p><h4 id="Runtime-执行">Runtime 执行</h4><blockquote><p><em>TVM runtime 的主要目标是提供一个最小的 API，从而能以选择的语言（包括 Python、C++、Rust、Go、Java 和 JavaScript）加载和执行编译好的工件</em></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tvm<br><span class="hljs-comment"># Python 中 runtime 执行程序示例，带有类型注释</span><br>mod: tvm.runtime.Module = tvm.runtime.load_module(<span class="hljs-string">&quot;compiled_artifact.so&quot;</span>)<br>arr: tvm.runtime.NDArray = tvm.nd.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], device=tvm.cuda(<span class="hljs-number">0</span>))<br>fun: tvm.runtime.PackedFunc = mod[<span class="hljs-string">&quot;addone&quot;</span>]<br>fun(a)<br><span class="hljs-built_in">print</span>(a.numpy())<br></code></pre></td></tr></table></figure><p>tvm.runtime.Module 封装了编译的结果。runtime.Module 包含一个 GetFunction 方法，用于按名称获取 PackedFuncs。</p><p>tvm.runtime.PackedFunc 是一种为各种构造函数消解类型的函数接口。runtime.PackedFunc 的参数和返回值的类型如下：POD 类型（int, float）、string、runtime.PackedFunc、runtime.Module、runtime.NDArray 和 runtime.Object 的其他子类。</p><p>tvm.runtime.Module 和 tvm.runtime.PackedFunc 是模块化 runtime 的强大机制。例如，要在 CUDA 上获取上述 addone 函数，可以用 LLVM 生成主机端代码来计算启动参数（例如线程组的大小），然后用 CUDA 驱动程序 API 支持的 CUDAModule 调用另一个 PackedFunc。OpenCL 内核也有相同的机制。</p><p>下面的代码片段给出了用相同接口执行端到端模型的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tvm<br><span class="hljs-comment"># python 中 runtime 执行程序的示例，带有类型注释</span><br>factory: tvm.runtime.Module = tvm.runtime.load_module(<span class="hljs-string">&quot;resnet18.so&quot;</span>)<br><span class="hljs-comment"># 在 cuda(0) 上为 resnet18 创建一个有状态的图执行模块</span><br>gmod: tvm.runtime.Module = factory[<span class="hljs-string">&quot;resnet18&quot;</span>](tvm.cuda(<span class="hljs-number">0</span>))<br>data: tvm.runtime.NDArray = get_input_data()<br><span class="hljs-comment"># 设置输入</span><br>gmod[<span class="hljs-string">&quot;set_input&quot;</span>](<span class="hljs-number">0</span>, data)<br><span class="hljs-comment"># 执行模型</span><br>gmod[<span class="hljs-string">&quot;run&quot;</span>]()<br><span class="hljs-comment"># 得到输出</span><br>result = gmod[<span class="hljs-string">&quot;get_output&quot;</span>](<span class="hljs-number">0</span>).numpy()<br></code></pre></td></tr></table></figure><p>主要的结论是 runtime.Module 和 runtime.PackedFunc 可以封装算子级别的程序（例如 addone），以及端到端模型。</p><h3 id="逻辑架构组件">逻辑架构组件</h3><p><img src="image2.png" alt="Alt text"></p><h2 id="3-MLIR">3. MLIR</h2><p>参考<br><a href="https://www.lei.chat/zh/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/">某大佬博客</a></p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLVM</tag>
      
      <tag>MLIR</tag>
      
      <tag>TVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Texas Hold&#39;em</title>
    <link href="/2023/09/25/%E5%BE%B7%E5%B7%9E%E6%89%91%E5%85%8B%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/09/25/%E5%BE%B7%E5%B7%9E%E6%89%91%E5%85%8B%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<blockquote><p><em>博弈论的基础建立在理性人假设之上：玩家是理性且自利的</em></p></blockquote><span id="more"></span><h2 id="概率">概率</h2><blockquote><p>VScode的Markdown preview enhanced（MPE）插件支持 Mathjax 和 Katex，可以实时预览Latex公式，完全没必要花两个小时装Texlive</p></blockquote><h3 id="牌型概率">牌型概率</h3><p>虽然52取5对德扑来说没什么意义</p><table><thead><tr><th style="text-align:center">牌型</th><th style="text-align:center">翻牌后</th></tr></thead><tbody><tr><td style="text-align:center">Royal Flush</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>4</mn><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>649740</mn></mfrac><mo>≈</mo><mn>0.00015</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{4}{C^5_{52}} = \frac{1}{649740} \approx 0.00015\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4367em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">649740</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0.00015%</span></span></span></span></td></tr><tr><td style="text-align:center">Straight Flush</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>649740</mn></mfrac><mo>×</mo><mn>10</mn><mo>≈</mo><mn>0.0015</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{1}{649740} \times 10 \approx  0.0015\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">649740</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">10</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0.0015%</span></span></span></span></td></tr><tr><td style="text-align:center">Four-of-a-Kind</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>13</mn><mo>∗</mo><mn>48</mn></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>4165</mn></mfrac><mo>≈</mo><mn>0.024</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{13*48}{C^5_{52}} = \frac{1}{4165} \approx 0.024\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4367em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span><span class="mbin mtight">∗</span><span class="mord mtight">48</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4165</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0.024%</span></span></span></span></td></tr><tr><td style="text-align:center">Full House</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>13</mn><mo>∗</mo><mn>4</mn><mo>∗</mo><mn>12</mn><mo>∗</mo><mn>6</mn></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>694</mn></mfrac><mo>≈</mo><mn>0.144</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{13*4*12*6}{C^5_{52}} = \frac{1}{694} \approx 0.144\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4367em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span><span class="mbin mtight">∗</span><span class="mord mtight">4</span><span class="mbin mtight">∗</span><span class="mord mtight">12</span><span class="mbin mtight">∗</span><span class="mord mtight">6</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">694</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0.144%</span></span></span></span></td></tr><tr><td style="text-align:center">Flush</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>4</mn><mo>∗</mo><msubsup><mi>C</mi><mn>13</mn><mn>5</mn></msubsup></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>≈</mo><mn>0.2</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{4*C^5_{13}}{C^5_{52}} \approx 0.2\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.7258em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1341em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5102em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.214em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0.2%</span></span></span></span></td></tr><tr><td style="text-align:center">Straight</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>10</mn><mo>∗</mo><msup><mn>4</mn><mn>5</mn></msup></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>≈</mo><mn>0.39</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{10*4^5}{C^5_{52}} \approx 0.39\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.6096em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mtight">4</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">0.39%</span></span></span></span></td></tr><tr><td style="text-align:center">Three-of-a-Kind</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>13</mn><mo>∗</mo><mn>4</mn><mo>∗</mo><msubsup><mi>C</mi><mn>12</mn><mn>2</mn></msubsup><mo>∗</mo><msup><mn>4</mn><mn>2</mn></msup></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>≈</mo><mn>2.1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{13*4*C^2_{12}*4^2}{C^5_{52}} \approx 2.1\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.7258em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1341em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5102em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span><span class="mbin mtight">∗</span><span class="mord mtight">4</span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.214em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mtight">4</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">2.1%</span></span></span></span></td></tr><tr><td style="text-align:center">TwoPair</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msubsup><mi>C</mi><mn>13</mn><mn>2</mn></msubsup><mo>∗</mo><msup><mn>6</mn><mn>2</mn></msup><mo>∗</mo><mn>44</mn></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>≈</mo><mn>4.75</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{C^2_{13}*6^2*44}{C^5_{52}} \approx 4.75\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.7258em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1341em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5102em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.214em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mtight">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight">44</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">4.75%</span></span></span></span></td></tr><tr><td style="text-align:center">OnePair</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mn>13</mn><mo>∗</mo><msubsup><mi>C</mi><mn>4</mn><mn>2</mn></msubsup><mo>∗</mo><msubsup><mi>C</mi><mn>12</mn><mn>3</mn></msubsup><mo>∗</mo><msup><mn>4</mn><mn>3</mn></msup></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>≈</mo><mn>42.3</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{13*C^2_{4}*C^3_{12}*4^3}{C^5_{52}} \approx 42.3\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.7258em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1341em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5102em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.214em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.214em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mtight">4</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">42.3%</span></span></span></span></td></tr><tr><td style="text-align:center">Single</td><td style="text-align:center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msubsup><mi>C</mi><mn>13</mn><mn>5</mn></msubsup><mo>∗</mo><msup><mn>4</mn><mn>5</mn></msup></mrow><msubsup><mi>C</mi><mn>52</mn><mn>5</mn></msubsup></mfrac><mo>≈</mo><mn>50.7</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\frac{C^5_{13} * 4^5}{C^5_{52}} \approx 50.7\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.7258em;vertical-align:-0.5916em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1341em;"><span style="top:-2.6264em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1885em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">52</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5102em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.214em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mtight">4</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5916em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em;"></span><span class="mord">50.7%</span></span></span></span></td></tr></tbody></table><h3 id="四二法则">四二法则</h3><p>Flop后听N张牌，则最终成牌的概率约为4N%，Turn后听N张牌，则最终成牌的概率约为2N%</p><p>Flop后成牌概率为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mo stretchy="false">(</mo><mfrac><mrow><mn>47</mn><mo>−</mo><mi>N</mi></mrow><mn>47</mn></mfrac><mo>×</mo><mfrac><mrow><mn>46</mn><mo>−</mo><mi>N</mi></mrow><mn>46</mn></mfrac><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>93</mn><mi>N</mi><mo>−</mo><msup><mi>N</mi><mn>2</mn></msup></mrow><mrow><mn>47</mn><mo>×</mo><mn>46</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">1 -(\frac{47-N }{47} \times \frac{46-N}{46} ) = \frac{93N-N^2}{47\times46}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">47</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">47</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">46</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">46</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4213em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">47</span><span class="mbin mtight">×</span><span class="mord mtight">46</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">93</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>在Desmos中可以看出，当N在10以内时，四二法则的误差在 +1.6% 以内</p><p>Turn后成牌概率为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>N</mi><mn>46</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{N}{46}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">46</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br>当N在10以内时，误差在 -1.7% 以内</p><h3 id="起手牌胜率">起手牌胜率</h3><h4 id="同花面">同花面</h4><p>翻后同花概率</p><h4 id="同花面没有那么强">同花面没有那么强</h4><p>对于同花牌，翻后成花的概率 0.2%，翻后听花概率</p><h3 id="翻后策略求解">翻后策略求解</h3><p><a href="https://github.com/bupticybee/TexasSolver">TexasSolver</a></p><h3 id="期望价值">期望价值</h3><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>V</mi><mo>=</mo><mtext>正向收益</mtext><mo>∗</mo><mtext>获胜概率</mtext><mo>+</mo><mtext>负向收益</mtext><mo>∗</mo><mtext>失败概率</mtext></mrow><annotation encoding="application/x-tex">EV = 正向收益 * 获胜概率 + 负向收益 * 失败概率</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">正向收益</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord cjk_fallback">获胜概率</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">负向收益</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">失败概率</span></span></span></span></p><p>即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>V</mi><mo>=</mo><mi>P</mi><mi>o</mi><mi>t</mi><mo>×</mo><mi>W</mi><mi>i</mi><mi>n</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>−</mo><mi>B</mi><mi>e</mi><mi>t</mi><mo>×</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>e</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">EV = Pot \times WinRate - Bet \times LoseRate</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">Win</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">ose</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>V</mi><mo>≥</mo><mn>0</mn><mo>⇔</mo><mi>B</mi><mi>e</mi><mi>t</mi><mo>≤</mo><mi>P</mi><mi>o</mi><mi>t</mi><mo>×</mo><mfrac><mrow><mi>W</mi><mi>i</mi><mi>n</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>W</mi><mi>i</mi><mi>n</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">EV \geq 0 \Leftrightarrow Bet \leq Pot \times \frac{WinRate}{1 - WinRate}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.136em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⇔</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.136em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2757em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">Win</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Win</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><table><thead><tr><th style="text-align:center">胜率</th><th style="text-align:center">下注量(*Pot)</th></tr></thead><tbody><tr><td style="text-align:center">8%</td><td style="text-align:center">0.09</td></tr><tr><td style="text-align:center">16%</td><td style="text-align:center">0.19</td></tr><tr><td style="text-align:center">32%</td><td style="text-align:center">0.47</td></tr><tr><td style="text-align:center">50%</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">67%</td><td style="text-align:center">2</td></tr></tbody></table><h2 id="策略">策略</h2><p>参考</p><ul><li><a href="https://space.bilibili.com/2143447">https://space.bilibili.com/2143447</a></li><li><a href="https://www.youtube.com/@ronniepoker">https://www.youtube.com/@ronniepoker</a></li></ul><h2 id="GTO">GTO</h2>]]></content>
    
    
    <categories>
      
      <category>interest</category>
      
    </categories>
    
    
    <tags>
      
      <tag>德扑</tag>
      
      <tag>博弈论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Learning Compilation</title>
    <link href="/2023/09/21/Machine-Learning-Compilation/"/>
    <url>/2023/09/21/Machine-Learning-Compilation/</url>
    
    <content type="html"><![CDATA[<p>陈天奇的MLC课程，参考</p><p><a href="https://github.com/BBuf/tvm_mlir_learn">TVM学习仓库</a></p><p><a href="https://mlc.ai/zh/index.html">MLC官方课程文档</a></p><p><a href="https://space.bilibili.com/1663273796/channel/collectiondetail?sid=499979">MLC课程视频</a></p><span id="more"></span><h2 id="1-概述">1.概述</h2><p>定义：将机器学习的算法（模型）从开发形式（如pytorch、tf等通用框架编写的模型描述以及相关权重），通过变换和优化，转化为部署形式（如模型支撑代码、内存控制、接口等）<br>即，将神经网络模型转变成在特定硬件上运行的张量函数代码</p><p>机器学习编译目标：</p><ol><li>集成和最小化依赖</li><li>利用硬件加速：利用到每个部署环境的原生加速技术</li><li>通用优化</li></ol><h2 id="2-张量程序抽象">2. 张量程序抽象</h2><p>元张量函数：机器学习模型执行中的每一个步骤（或者说算子？），如linear、relu、softmax</p><p>许多不同的抽象可以表达同一种元张量函数，如torch.add和numpy.add，同时，有些机器学习框架也提供模型的编译过程优化，将元张量函数转变成更专门的、针对性的函数</p><p>张量程序抽象：一个典型的元张量函数实现包括：</p><ol><li>存储数据的多维数组</li><li>驱动张量计算的循环嵌套</li><li>计算语句</li></ol><p>根据抽象出来的共同特征，元张量函数因此可以被一系列有效的程序变换所改变，即优化。<br>一般情况下，我们感兴趣的大部分元张量函数都具有良好的可变换属性。</p><h3 id="TensorIR：TVM使用的张量程序抽象">TensorIR：TVM使用的张量程序抽象</h3><p>前提：大多数的机器学习编译可以视为张量函数之间的变换</p><h4 id="示例：一个经典的点积-relu-网络">示例：一个经典的点积 + relu 网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dtype = <span class="hljs-string">&quot;float32&quot;</span><br>a_np = np.random.rand(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>).astype(dtype)<br>b_np = np.random.rand(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>).astype(dtype)<br>c_mm_relu = np.maximum(a_np @ b_np, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>在底层，numpy可能使用循环和算术运算实现上述操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mm_relu</span>(<span class="hljs-params">A: np.ndarray, B: np.ndarray, C: np.ndarray</span>):<br>    <span class="hljs-comment"># 存储数据的多维数组</span><br>    Y = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                <span class="hljs-comment"># 计算语句</span><br>                Y[i, j] = Y[i, j] + A[i, k] * B[k, j]<br>    <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            C[i, j] = <span class="hljs-built_in">max</span>(Y[i, j], <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="TensorIR实现：">TensorIR实现：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@tvm.script.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">                B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">                C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># 存储数据的多维数组（缓冲区）</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j)<br>                vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j)<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p>ir_module是TVM编译的最小完整单元，在TVM前端，其通常包括一个或多个relay（一个relay通常对应一个端到端模型），在经过如autoTVM、tirPasses之后relay被分解成一个或多个primFunc</p><p>块是tensorIR的基本计算单位。定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[block_axis] = T.axis.[axis_type]([axis_range], [mapped_value])<br></code></pre></td></tr></table></figure><p>如<code>vi = T.axis.spatial(128, i)</code> 即表示vi为i的映射，范围为(0,128)，且该块轴属性为spatial（空间轴），而vk的属性则为reduce规约轴。（可以理解为空间轴是原本就在的，规约轴是在上面做滑动的）</p><p>块轴加属性的好处是使得vi，vj，vk独立于外部的循环嵌套i，j，k，同时也对外部循环正确性做了二次验证。同时这些附加信息也有助于机器学习编译分析，比如说，我们总是可以在空间轴上做并行化，但在规约轴上做并行化则需要特定的策略</p><pre><code class="hljs">如果觉得自定义属性比较麻烦也可以一键绑定</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># SSR means the properties of each axes are &quot;spatial&quot;, &quot;spatial&quot;, &quot;reduce&quot;</span><br>vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br></code></pre></td></tr></table></figure><h4 id="tensorIR的元张量函数变换">tensorIR的元张量函数变换</h4><p>tensorIR引入了名为Schedule的辅助结构，允许我们进行方便的元张量函数变换</p><p>这是原来的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> IPython<br>IPython.display.Code(MyModule.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-comment"># from tvm.script import ir as I</span><br><span class="hljs-comment"># from tvm.script import tir as T</span><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br>                T.reads(A[vi, vk], B[vk, vj])<br>                T.writes(Y[vi, vj])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                T.reads(Y[vi, vj])<br>                T.writes(C[vi, vj])<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p>使用Schedule进行变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 以给定的module作为输入的辅助Schedule类</span><br>sch = tvm.tir.Schedule(MyModule)<br><span class="hljs-comment"># 获取对应的块及相应循环的引用</span><br>block_Y = sch.get_block(<span class="hljs-string">&quot;Y&quot;</span>, func_name=<span class="hljs-string">&quot;mm_relu&quot;</span>)<br>i, j, k = sch.get_loops(block_Y)<br><span class="hljs-comment"># 变换：将原有的j循环拆分成两个循环（4表示内部循环长度）</span><br>j0, j1 = sch.split(j, factors=[<span class="hljs-literal">None</span>, <span class="hljs-number">4</span>])<br><span class="hljs-comment"># 再次检查结果</span><br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j_0, j_1, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">32</span>, <span class="hljs-number">4</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + j_1)<br>                vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                T.reads(A[vi, vk], B[vk, vj])<br>                T.writes(Y[vi, vj])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                T.reads(Y[vi, vj])<br>                T.writes(C[vi, vj])<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br><span class="hljs-comment"># 还可以更换循环次序</span><br><span class="hljs-comment"># sch.reorder(j0, k, j1)</span><br></code></pre></td></tr></table></figure><p>此外，块之间也可以通过变换完成组合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将块C放到Y的内循环中</span><br>block_C = sch.get_block(<span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;mm_relu&quot;</span>)<br><span class="hljs-comment"># 感觉意思是将块C与j0循环绑定，及j0这个空间轴变换时，原本只有Y有动作，现在C也有动作</span><br>sch.reverse_compute_at(block_C, j0)<br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j_0 <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">32</span>):<br>            <span class="hljs-keyword">for</span> k, j_1 <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">4</span>):<br>                <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                    vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                    vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + j_1)<br>                    vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                    T.reads(A[vi, vk], B[vk, vj])<br>                    T.writes(Y[vi, vj])<br>                    <span class="hljs-keyword">with</span> T.init():<br>                        Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>            <span class="hljs-keyword">for</span> ax0 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                    vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                    <span class="hljs-comment"># 注意这里vj的变化，原本vj = j = j_0 * 4 + j_1，现在变成了j_0 * 4 + ax0</span><br>                    <span class="hljs-comment"># 感觉是因为上面 reverse_compute_at 只是将C与j0绑定，所以j_1这个循环还是在Y中，C里还需要单独循环ax0</span><br>                    vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + ax0)<br>                    T.reads(Y[vi, vj])<br>                    T.writes(C[vi, vj])<br>                    C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br></code></pre></td></tr></table></figure><p>此外还介绍了另一种原语decompose_reduction，用于将语块中元素的初始化与规约更新分开：<br>这也是 TVM 在以后编译的时候隐式做的，所以这一步的主要目的是让它显式，看看最终效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将块Y中的初始化与循环k无关(k是规约轴)</span><br>sch.decompose_reduction(block_Y, k)<br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mm_relu_v3</span>(<span class="hljs-params">A: np.ndarray, B: np.ndarray, C: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j0 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>):<br>            <span class="hljs-comment"># Y_init</span><br>            <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                j = j0 * <span class="hljs-number">4</span> + j1<br>                <span class="hljs-comment"># 此时初始化在k循环之前就已经做好</span><br>                Y[i, j] = <span class="hljs-number">0</span><br>            <span class="hljs-comment"># Y_update</span><br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                    j = j0 * <span class="hljs-number">4</span> + j1<br>                    Y[i, j] = Y[i, j] + A[i, k] * B[k, j]<br>            <span class="hljs-comment"># C</span><br>            <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                j = j0 * <span class="hljs-number">4</span> + j1<br>                C[i, j] = <span class="hljs-built_in">max</span>(Y[i, j], <span class="hljs-number">0</span>)<br><br>c_np = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=dtype)<br>lnumpy_mm_relu_v3(a_np, b_np, c_np)<br>np.testing.assert_allclose(c_mm_relu, c_np, rtol=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure><h4 id="构建与运行">构建与运行</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用llvm将模型编译到本机平台</span><br>rt_lib = tvm.build(MyModule, target=<span class="hljs-string">&quot;llvm&quot;</span>)<br><br><span class="hljs-comment"># 用于存储输入和输出的TVM NDArray</span><br>a_nd = tvm.nd.array(a_np)<br>b_nd = tvm.nd.array(b_np)<br>c_nd = tvm.nd.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br><br><span class="hljs-comment"># 调用编译好的函数</span><br>func_mm_relu = rt_lib[<span class="hljs-string">&quot;mm_relu&quot;</span>]<br>func_mm_relu(a_nd, b_nd, c_nd)<br><span class="hljs-comment"># 将TVM与numpy的结果进行比较</span><br>np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=<span class="hljs-number">1e-5</span>)<br><br><span class="hljs-comment"># 调用TVM变换后的函数，继续比较</span><br>rt_lib_after = tvm.build(sch.mod, target=<span class="hljs-string">&quot;llvm&quot;</span>)<br>rt_lib_after[<span class="hljs-string">&quot;mm_relu&quot;</span>](a_nd, b_nd, c_nd)<br>np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure><p>在最后的结果中，TVM变换后的函数运行时间相比原先的TVM函数大幅缩短，为什么不同的循环变体会导致不同的时间性能呢？</p><p>关键在于CPU的访存策略，由于局部性原理，CPU在读取内存某元素时会尝试将该元素附近的元素一起获取到缓存中（cache块？特么OS快忘干净了😅）。因此具有连续内存访问的代码通常比随机访问内存不同部分的代码更快。</p><h2 id="3-端到端的模型执行">3. 端到端的模型执行</h2><p>现在考虑一个基础的两层神经网络，由2个MLP和1个relu组成（简化问题，删除最后的softmax）</p><p>numpy实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numpy_mlp</span>(<span class="hljs-params">data, w0, b0, w1, b1</span>):<br>    lv0 = data @ w0.T + b0<br>    lv1 = np.maximum(lv0, <span class="hljs-number">0</span>)<br>    lv2 = lv1 @ w1.T + b1<br>    <span class="hljs-keyword">return</span> lv2<br><br>res = numpy_mlp(img.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">784</span>),<br>                mlp_params[<span class="hljs-string">&quot;w0&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;b0&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;w1&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;b1&quot;</span>])<br></code></pre></td></tr></table></figure><p>底层实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_linear0</span>(<span class="hljs-params">X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">784</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            Z[i, j] = Y[i, j] + B[j]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_relu0</span>(<span class="hljs-params">X: np.ndarray, Y: np.ndarray</span>):<br>     <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            Y[i, j] = np.maximum(X[i, j], <span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_linear1</span>(<span class="hljs-params">X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>            Z[i, j] = Y[i, j] + B[j]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mlp</span>(<span class="hljs-params">data, w0, b0, w1, b1</span>):<br>    lv0 = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_linear0(data, w0, b0, lv0)<br><br>    lv1 = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_relu0(lv0, lv1)<br><br>    out = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_linear1(lv1, w1, b1, out)<br>    <span class="hljs-keyword">return</span> out<br><br>result =lnumpy_mlp(<br>    img.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">784</span>),<br>    mlp_params[<span class="hljs-string">&quot;w0&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;b0&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;w1&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;b1&quot;</span>])<br><br>pred_kind = result.argmax(axis=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Low-level Numpy MLP Prediction:&quot;</span>, class_names[pred_kind[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><p>该模型的TVMScript实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@tvm.script.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">relu0</span>(<span class="hljs-params">x: T.handle, y: T.handle</span>):<br>        n = T.int64()<br>        X = T.match_buffer(x, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Y = T.match_buffer(y, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                Y[vi, vj] = T.<span class="hljs-built_in">max</span>(X[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">linear0</span>(<span class="hljs-params">x: T.handle,</span><br><span class="hljs-params">                w: T.handle,</span><br><span class="hljs-params">                b: T.handle,</span><br><span class="hljs-params">                z: T.handle</span>):<br>        m, n, k = T.int64(), T.int64(), T.int64()<br>        X = T.match_buffer(x, (<span class="hljs-number">1</span>, m), <span class="hljs-string">&quot;float32&quot;</span>)<br>        W = T.match_buffer(w, (n, m), <span class="hljs-string">&quot;float32&quot;</span>)<br>        B = T.match_buffer(b, (n, ), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Z = T.match_buffer(z, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Y = T.alloc_buffer((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n, m):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Z&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                Z[vi, vj] = Y[vi, vj] + B[vj]<br><br><span class="hljs-meta">    @R.function</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">x: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">1</span>, <span class="hljs-string">&quot;m&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             w0: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;m&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             b0: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;n&quot;</span>, </span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             w1: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;k&quot;</span>, <span class="hljs-string">&quot;n&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             b1: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;k&quot;</span>, </span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        m, n, k = T.int64(), T.int64(), T.int64()<br>        <span class="hljs-keyword">with</span> R.dataflow():<br>            lv0 = R.call_dps_packed(<span class="hljs-string">&quot;linear0&quot;</span>, (x, w0, b0), R.Tensor((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>))<br>            lv1 = R.call_dps_packed(<span class="hljs-string">&quot;relu0&quot;</span>, (lv0, ), R.Tensor((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>))<br>            out = R.call_dps_packed(<span class="hljs-string">&quot;linear0&quot;</span>, (lv1, w1, b1), R.Tensor((<span class="hljs-number">1</span>, k), <span class="hljs-string">&quot;float32&quot;</span>))<br>            R.output(out)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>引入了一个新的 <code>@R.function</code> 即Relex函数，是一种表示上层神经网络执行的全新抽象</p><p><img src="image.png" alt="Alt text"></p><p>注意到，其中<code>call_dps_packed</code>将我们的元函数嵌入到计算图中，其主要作用是满足<strong>目标传递</strong>的调用约定，即 pure 或 side-effect free ，函数只从其输入中读取数据并输出返回结果，而不改变程序的其他部分，这可以方便我们隐藏调用底层元函数的细节</p><p>如果只是像numpy实现中那样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lnumpy_linear0(data, w0, b0, lv0)<br>lnumpy_relu0(lv0, lv1)<br>lnumpy_linear1(lv1, w1, b1, out)<br></code></pre></td></tr></table></figure><p>计算图可能会变成这样：lv0既是<code>lnumpy_linear0</code>的入参，也是<code>lnumpy_relu0</code>的入参，其余同理<br><img src="image-1.png" alt="Alt text"></p><blockquote><p>计算图通常具有以下性质：</p><ul><li>框的每个输入边对应于操作的输入</li><li>每个出边对应于操作的输出</li><li>每个操作可以任意重新排序，直到边缘的拓扑顺序</li></ul></blockquote><p>当然，numpy的底层同样也使用了如<code>lnumpy_call_dps_packed</code>的类似调用</p><p>此外，注意<code>with R.dataflow():</code> 是一个帮助我们标注程序计算图范围的方式，后面的构建运行就不多说了</p><h2 id="4-自动程序优化">4. 自动程序优化</h2><p>这一章主要讲随机调度变换，当我们无法决定原张量函数优化的每一个细节时，可以使用机器的一些<strong>随机变换</strong>做法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">stochastic_schedule_mm</span>(<span class="hljs-params">sch: tvm.tir.Schedule</span>):<br>    block_C = sch.get_block(<span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;main&quot;</span>)<br>    i, j, k = sch.get_loops(block=block_C)<br>    <span class="hljs-comment"># 注意 j_factors 没有使用固定的[none,4]，而是采用随机值</span><br>    j_factors = sch.sample_perfect_tile(loop=j, n=<span class="hljs-number">2</span>)<br>    j_0, j_1 = sch.split(loop=j, factors=j_factors)<br>    sch.reorder(i, j_0, k, j_1)<br>    sch.decompose_reduction(block_C, k)<br>    <span class="hljs-keyword">return</span> sch<br></code></pre></td></tr></table></figure><p>上述代码中，用到了 sch.sample_perfect_tile 来随机拆分循环。它会将输入的循环的长度进行随机分割，例如原始j =128 时，就可以分割为 [8,16]、[32,4]、[2,64] 等等，可以发现，每次运行时该函数的采样都不一样</p><p>此外还讲了一些随机搜索的东西，大概类似超参数的网格搜索之类的，在TVM里叫<code>meta_schedule</code>，主要还做了以下事情：</p><ol><li>跨多个进程的并行基准测试</li><li>使用代价模型<code>cost model</code>进行代价评估，这样可以避免每组都进行基准测试</li><li>根据历史轨迹来进行遗传搜索，而不是每次都随机采样</li></ol><p>关键思想就是使用随机变换来指定好的程序的搜索空间，使用 <code>tune_tir</code> API 帮助在搜索空间内搜索并找到最优的调度变换</p><blockquote><p><strong>前面几章内容总结，就是为什么通过编译可以使模型运行更快（cache空间局部性），以及怎么样编译可以更快（元张量函数变换），同时也介绍了一些随机变换的方法（网格搜索），感觉随机变换的算法才是MLC性能的核心，也就是自动调优，TVM后面似乎用到了一些 autoTVM、autoSchedule 之类的方法进行 auto tune，这也是我需要重点关注的部分</strong></p></blockquote><h2 id="5-与机器学习框架的整合">5. 与机器学习框架的整合</h2><p>如何将机器学习模型从现有框架引入MLC，一些API的基础教程，参考 <a href="https://mlc.ai/zh/chapter_integration/index.html">https://mlc.ai/zh/chapter_integration/index.html</a></p><h2 id="6-GPU硬件加速">6. GPU硬件加速</h2><p>在GPU环境下的MLC流程，第一部分主要讨论CUDA，第二部分讨论专门的GPU环境，后面再看吧</p><h2 id="7-计算图优化">7. 计算图优化</h2><p>提供了一些算子融合的基础代码，也不太想看</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>TVM</tag>
      
      <tag>课程笔记</tag>
      
      <tag>MLC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网络状态检查</title>
    <link href="/2023/09/16/%E7%BD%91%E7%BB%9C%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5/"/>
    <url>/2023/09/16/%E7%BD%91%E7%BB%9C%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5/</url>
    
    <content type="html"><![CDATA[<p>最近fps老是延迟+掉包（把把被一枪头想必不是我的问题），加上网口接触不良一直不好确定源头在哪，在此记录一下问题定位的过程</p><span id="more"></span><h2 id="原因分析：">原因分析：</h2><ul><li>数据传输流程：<br>本地主机 — 本地路由（如果有） — 网关 — 外部网络中转 — 目标主机</li></ul><p>很显然突然的延迟和掉包几乎不可能是外部网络中转和目标主机的问题，加上目前是直连宽带，问题只可能出在主机 &lt;—&gt; 网关上</p><h2 id="问题排查：">问题排查：</h2><h3 id="1-查看自身网络配置：">1. 查看自身网络配置：</h3><p><img src="0.png" alt=""></p><p>本地主机有两条连接，网关分别为192.168.14.1（网口）和172.168.15.1(wifi)</p><h3 id="2-运行任一出现掉包的应用">2. 运行任一出现掉包的应用</h3><h3 id="3-任务管理器中的资源监视器，找到进程PID以及通信的外部IP">3. 任务管理器中的资源监视器，找到进程PID以及通信的外部IP</h3><p><img src="1.png" alt=""></p><p>我也不知道这么多IP哪一个是导致掉包和延迟的。。先找收发高的吧 180.102.211.22 121.229.89.178</p><h3 id="4-netstat查看该进程的网络相关信息">4. netstat查看该进程的网络相关信息</h3><p><img src="2.png" alt=""></p><p>很显然主机是用的192.1168.14.1上的某个端口向外部通信，并且问题大概率出现在与121.229与180.102的通信上</p><h3 id="5-tracert查看路由（pathping也行）">5. tracert查看路由（pathping也行）</h3><table><thead><tr><th style="text-align:center"><img src="4.png" alt=""></th><th style="text-align:center"><img src="5.png" alt=""></th></tr></thead></table><p>两次在走过网口后，都又经过了一个本地IP 100.69.0.1 ，估计是一整栋楼或者本层局域网统一之后的二级网关，经过这个才到外部网络</p><h3 id="6-ping">6. ping</h3><p>分别 ping -t 了一下网关和100.69两个ip，发现到网关基本上没问题，也就是主机到网口的全过程流畅</p><p>每次出现掉包时，到100.69也会出现请求超时，问题就在网口到100.69这段，分析有两种可能：</p><ol><li>网口到100.69线路导致丢包</li><li>100.69负载过高，收到数据后转发丢包</li></ol><h2 id="问题解决">问题解决</h2><p><s>不信邪，直接去楼上的两间空房和本层的另一间空房试了一下，楼上经网关后统一发向100.65.0.1，楼下统一发向100.69.0.1，且本层其他房间ping 100.69也会出现超时或延迟过高问题，服了，确实是层级网关负载过高的原因</s></p><p>并不是，过几天发现有时网络也会走100.65，而且同样会有丢包现象。</p><p>于是偷偷溜进弱点机房观察了一下，一层大概二十个房间，使用两组交换机，每个交换机的24号口连接主路由，主路由的WAN口再连接光猫。主路由还有一个口用来给wifi的路由器。</p><p>这个路由器应该就是100.65.0.1，但是为什么之前测试走的100.69，可能有负载均衡吧。</p><p>实测不管换线还是换接口，延迟和丢包依旧在。看到主路由还有一个闲置LAN口，索性把自己房间直接跳过交换机接到主路由上，发现延迟还是有些不稳定，但丢包彻底解决。公寓该换交换机了。</p><p>本来之后还想着直接接到光猫的闲置口上，但无法识别，可能端口没开启或者是手动分配ip的吧。</p><p>1Gbps速率测速能跑到七八十M/s，应该没啥问题，但wifi延迟和带宽还是很拉跨，应该也是wifi路由器的问题。</p><h2 id="有线和wifi双网叠加">有线和wifi双网叠加</h2><p>参考：<a href="https://www.zhihu.com/question/294289602/answer/2912972037">https://www.zhihu.com/question/294289602/answer/2912972037</a></p><p>系统会优先选择跃点数小的网络进行传输， route print查看连接网络跃点数，之后将两个网络跃点数设置为相同即可</p><p>感觉很扯，实测下载速度不变，上传速度翻倍。。。牛皮</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>记录</tag>
      
      <tag>网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dive into Deeplearning</title>
    <link href="/2023/09/14/Dive-into-Deeplearning/"/>
    <url>/2023/09/14/Dive-into-Deeplearning/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><p>参考：</p><p><a href="https://zh.d2l.ai/index.html">《动手学深度学习》</a></p><p><a href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497&amp;ctype=0">Bilibili_跟李沐学AI</a></p><p><img src="image.png" alt="全书结构"></p><h2 id="Intro">Intro</h2><h3 id="机器学习的关键组件：">机器学习的关键组件：</h3><ol><li>用来学习的数据：data</li><li>转换数据的模型：model</li><li>用来量化模型有效性的目标函数：objective function（lose function）</li><li>调整模型参数以优化目标函数得到的值的算法：algorithm</li></ol><h4 id="数据">数据</h4><p>用于机器学习的数据一般可分为训练集（train）、测试集（validate）和验证集（test）</p><ol><li>训练集用于训练即反向传播调整模型参数</li><li>测试集用于调整模型超参数</li><li>验证集用于评估模型最终性能</li></ol><p>每个数据集由一个个样本（sample，也称数据点或数据实例）组成，大多时候这些样本都遵循独立同分布，每个样本由一组称为特征（features）的属性组成，特征数量称为该样本数据的维数（dimensionality）。机器学习模型会根据这些属性进行预测，在监督学习中，预测结果是一个特殊属性，称为标签（label）</p><h4 id="模型">模型</h4><p>这是深度学习与经典机器学习的主要区别点：深度学习的模型更复杂，数据转换层数更多</p><h4 id="目标函数">目标函数</h4><p>即误差函数，一般用来度量预测值与真实值之间的误差</p><h4 id="优化算法">优化算法</h4><p>用于搜索模型的最佳参数，从而最小化目标函数。一般采用梯度下降</p><h3 id="机器学习问题分类">机器学习问题分类</h3><p>监督学习：回归、分类、标记（不排斥的图片多物体标记）、搜索、推荐系统、序列学习（RNN）等</p><p>无监督学习：聚类、主成分分析、GAN等</p><p>在线学习：以上监督/无监督都属于预先获取大量数据，然后启动模型开始学习的离线学习类。离线学习将算法与环境断开，可以孤立的进行模式识别而不必被影响。而在线学习强调与环境的互动</p><p>强化学习：一个典型的与环境交互的学习方式</p><h2 id="线性神经网络">线性神经网络</h2><p>。。。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Deeplearning</tag>
      
      <tag>课程笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>General-Purpose Graphics Processor Architectures</title>
    <link href="/2023/09/14/General-Purpose-Graphics-Processor-Architectures/"/>
    <url>/2023/09/14/General-Purpose-Graphics-Processor-Architectures/</url>
    
    <content type="html"><![CDATA[<h1>General-Purpose Graphics Processor Architectures</h1><h2 id="Abstract-Preface">Abstract &amp;&amp; Preface</h2><p>与CPU相比，GPU可以更加聚焦与计算，因此性能和效率更高。——通用可编程GPU</p><p>章节介绍：GPU基本结构与历史 —— GPU编程模型 —— GPU计算核心的体系结构 —— 计算核心与内存系统的交叉研究</p><h2 id="Chapter-1-Intro">Chapter 1: Intro</h2><h3 id="1-1-计算加速器的前景">1.1 计算加速器的前景</h3><p>过去，计算系统的性能提升大部分依赖于工艺的进步，使得晶体管尺寸缩小，从而提升集成度，使得运行速度更快。</p><p>Dennard Scaling：从05年开始晶体管缩放规则失效。因此为了提高性能，需要找到更高效的硬件架构。</p><p>hardware specialization：定制化硬件，可以使能效比大幅提高，两大方向：</p><ul><li>硬件向量化，消除指令处理的开销</li><li>优化计算过程，减少数据运输开销</li></ul><p>计算架构的关键：专业化硬件带来的收益与支持广泛程序所需的灵活性之间的平衡。相比于专用加速器（例如google的TPU），仍然需要GPU这种较为通用的计算硬件。</p><p>Turing-complete：图灵完备，只要给足够的时间与内存，GPU可以完成一切运算。</p><h3 id="1-2-GPU硬件基础">1.2 GPU硬件基础</h3><p>GPU不会完全取代CPU：GPU不是独立的计算设备，通常来说，CPU负责在GPU上启动计算并负责GPU上的数据传输。当前访问I/O设备或提供OS服务的软件主要还是运行在CPU上（这些软件缺乏大规模并行性），因此，需要首先考虑GPU与CPU的交互。</p><ul><li>独立GPU：两个U各有各的mem，同时核心通过PCIE总线进行数据传输。注意对于独显来说，两个U的mem的DRAM技术通常是不一样的，CPU的DRAM通常针对低延迟访问进行优化（DDR），而GPU的DRAM通常针对高吞吐进行优化（GDDR）。</li><li>集成GPU：两个U共享一个cache，cache与一个内存进行数据交互。由于共享内存所以只能采取单一技术，集成式GPU通常搭载在低功耗设备上，因此DRAM通常针对低功耗进行优化（LPDDR）。</li></ul><p>一个GPU计算应用会从CPU上开始，通常，该应用程序的CPU部分负责分配和初始化一些数据结构。在旧的N卡和A卡上，CPU需要为CPU和GPU内存中的数据结构分配空间，并协调数据从CPUmem到GPUmem的移动。在新的N卡（Pascal，10系）上的软硬件支持数据从Cmem到Gmem的自动传输，这项技术通过利用虚拟内存支持来实现，NV称之为unified memory。对于集显来说不存在数据mem传输的问题，但是由于两个U共享cache并且有些cache可能是私有的，因此也需要关注缓存一致性问题 (cache-coherence) 。</p><p>启动GPU运算一般需要驱动程序完成，在GPU启动运算前，CPU通过驱动程序指定GPU运行哪些代码，这些代码称为内核（kernel），同时，CPU还需要指定线程数、每个线程的数据位置等等。配置完毕后，CPU向GPU发出信号，GPU开始运算。</p><p>现代GPU由许多核心（SIMT Core）组成，NV称之为流式多处理器(Streaming Multiprocessor, SM)，AMD称之为计算单元(compute unit)，每个核心都执行一个与此时运行的内核相关的单指令多线程程序，一个核心可以运行上千个线程，这些线程通过暂存区mem进行通信，并使用快速屏障技术（fast barrier operations）进行同步。每个核心同时还有一级指令和一级缓存，这些缓存可以充当带宽过滤器，减少向低级别内存的流量，当拥有大量线程时，可以隐藏由于有时某线程的缓存未命中而访问内存带来的的性能下滑。</p><p>高计算吞吐需要高内存带宽的支持，这又对内存系统的并行性提出要求。这种并行性又多通道内存实现，每个通道与内存分区中的最后一级缓存（LLC）相连，GPU核心通过片上互连网络与内存分区相连。也有一些替代的方案，例如Intel的Xeon Phi，就是将LLC直接交由GPU核心分配</p><p>对于高并发任务来说，GPU相对超标量无序CPU拥有更高的单位面积性能，因为GPU可以将其芯片面积的大部分专用于算术逻辑单元，并相应的减小控制逻辑的面积。</p><p>09年出来一个性能随线程变化的分析模型。模型显示：</p><ol><li>当少量线程共享大缓存时（如多核CPU），性能会随着线程数量的增加而提高。</li><li>当线程数增加到缓存无法容纳整个工作集时，性能反而会随着线程数量增加而下降。</li><li>但是随着线程数量的进一步增加，性能会随着多线程隐藏片外延迟的能力而提高。<br>GPU就是通过采用多线程来容忍频繁的缓存未命中，提高运算性能。</li></ol><p>内存访问不仅降低性能，同时也会提高能耗。新的GPGPU架构的重点是改善内存访问。</p><h3 id="1-3-GPU简史">1.3 GPU简史</h3><h3 id="1-4-书籍大纲">1.4 书籍大纲</h3><ul><li>第二章：编程模型、代码开发过程、编译流程</li><li>第三章：单个GPU核心（SM）的体系结构</li><li>第四章：内存系统</li><li>第五章：其他研究</li></ul><h2 id="Chapter-2：编程模型">Chapter 2：编程模型</h2><p>现代GPU广泛采用SIMD硬件来利用数据级并行，但GPU的计算API（如NV的cuda和AMD的opencl）并不向程序员暴露SIMD的硬件，而是采取类似MIMD的编程模型，允许程序员在GPU上启动大量标量线程。其中的每一个标量线程都有自己独特的执行路径，并都可以访问内存。运行时，GPU上的的SIMD硬件利用线程的规律性和空间局部性，同步启动这些标量线程组，称为SIMT（单指令多线程）</p><ul><li>SIMD：例如两个向量，对两个向量的每一个分量进行相同的op操作，输出为一个向量（在一个线程里，受ALU宽度限制）</li><li>SIMT：与SIMD在一个线程公用一个ALU不同，SIMT有多个线程，每个线程各有各的ALU和自己的数据，但执行的指令相同（但是由于数据不同，执行指令时的控制分支可能会不一样）</li></ul><h3 id="2-1-运行模型">2.1 运行模型</h3><p>为GPU优化的代码很可能在CPU架构上表现不佳。假定一个 单精度标量A * 向量X + 向量Y 的函数实现：</p><p>CPU实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-type">void</span> <span class="hljs-title function_">saxpy_serial</span><span class="hljs-params">(<span class="hljs-type">int</span> n, <span class="hljs-type">float</span> a, <span class="hljs-type">float</span> *x, <span class="hljs-type">float</span> *y)</span><br>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; ++i)<br>        y[i] = a * x[i] + y[i];<br>&#125;<br>main()<br>&#123;<br>    <span class="hljs-type">float</span> *x, *y;<br>    <span class="hljs-type">int</span> n;<br>    <span class="hljs-comment">// 省略*x、*y的赋值操作</span><br>    saxpy_serial(n, <span class="hljs-number">2.0</span>, x, y);<br>    <span class="hljs-comment">// 省略内存释放操作</span><br>&#125;<br></code></pre></td></tr></table></figure><p>GPU-CUDA实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs C">__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">saxpy</span><span class="hljs-params">(<span class="hljs-type">int</span> n, <span class="hljs-type">float</span> a, <span class="hljs-type">float</span> *x, <span class="hljs-type">float</span> *y)</span> <span class="hljs-comment">// __global__代表函数在GPU上运行</span><br>&#123;<br>   <span class="hljs-comment">// 每个线程都有各自的blockIdx.x、blockDim.x、threadIdx.x</span><br>   <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>   <span class="hljs-comment">// threadIdx.x代表线程在线程块中的x坐标</span><br>   <span class="hljs-comment">// blockIdx.x代表线程所属的线程块在grid中的x坐标</span><br>   <span class="hljs-comment">// blockDim.x一个线程块在x维度的最大线程数</span><br>   <span class="hljs-comment">// 一般来说线程在线程块中有xyz三个坐标，线程块在网格中也有xyz三个坐标，这里省略y，z</span><br>   <span class="hljs-keyword">if</span> (i &lt; n)<br>      y[i] = a * x[i] + y[i];<br>&#125;<br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>   <span class="hljs-comment">// 一般h_表示cpu的内存指针，d_表示gpu的内存指针</span><br>   <span class="hljs-type">float</span> *h_x, *h_y;<br>   <span class="hljs-type">int</span> n;<br>   <span class="hljs-comment">// 省略*h_x、*h_y的赋值操作</span><br>   <span class="hljs-type">float</span> *d_x, *d_y;<br>   <span class="hljs-type">int</span> nblocks = (n + <span class="hljs-number">255</span>) / <span class="hljs-number">256</span>;<br>   <span class="hljs-comment">// 调用GPU驱动程序并要求分配gpu内存，并将这一片内存的地址赋给&amp;d_x</span><br>   cudaMalloc(&amp;d_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));<br>   cudaMalloc(&amp;d_y, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));<br>   <span class="hljs-comment">// 将h_x指向的内容赋值给d_x指向的区域</span><br>   cudaMemcpy(d_x, h_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>   cudaMemcpy(d_y, h_y, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>   <span class="hljs-comment">// 交由GPU，并启动nblocks个线程块（Thread Block，或CTA），每个线程块256个线程，所有的线程块组成一个grid，即本次内核的计算单元</span><br>   saxpy&lt;&lt;&lt;nblocks, <span class="hljs-number">256</span>&gt;&gt;&gt;(n, <span class="hljs-number">2.0</span>, d_x, d_y);<br>   <span class="hljs-comment">// 为了提高效率，每个线程块中，每32个线程以锁步形式组成一组warp，warp往上再组成线程块</span><br>   <span class="hljs-comment">// 一个warp包含多少线程是硬件概念，而一个线程块可以有多少线程则是软件概念（当然得是warp的整数倍）</span><br>   <span class="hljs-comment">// 将计算结果返回给CPU内存</span><br>   cudaMemcpy(h_x, d_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyDeviceToHost);<br>   <span class="hljs-comment">// 省略内存释放操作</span><br>&#125;<br></code></pre></td></tr></table></figure><p>硬件：一个GPU有多个SM，每个SM包含多个SP（Stream Processor）</p><p>软件：一个GPU内核对应一个grid，一个grid包含多个CTA。CTA中有多个warp，每个warp包含固定数量的线程数（与SM中SP数量相同）。</p><p>GPU在运算时，可能是一个grid独占GPU，也可以多个grid并行跑GPU；SM对应的工作单元是CTA，其中的基本执行单元是warp（即每个SP对应一个warp中的线程），在某个warp受阻时SM可以切换同一个CTA中的其他warp，但只有该CTA执行完，才切换其他CTA</p><p>CTA中的线程之间可以通过暂存器内存互相通信（NV称之为共享内存），同步也轻松，同时每个SM中也有一个共享内存，可以分配给在该SM上运行的所有CTA</p><p>不同CTA中的线程也可以通过所有线程都能访问的全局地址空间通信，但代价较高</p><h3 id="2-2-指令模型">2.2 指令模型</h3><p>NV的并行线程执行ISA：Parallel Thread eXecution，简称PTX （虚拟指令，类似汇编指令）</p><p>GPU运行PTX代码前，需要编译（汇编）成实际的机器指令，NV称此为SASS（Streaming ASSembler），该过程有NV的工具包完成，并没有开放，这使得NV可以在硬件级别提供向后兼容性，每一代都可以重新设计ISA架构</p><h2 id="Chapter-3：SIMT核心：指令和寄存器数据流">Chapter 3：SIMT核心：指令和寄存器数据流</h2><p>对传统图形渲染来说，GPU通常需要访问详细的纹理图，这样的数据集因为太大不可能完全缓存在芯片上，因此有必要采用能够维持大片外带宽的GPU架构。所以如今的GPU都往高并发线程发展（大概意思是线程越多越能够隐藏访存损失）。并且，尽管每个线程的片上缓存很小，但因为局部性原理，仍然可以有效减少大量的片外存储访问。</p><p>SM的微体系结构，流水线分为SIMT前端和SIMD后端，共3个循环：</p><ol><li>取值（fetch）循环：fetch、I-Cache、Decode和I-Buffer模块</li><li>发指（issue）循环：I-Buffer、Scoreboard、issue和SIMT stack模块</li><li>寄存器访问调度循环：Operand Collector、ALU和Memory模块</li></ol><p><img src="image.png" alt=" "></p><h3 id="3-1-单循环近似">3.1 单循环近似</h3><p>线程的调度单位是warp（AMD称之为wavefronts）。每一个周期，SM选择一个warp进行调度。</p><p>单循环中，warp的程序计数器（PC）用于访问指令存储器时查找为warp执行的下一条指令。获得指令后，对指令解码，并找到源操作数寄存器。与此同时，SIMT的执行掩码值也被确定。</p><p>在执行掩码与源寄存器可用后，执行以SIMD的方式进行。如果设置了SIMT执行掩码，则每个线程都在与通路关联的功能单元上执行。与现代CPU一样，功能单元通常异构，即不同的单元支持不同的指令运行。</p><p>每个功能单元在名义上包含的通路数与warp的线程数相同，但也有一些GPU使用不同的实现，使其中的warp在多个时钟周期内执行。</p><h4 id="3-1-1-SIMT执行掩码">3.1.1 SIMT执行掩码</h4><p>现代GPU的关键特性是SIMT执行模型，为程序员提供了单个线程完全独立执行的抽象，这是通过传统谓词（prediction）与SIMT谓词掩码堆栈结合实现的。</p><p>SIMT堆栈有助于处理线程可以独立执行时出现的两个关键问题：</p><ol><li>嵌套控制流</li><li>完全跳过计算</li></ol><table><thead><tr><th style="text-align:center"><img src="image-1.png" alt=" "></th><th style="text-align:center"><img src="image-2.png" alt=" "></th><th style="text-align:center"><img src="image-3.png" alt=" "></th></tr></thead></table><p>假设每个warp有四个线程，所有线程都执行了A基本块，之后遵循不同的控制流，有3个线程进入B，1个线程进入F。如此流动，最后所有线程统一到达G。</p><table><thead><tr><th style="text-align:center"><img src="image-4.png" alt=" "></th><th style="text-align:center"><img src="image-5.png" alt=" "></th></tr></thead></table><p>堆栈包括三项：重新收敛程序计数器(Reconvergence program counter, RPC)、要执行的下一条指令的地址(Next PC)和活跃掩码(active mask)。warp每次都执行栈顶指针指向的条目的nextPC指向的代码块</p><ol><li>开始时堆栈中只有一个条目“-，A，1111”。代表所有线程都将进入A。</li><li>所有4个线程在走完A之后进行分支，此时需要有3处修改：<ul><li>将原先条目的nextPC值修改成分支后的重新汇聚点，对于这次分支B和F，将在G重新汇聚，因此将第一条的nextPC由A修改为G</li><li>这次分支有3个进入B，1个进入F，因此在堆栈中压入关于B和F的两个条目</li></ul></li><li>线程执行栈顶条目“G，B，1110”，掩码是1110代表这行条目对前三个线程active，走完B后进行分支，同理修改原来条目的nextPC，改成最近的重新收敛点E，同时添加两个分支条目。<ul><li>一般改成最近的重新收敛点，是为了从该位置将之前发散的线程以锁步的方式继续执行，便于同步</li><li>通常来说，在分支过后，最好是先将最多活跃线程的条目先入栈，少活跃线程的条目后入栈，例如d部分，而c部分的例子相反</li></ul></li></ol><h4 id="3-1-2-SIMT死锁与无堆栈SIMT结构">3.1.2 SIMT死锁与无堆栈SIMT结构</h4><p>SIMT基于堆栈的实现可能导致死锁：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-comment">// 将mutex置0代表资源空闲</span><br>*mutex = <span class="hljs-number">0</span>;<br><span class="hljs-comment">// atomicCAS读取mutex，若为0，则置1（即若空闲，则访问），返回mutex原始值</span><br><span class="hljs-comment">// 一个warp中的所有线程都执行，因此只有一个线程看到mutex=0，其他都看到=1</span><br><span class="hljs-keyword">while</span>(!atomicCAS(mutex,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>));<br><span class="hljs-comment">// 释放mutex</span><br>atomicExch(mutex,<span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><p>简而言之，对于一个互斥资源，当一个warp的所有线程同时执行互斥锁式访问时，只有一个线程拿到资源，其他线程陷入原地等待。但是，拿到资源的线程在执行完毕后，达到了上文中的重新收敛点，会等待其他所有线程一起到这个点，才能继续执行第三句释放锁。</p><p>无堆栈分支的重新收敛机制：warp收敛屏障</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>GPGPA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>00：个人网站部署</title>
    <link href="/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/"/>
    <url>/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>花了两三天时间搭了一个用于笔记的个人博客，部署在了 github 上，这里记录一下部署的过程。</p><span id="more"></span><h2 id="选型">选型</h2><p>网站的建立主要是为了搭一个在公司和家里都能访问的博客环境，对工作和学习做一些记录，所以直接放弃传统的带前端后端的动态页面，时间成本太高，整一个可以一键上传 markdown 的静态页面就挺ok。前端框架采用 hexo，UI选择 fluid，代码放在 github 上，并使用 github action 进行持续集成，部署到 github pages 后，后续写作只需要一次 git push 就可以自动将文章更新到目标网站上。</p><h2 id="框架搭建">框架搭建</h2><p><a href="https://hexo.io/zh-cn/docs/">https://hexo.io/zh-cn/docs/</a></p><h2 id="UI设置">UI设置</h2><p><a href="https://hexo.fluid-dev.com/docs/guide/">https://hexo.fluid-dev.com/docs/guide/</a></p><h2 id="部署">部署</h2><h3 id="快速部署">快速部署</h3><p><a href="https://hexo.io/zh-cn/docs/one-command-deployment">https://hexo.io/zh-cn/docs/one-command-deployment</a></p><p>适用于希望源代码保存在本地而不用上传的情况，相当于本地构建完再将构建好的网页直接推给gh page</p><p>需要在 _config.yml 中配置 gh page 的仓库地址和分支，推送后，hexo 会将 public 目录中的文件推送至_config.yml 中指定的分支中，并且完全覆盖该分支下的已有内容。</p><p>这就导致了一个问题，由于是只传 public 目录，域名映射需要的CNAME文件只能放到 public 下，这样每次 hexo clean 后会清空 public，还得再编辑一次CNAME，但是好处在于刨除了云端构建的不稳定性，每次可以本地看看网站效果，再直接放到 gh page 中</p><h3 id="gh-actions-持续集成">gh actions 持续集成</h3><p><a href="https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html">https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html</a></p><p>源代码放到 <a href="http://user.github.io">user.github.io</a> 仓库中后（仓库名只能设为这个，否则生成网页会变成 <a href="http://user.github.io">user.github.io</a> 的子页），CNAME 放在 source 中，然后在 .github/workflws 中定义 gh actions 的详细配置</p><p>采用的hexo官方文档中的配置，最后一步使用 peaceiris/actions-gh-pages@v3 咱也不太懂，参考知乎上的其他配置，大概相当于安装 hexo 完了在将 main 分支的源码 deploy 到 gh-pages 分支上，之后在设置时选择这个分支即可</p><p>主要问题在于每次 push 都要重新 build，推测后期内容增多后网站更新会十分不及时，可能需要看看别人的追加更新是咋弄的</p><h3 id="CDN加速">CDN加速</h3><p>更换 Cloudflare 的 DNS，注意 SSL/TLS 加密模式设为严格</p><h3 id="增加评论区">增加评论区</h3><p>一直拖着没弄（本来也没指望有人会评论hhh）朋友推荐选择valine，基本上属于开箱即用了<br>参考： <a href="https://valine.js.org/quickstart.html">https://valine.js.org/quickstart.html</a></p><h2 id="Summary">Summary</h2><p>属于我的第0篇博客，大概，能在网站上正常显示，证明基本功能已经ok</p><h2 id="TODO">TODO</h2>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>前端</tag>
      
      <tag>记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
