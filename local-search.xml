<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>DianNao系列论文阅读</title>
    <link href="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning"><a href="#DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning" class="headerlink" title="DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning"></a>DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ol><li>DianNao：一个用于普遍机器学习的小规模高通量加速器。寒武纪开山之作</li><li>现阶段（2014）机器学习工作中（CNN、神经网络）CPU性能不够，GPU、FPGA等功耗过高。</li><li>当前的绝大多数acc的关注点都在于算法计算部分（efficiently implementing the computational part of the algorithms），然而CNN和DNN的特点是大尺寸、大计算量。在这种情况下，DMA的效果不太好。因此我们针对这一特性，设计了一个专门对访存做特别优化的加速器</li><li>对比常规SIMD处理器速度快117倍，能耗比提高21倍</li></ol><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><ol><li>加速器设计的权衡：灵活性和高性能。而由于当时机器学习的SOTA就是CNN和DNN，种类有限，所以可以这类计算设计出有针对性的加速器</li><li>当前针对CNN或MLP的加速器都专注于神经网络中计算原语如卷积的有效实现，如矩阵乘法、向量计算方面的优化。但是忽略了对性能影响同样巨大的访存部分</li><li>由于阿姆达尔定律，即使计算原件做大量优化，整体性能依旧会受制于内存传输部分，并且在机器学习领域，为了实现更高的精度和功能，有一个必然的趋势就是提高神经网络的规模，而这也证明了访存优化对于设计加速卡的重要性</li><li>主要贡献：高吞吐、高能耗比、侧重内存性能的加速卡设计</li></ol><h3 id="最新机器学习技术入门"><a href="#最新机器学习技术入门" class="headerlink" title="最新机器学习技术入门"></a>最新机器学习技术入门</h3><ol><li>当前的市场来说，我们的加速器应该聚焦于网络的前馈而非反馈。这是由于在许多业务场景下离线学习都是主流，网路可以周期性的进行离线学习，而拿到客户手中只需要高效的前向推理。并且由于反向传播和正向路径原理类似，我们在之后也会针对反向传播做出优化工作</li></ol><h3 id="基于处理器的（大型）神经网络实现"><a href="#基于处理器的（大型）神经网络实现" class="headerlink" title="基于处理器的（大型）神经网络实现"></a>基于处理器的（大型）神经网络实现</h3><p><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image.png" alt=" "></p><ol><li>classifier层<ul><li>就是全连接层，在一般的理解中，每一个output相当于所有input的加权求和在Sigmoid出的概率值，如图所示，synapses矩阵对于每个output来说该input的权重</li><li>首先展示的是针对全连接层的优化，一般比较符合直观思维的模式是逐行运算，每次输出一个output。这种方式在遇到大规模神经网络（i&#x2F;o神经元数量大）时有一个问题：对带宽要求高，内存运输总数&#x3D; inputs loaded + synapses loaded + outputs written &#x3D; Ni x Nn + Ni x Nn + Nn</li><li>改进方式：将输入神经元进行tile loop，在L1缓存不够大的情况下，平铺再分块，类似卷积层，这样做的好处是对input神经元的数据做了复用，大大降低了输入神经元的内存带宽需求，略微增加了输出带宽需求（因为不再是一次出结果，需要written多次）</li><li>同时如果将synapses矩阵存入L2缓存（对于当时的神经网络来说，权重总数在百万数量级，仍在L2的范围内），可以进一步减少所需带宽</li></ul></li></ol><p><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-1.png" alt=" "><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-2.png" alt=" "></p><ol start="2"><li><p>卷积层</p><ul><li>有两种数据复用可能：滑动窗口（就是卷积核权重，滑动过程中卷积核不变）以及跨通道的输出复用</li><li>简单来说就是每次不再都整个input feature map，而是在map上截取一片Tx*Ty，每次不同的卷积核在上面卷积运算之后送给output，再最后加权求和</li><li>无非是由于缓存空间限制而对feture maps、Input channel、output channel做出截取，让运算时数据都在缓存中，降低内存带宽</li><li>针对卷积核权值共享做了优化，文章倾向于共享卷积核</li></ul></li><li><p>池化层:</p><ul><li>重用机会少，对于增加Tx，Ty效果不显著</li></ul></li></ol><h3 id="小型神经网络加速器"><a href="#小型神经网络加速器" class="headerlink" title="小型神经网络加速器"></a>小型神经网络加速器</h3><ul><li>对于小型网络，可假设所有神经元和突触都由硬件实现，内存仅用于I&#x2F;O</li><li>对于小型神经网络可以大幅提高能耗，但随神经元数量增多面积、能量和延迟呈二次方增长</li></ul><h3 id="大型神经网络加速器"><a href="#大型神经网络加速器" class="headerlink" title="大型神经网络加速器"></a>大型神经网络加速器</h3><p><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-3.png" alt=" "></p><ol><li><p>总共三大部件：</p><ul><li>运算部分：NFU</li><li>存储部分：输入缓存NBin、输出缓存NBout、突触权重SB</li><li>控制逻辑：CP</li></ul></li><li><p>NFU：</p><ul><li>按照第三节，将每一层分解为Ti和Tn的计算块</li><li>流水线：layer都可以分为若干个规范计算单元的组合，将整个计算规范化，流程化</li></ul><ol><li>NFU-1：乘法单元</li><li>NFU-2：加法树</li><li>NFU-3：激活单元</li></ol></li><li><ol><li>全连接层：突出*输入；乘积求和；激活函数sigmoid；</li><li>卷积层：计算阶段相同，只是激活函数可能不同；</li><li>池化层：没有乘积的操作，可以是求最大池化和平均池化</li></ol></li></ol><h2 id="DaDianNao-A-Machine-Learning-Supercomputer"><a href="#DaDianNao-A-Machine-Learning-Supercomputer" class="headerlink" title="DaDianNao: A Machine-Learning Supercomputer"></a>DaDianNao: A Machine-Learning Supercomputer</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><ol><li>在多核芯片中，由于CNNs和DNNs所需内存并未超过其片上存储空间，结合CNN&#x2F;DNN算法自身特点，会导致高内部带宽和低外部通信这一情况，从而能在合理的区域成本下实现高并发。</li></ol><h3 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h3><ol><li>前人工作缺陷：加速芯片要么有神经网络大小限制，要么对于大型网络，神经元和突触必须存储在内存中</li><li>神经网络性能瓶颈：内存访问</li></ol><h3 id="The-GPU-Option-The-Accelerator-Option"><a href="#The-GPU-Option-The-Accelerator-Option" class="headerlink" title="The GPU Option&#x2F; The Accelerator Option"></a>The GPU Option&#x2F; The Accelerator Option</h3><ol><li>CPU与GPU与DianNao的一些比较；DianNao的介绍，现有DianNao的不足</li><li>主要的限制来源于两种重要层的内存带宽需求：私有内核的卷积层(用于dnn)和全连接层</li></ol><h3 id="A-Machine-Learning-Supercomputer"><a href="#A-Machine-Learning-Supercomputer" class="headerlink" title="A Machine-Learning Supercomputer"></a>A Machine-Learning Supercomputer</h3><ol><li><p>权重存储在将使用它们的神经元附近，最大限度地降低数据移动，节省时间和能量;架构是完全分布式的，没有主存</p></li><li><p>非对称的体系结构，其中每个节点占用的空间大量偏向于存储而不是计算</p></li><li><p>传递神经元值而不是权重，因为在两个典型层中，神经元值比权重数量级小，需要相对较少的外部(跨芯片)带宽</p></li><li><p>通过将本地存储分解成许多块来实现高的内部带宽</p></li><li><p>将SRAM更换为eDRAM，缩小面积，但需要周期性刷新、延迟高</p></li><li><p>NFU无法简单扩大规模：数据布线面积占用过多，可扩展性差</p></li><li><p>对NFU进行分片</p></li></ol><h2 id="Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks"><a href="#Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks" class="headerlink" title="Cambricon: An Instruction Set Architecture for Neural Networks"></a>Cambricon: An Instruction Set Architecture for Neural Networks</h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><ol><li>传统神经网络往往在CPU、GPGPU这样的的通用平台执行，通常来说不够节能，因为这种平台主要是为了灵活支持各类型工作</li><li>最近的一些硬件加速器，这类加速器通常采用高级指令直接控制高级功能块。但是，当需要灵活支持各种不同的NN时，这种直接控制块的方式就不太行</li><li>思路 a. 分解大块的操作成一个个小的指令，获得更大的灵活性。用户可以用低层次的操作组合成高层次的功能。b. 简单的短指令可以大幅降低设计验证的复查度和解码器的功耗和面积。</li></ol>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DianNao</tag>
      
      <tag>论文阅读</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>00：个人网站部署</title>
    <link href="/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/"/>
    <url>/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>花了两三天时间搭了一个用于笔记的个人博客，部署在了 github 上，这里记录一下部署的过程。</p><span id="more"></span><h2 id="选型"><a href="#选型" class="headerlink" title="选型"></a>选型</h2><p>网站的建立主要是为了搭一个在公司和家里都能访问的博客环境，对工作和学习做一些记录，所以直接放弃传统的带前端后端的动态页面，时间成本太高，整一个可以一键上传 markdown 的静态页面就挺ok。前端框架采用 hexo，UI选择 fluid，代码放在 github 上，并使用 github action 进行持续集成，部署到 github pages 后，后续写作只需要一次 git push 就可以自动将文章更新到目标网站上。</p><h2 id="框架搭建"><a href="#框架搭建" class="headerlink" title="框架搭建"></a>框架搭建</h2><p><a href="https://hexo.io/zh-cn/docs/">https://hexo.io/zh-cn/docs/</a></p><h2 id="UI设置"><a href="#UI设置" class="headerlink" title="UI设置"></a>UI设置</h2><p><a href="https://hexo.fluid-dev.com/docs/guide/">https://hexo.fluid-dev.com/docs/guide/</a></p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="快速部署"><a href="#快速部署" class="headerlink" title="快速部署"></a>快速部署</h3><p><a href="https://hexo.io/zh-cn/docs/one-command-deployment">https://hexo.io/zh-cn/docs/one-command-deployment</a></p><p>适用于希望源代码保存在本地而不用上传的情况，相当于本地构建完再将构建好的网页直接推给gh page</p><p>需要在 _config.yml 中配置 gh page 的仓库地址和分支，推送后，hexo 会将 public 目录中的文件推送至_config.yml 中指定的分支中，并且完全覆盖该分支下的已有内容。</p><p>这就导致了一个问题，由于是只传 public 目录，域名映射需要的CNAME文件只能放到 public 下，这样每次 hexo clean 后会清空 public，还得再编辑一次CNAME，但是好处在于刨除了云端构建的不稳定性，每次可以本地看看网站效果，再直接放到 gh page 中</p><h3 id="gh-actions-持续集成"><a href="#gh-actions-持续集成" class="headerlink" title="gh actions 持续集成"></a>gh actions 持续集成</h3><p><a href="https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html">https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html</a></p><p>源代码放到 user.github.io 仓库中后（仓库名只能设为这个，否则生成网页会变成 user.github.io 的子页），CNAME 放在 source 中，然后在 .github&#x2F;workflws 中定义 gh actions 的详细配置</p><p>采用的hexo官方文档中的配置，最后一步使用 peaceiris&#x2F;actions-gh-pages@v3 咱也不太懂，参考知乎上的其他配置，大概相当于安装 hexo 完了在将 main 分支的源码 deploy 到 gh-pages 分支上，之后在设置时选择这个分支即可</p><p>主要问题在于每次 push 都要重新 build，推测后期内容增多后网站更新会十分不及时，可能需要看看别人的追加更新是咋弄的</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>属于我的第0篇博客，大概，能在网站上正常显示，证明基本功能已经ok</p><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>前端</tag>
      
      <tag>流程记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
