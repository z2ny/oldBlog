<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Dive into Deeplearning</title>
    <link href="/2023/09/14/Dive-into-Deeplearning/"/>
    <url>/2023/09/14/Dive-into-Deeplearning/</url>
    
    <content type="html"><![CDATA[<p>作为一切的开始，最近想要重新整理一份笔记，便于后期快速复习</p><span id="more"></span><p>参考：</p><ul><li><a href="https://zh.d2l.ai/index.html">https://zh.d2l.ai/index.html</a></li><li><a href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497&ctype=0">https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497&amp;ctype=0</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>课程笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GPGPA阅读</title>
    <link href="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/"/>
    <url>/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="General-Purpose-Graphics-Processor-Architectures"><a href="#General-Purpose-Graphics-Processor-Architectures" class="headerlink" title="General-Purpose Graphics Processor Architectures"></a>General-Purpose Graphics Processor Architectures</h1><h2 id="Abstract-Preface"><a href="#Abstract-Preface" class="headerlink" title="Abstract &amp;&amp; Preface"></a>Abstract &amp;&amp; Preface</h2><p>与CPU相比，GPU可以更加聚焦与计算，因此性能和效率更高。——通用可编程GPU</p><p>章节介绍：GPU基本结构与历史 —— GPU编程模型 —— GPU计算核心的体系结构 —— 计算核心与内存系统的交叉研究</p><h2 id="Chapter-1-Intro"><a href="#Chapter-1-Intro" class="headerlink" title="Chapter 1: Intro"></a>Chapter 1: Intro</h2><h3 id="1-1-计算加速器的前景"><a href="#1-1-计算加速器的前景" class="headerlink" title="1.1 计算加速器的前景"></a>1.1 计算加速器的前景</h3><p>过去，计算系统的性能提升大部分依赖于工艺的进步，使得晶体管尺寸缩小，从而提升集成度，使得运行速度更快。</p><p>Dennard Scaling：从05年开始晶体管缩放规则失效。因此为了提高性能，需要找到更高效的硬件架构。</p><p>hardware specialization：定制化硬件，可以使能效比大幅提高，两大方向：</p><ul><li>硬件向量化，消除指令处理的开销</li><li>优化计算过程，减少数据运输开销</li></ul><p>计算架构的关键：专业化硬件带来的收益与支持广泛程序所需的灵活性之间的平衡。相比于专用加速器（例如google的TPU），仍然需要GPU这种较为通用的计算硬件。</p><p>Turing-complete：图灵完备，只要给足够的时间与内存，GPU可以完成一切运算。</p><h3 id="1-2-GPU硬件基础"><a href="#1-2-GPU硬件基础" class="headerlink" title="1.2 GPU硬件基础"></a>1.2 GPU硬件基础</h3><p>GPU不会完全取代CPU：GPU不是独立的计算设备，通常来说，CPU负责在GPU上启动计算并负责GPU上的数据传输。当前访问I&#x2F;O设备或提供OS服务的软件主要还是运行在CPU上（这些软件缺乏大规模并行性），因此，需要首先考虑GPU与CPU的交互。</p><ul><li>独立GPU：两个U各有各的mem，同时核心通过PCIE总线进行数据传输。注意对于独显来说，两个U的mem的DRAM技术通常是不一样的，CPU的DRAM通常针对低延迟访问进行优化（DDR），而GPU的DRAM通常针对高吞吐进行优化（GDDR）。</li><li>集成GPU：两个U共享一个cache，cache与一个内存进行数据交互。由于共享内存所以只能采取单一技术，集成式GPU通常搭载在低功耗设备上，因此DRAM通常针对低功耗进行优化（LPDDR）。</li></ul><p>一个GPU计算应用会从CPU上开始，通常，该应用程序的CPU部分负责分配和初始化一些数据结构。在旧的N卡和A卡上，CPU需要为CPU和GPU内存中的数据结构分配空间，并协调数据从CPUmem到GPUmem的移动。在新的N卡（Pascal，10系）上的软硬件支持数据从Cmem到Gmem的自动传输，这项技术通过利用虚拟内存支持来实现，NV称之为unified memory。对于集显来说不存在数据mem传输的问题，但是由于两个U共享cache并且有些cache可能是私有的，因此也需要关注缓存一致性问题 (cache-coherence) 。</p><p>启动GPU运算一般需要驱动程序完成，在GPU启动运算前，CPU通过驱动程序指定GPU运行哪些代码，这些代码称为内核（kernel），同时，CPU还需要指定线程数、每个线程的数据位置等等。配置完毕后，CPU向GPU发出信号，GPU开始运算。</p><p>现代GPU由许多核心（SIMT Core）组成，NV称之为流式多处理器(Streaming Multiprocessor, SM)，AMD称之为计算单元(compute unit)，每个核心都执行一个与此时运行的内核相关的单指令多线程程序，一个核心可以运行上千个线程，这些线程通过暂存区mem进行通信，并使用快速屏障技术（fast barrier operations）进行同步。每个核心同时还有一级指令和一级缓存，这些缓存可以充当带宽过滤器，减少向低级别内存的流量，当拥有大量线程时，可以隐藏由于有时某线程的缓存未命中而访问内存带来的的性能下滑。</p><p>高计算吞吐需要高内存带宽的支持，这又对内存系统的并行性提出要求。这种并行性又多通道内存实现，每个通道与内存分区中的最后一级缓存（LLC）相连，GPU核心通过片上互连网络与内存分区相连。也有一些替代的方案，例如Intel的Xeon Phi，就是将LLC直接交由GPU核心分配</p><p>对于高并发任务来说，GPU相对超标量无序CPU拥有更高的单位面积性能，因为GPU可以将其芯片面积的大部分专用于算术逻辑单元，并相应的减小控制逻辑的面积。</p><p>09年出来一个性能随线程变化的分析模型。模型显示：</p><ol><li>当少量线程共享大缓存时（如多核CPU），性能会随着线程数量的增加而提高。</li><li>当线程数增加到缓存无法容纳整个工作集时，性能反而会随着线程数量增加而下降。</li><li>但是随着线程数量的进一步增加，性能会随着多线程隐藏片外延迟的能力而提高。<br>   GPU就是通过采用多线程来容忍频繁的缓存未命中，提高运算性能。</li></ol><p>内存访问不仅降低性能，同时也会提高能耗。新的GPGPU架构的重点是改善内存访问。</p><h3 id="1-3-GPU简史"><a href="#1-3-GPU简史" class="headerlink" title="1.3 GPU简史"></a>1.3 GPU简史</h3><h3 id="1-4-书籍大纲"><a href="#1-4-书籍大纲" class="headerlink" title="1.4 书籍大纲"></a>1.4 书籍大纲</h3><ul><li>第二章：编程模型、代码开发过程、编译流程</li><li>第三章：单个GPU核心（SM）的体系结构</li><li>第四章：内存系统</li><li>第五章：其他研究</li></ul><h2 id="Chapter-2：编程模型"><a href="#Chapter-2：编程模型" class="headerlink" title="Chapter 2：编程模型"></a>Chapter 2：编程模型</h2><p>现代GPU广泛采用SIMD硬件来利用数据级并行，但GPU的计算API（如NV的cuda和AMD的opencl）并不向程序员暴露SIMD的硬件，而是采取类似MIMD的编程模型，允许程序员在GPU上启动大量标量线程。其中的每一个标量线程都有自己独特的执行路径，并都可以访问内存。运行时，GPU上的的SIMD硬件利用线程的规律性和空间局部性，同步启动这些标量线程组，称为SIMT（单指令多线程）</p><ul><li>SIMD：例如两个向量，对两个向量的每一个分量进行相同的op操作，输出为一个向量（在一个线程里，受ALU宽度限制）</li><li>SIMT：与SIMD在一个线程公用一个ALU不同，SIMT有多个线程，每个线程各有各的ALU和自己的数据，但执行的指令相同（但是由于数据不同，执行指令时的控制分支可能会不一样）</li></ul><h3 id="2-1-运行模型"><a href="#2-1-运行模型" class="headerlink" title="2.1 运行模型"></a>2.1 运行模型</h3><p>为GPU优化的代码很可能在CPU架构上表现不佳。假定一个 单精度标量A * 向量X + 向量Y 的函数实现：</p><p>CPU实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-type">void</span> <span class="hljs-title function_">saxpy_serial</span><span class="hljs-params">(<span class="hljs-type">int</span> n, <span class="hljs-type">float</span> a, <span class="hljs-type">float</span> *x, <span class="hljs-type">float</span> *y)</span><br>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; ++i)<br>        y[i] = a * x[i] + y[i];<br>&#125;<br>main()<br>&#123;<br>    <span class="hljs-type">float</span> *x, *y;<br>    <span class="hljs-type">int</span> n;<br>    <span class="hljs-comment">// 省略*x、*y的赋值操作</span><br>    saxpy_serial(n, <span class="hljs-number">2.0</span>, x, y);<br>    <span class="hljs-comment">// 省略内存释放操作</span><br>&#125;<br></code></pre></td></tr></table></figure><p>GPU-CUDA实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs C">__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">saxpy</span><span class="hljs-params">(<span class="hljs-type">int</span> n, <span class="hljs-type">float</span> a, <span class="hljs-type">float</span> *x, <span class="hljs-type">float</span> *y)</span> <span class="hljs-comment">// __global__代表函数在GPU上运行</span><br>&#123;<br>   <span class="hljs-comment">// 每个线程都有各自的blockIdx.x、blockDim.x、threadIdx.x</span><br>   <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>   <span class="hljs-comment">// threadIdx.x代表线程在线程块中的x坐标</span><br>   <span class="hljs-comment">// blockIdx.x代表线程所属的线程块在grid中的x坐标</span><br>   <span class="hljs-comment">// blockDim.x一个线程块在x维度的最大线程数</span><br>   <span class="hljs-comment">// 一般来说线程在线程块中有xyz三个坐标，线程块在网格中也有xyz三个坐标，这里省略y，z</span><br>   <span class="hljs-keyword">if</span> (i &lt; n)<br>      y[i] = a * x[i] + y[i];<br>&#125;<br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>   <span class="hljs-comment">// 一般h_表示cpu的内存指针，d_表示gpu的内存指针</span><br>   <span class="hljs-type">float</span> *h_x, *h_y;<br>   <span class="hljs-type">int</span> n;<br>   <span class="hljs-comment">// 省略*h_x、*h_y的赋值操作</span><br>   <span class="hljs-type">float</span> *d_x, *d_y;<br>   <span class="hljs-type">int</span> nblocks = (n + <span class="hljs-number">255</span>) / <span class="hljs-number">256</span>;<br>   <span class="hljs-comment">// 调用GPU驱动程序并要求分配gpu内存，并将这一片内存的地址赋给&amp;d_x</span><br>   cudaMalloc(&amp;d_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));<br>   cudaMalloc(&amp;d_y, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));<br>   <span class="hljs-comment">// 将h_x指向的内容赋值给d_x指向的区域</span><br>   cudaMemcpy(d_x, h_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>   cudaMemcpy(d_y, h_y, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>   <span class="hljs-comment">// 交由GPU，并启动nblocks个线程块（Thread Block，或CTA），每个线程块256个线程，所有的线程块组成一个grid，即本次内核的计算单元</span><br>   saxpy&lt;&lt;&lt;nblocks, <span class="hljs-number">256</span>&gt;&gt;&gt;(n, <span class="hljs-number">2.0</span>, d_x, d_y);<br>   <span class="hljs-comment">// 为了提高效率，每个线程块中，每32个线程以锁步形式组成一组warp，warp往上再组成线程块</span><br>   <span class="hljs-comment">// 一个warp包含多少线程是硬件概念，而一个线程块可以有多少线程则是软件概念（当然得是warp的整数倍）</span><br>   <span class="hljs-comment">// 将计算结果返回给CPU内存</span><br>   cudaMemcpy(h_x, d_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyDeviceToHost);<br>   <span class="hljs-comment">// 省略内存释放操作</span><br>&#125;<br></code></pre></td></tr></table></figure><p>硬件：一个GPU有多个SM，每个SM包含多个SP（Stream Processor）</p><p>软件：一个GPU内核对应一个grid，一个grid包含多个CTA。CTA中有多个warp，每个warp包含固定数量的线程数（与SM中SP数量相同）。</p><p>GPU在运算时，可能是一个grid独占GPU，也可以多个grid并行跑GPU；SM对应的工作单元是CTA，其中的基本执行单元是warp（即每个SP对应一个warp中的线程），在某个warp受阻时SM可以切换同一个CTA中的其他warp，但只有该CTA执行完，才切换其他CTA</p><p>CTA中的线程之间可以通过暂存器内存互相通信（NV称之为共享内存），同步也轻松，同时每个SM中也有一个共享内存，可以分配给在该SM上运行的所有CTA</p><p>不同CTA中的线程也可以通过所有线程都能访问的全局地址空间通信，但代价较高</p><h3 id="2-2-指令模型"><a href="#2-2-指令模型" class="headerlink" title="2.2 指令模型"></a>2.2 指令模型</h3><p>NV的并行线程执行ISA：Parallel Thread eXecution，简称PTX （虚拟指令，类似汇编指令）</p><p>GPU运行PTX代码前，需要编译（汇编）成实际的机器指令，NV称此为SASS（Streaming ASSembler），该过程有NV的工具包完成，并没有开放，这使得NV可以在硬件级别提供向后兼容性，每一代都可以重新设计ISA架构</p><h2 id="Chapter-3：SIMT核心：指令和寄存器数据流"><a href="#Chapter-3：SIMT核心：指令和寄存器数据流" class="headerlink" title="Chapter 3：SIMT核心：指令和寄存器数据流"></a>Chapter 3：SIMT核心：指令和寄存器数据流</h2><p>对传统图形渲染来说，GPU通常需要访问详细的纹理图，这样的数据集因为太大不可能完全缓存在芯片上，因此有必要采用能够维持大片外带宽的GPU架构。所以如今的GPU都往高并发线程发展（大概意思是线程越多越能够隐藏访存损失）。并且，尽管每个线程的片上缓存很小，但因为局部性原理，仍然可以有效减少大量的片外存储访问。</p><p>SM的微体系结构，流水线分为SIMT前端和SIMD后端，共3个循环：</p><ol><li>取值（fetch）循环：fetch、I-Cache、Decode和I-Buffer模块</li><li>发指（issue）循环：I-Buffer、Scoreboard、issue和SIMT stack模块</li><li>寄存器访问调度循环：Operand Collector、ALU和Memory模块</li></ol><p><img src="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/image.png" alt=" "></p><h3 id="3-1-单循环近似"><a href="#3-1-单循环近似" class="headerlink" title="3.1 单循环近似"></a>3.1 单循环近似</h3><p>线程的调度单位是warp（AMD称之为wavefronts）。每一个周期，SM选择一个warp进行调度。</p><p>单循环中，warp的程序计数器（PC）用于访问指令存储器时查找为warp执行的下一条指令。获得指令后，对指令解码，并找到源操作数寄存器。与此同时，SIMT的执行掩码值也被确定。</p><p>在执行掩码与源寄存器可用后，执行以SIMD的方式进行。如果设置了SIMT执行掩码，则每个线程都在与通路关联的功能单元上执行。与现代CPU一样，功能单元通常异构，即不同的单元支持不同的指令运行。</p><p>每个功能单元在名义上包含的通路数与warp的线程数相同，但也有一些GPU使用不同的实现，使其中的warp在多个时钟周期内执行。</p><h4 id="3-1-1-SIMT执行掩码"><a href="#3-1-1-SIMT执行掩码" class="headerlink" title="3.1.1 SIMT执行掩码"></a>3.1.1 SIMT执行掩码</h4><p>现代GPU的关键特性是SIMT执行模型，为程序员提供了单个线程完全独立执行的抽象，这是通过传统谓词（prediction）与SIMT谓词掩码堆栈结合实现的。</p><p>SIMT堆栈有助于处理线程可以独立执行时出现的两个关键问题：</p><ol><li>嵌套控制流</li><li>完全跳过计算</li></ol><table><thead><tr><th align="center"><img src="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/image-1.png" alt=" "></th><th align="center"><img src="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/image-2.png" alt=" "></th><th align="center"><img src="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/image-3.png" alt=" "></th></tr></thead></table><p>假设每个warp有四个线程，所有线程都执行了A基本块，之后遵循不同的控制流，有3个线程进入B，1个线程进入F。如此流动，最后所有线程统一到达G。</p><table><thead><tr><th align="center"><img src="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/image-4.png" alt=" "></th><th align="center"><img src="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/image-5.png" alt=" "></th></tr></thead></table><p>堆栈包括三项：重新收敛程序计数器(Reconvergence program counter, RPC)、要执行的下一条指令的地址(Next PC)和活跃掩码(active mask)。warp每次都执行栈顶指针指向的条目的nextPC指向的代码块</p><ol><li>开始时堆栈中只有一个条目“-，A，1111”。代表所有线程都将进入A。</li><li>所有4个线程在走完A之后进行分支，此时需要有3处修改：<ul><li>将原先条目的nextPC值修改成分支后的重新汇聚点，对于这次分支B和F，将在G重新汇聚，因此将第一条的nextPC由A修改为G</li><li>这次分支有3个进入B，1个进入F，因此在堆栈中压入关于B和F的两个条目</li></ul></li><li>线程执行栈顶条目“G，B，1110”，掩码是1110代表这行条目对前三个线程active，走完B后进行分支，同理修改原来条目的nextPC，改成最近的重新收敛点E，同时添加两个分支条目。<ul><li>一般改成最近的重新收敛点，是为了从该位置将之前发散的线程以锁步的方式继续执行，便于同步</li><li>通常来说，在分支过后，最好是先将最多活跃线程的条目先入栈，少活跃线程的条目后入栈，例如d部分，而c部分的例子相反</li></ul></li></ol><h4 id="3-1-2-SIMT死锁与无堆栈SIMT结构"><a href="#3-1-2-SIMT死锁与无堆栈SIMT结构" class="headerlink" title="3.1.2 SIMT死锁与无堆栈SIMT结构"></a>3.1.2 SIMT死锁与无堆栈SIMT结构</h4><p>SIMT基于堆栈的实现可能导致死锁：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-comment">// 将mutex置0代表资源空闲</span><br>*mutex = <span class="hljs-number">0</span>;<br><span class="hljs-comment">// atomicCAS读取mutex，若为0，则置1（即若空闲，则访问），返回mutex原始值</span><br><span class="hljs-comment">// 一个warp中的所有线程都执行，因此只有一个线程看到mutex=0，其他都看到=1</span><br><span class="hljs-keyword">while</span>(!atomicCAS(mutex,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>));<br><span class="hljs-comment">// 释放mutex</span><br>atomicExch(mutex,<span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><p>简而言之，对于一个互斥资源，当一个warp的所有线程同时执行互斥锁式访问时，只有一个线程拿到资源，其他线程陷入原地等待。但是，拿到资源的线程在执行完毕后，达到了上文中的重新收敛点，会等待其他所有线程一起到这个点，才能继续执行第三句释放锁。</p><p>无堆栈分支的重新收敛机制：warp收敛屏障</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文笔记</tag>
      
      <tag>GPGPA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DianNao系列论文阅读</title>
    <link href="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning"><a href="#DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning" class="headerlink" title="DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning"></a>DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>DianNao：一个用于普遍机器学习的小规模高通量加速器。寒武纪开山之作</p><p>现阶段（2014）机器学习工作中（CNN、神经网络）CPU性能不够，GPU、FPGA等功耗过高。</p><p>当前的绝大多数acc的关注点都在于算法计算部分（efficiently implementing the computational part of the algorithms），然而CNN和DNN的特点是大尺寸、大计算量。在这种情况下，DMA的效果不太好。因此我们针对这一特性，设计了一个专门对访存做特别优化的加速器</p><p>对比常规SIMD处理器速度快117倍，能耗比提高21倍</p><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>加速器设计的权衡：灵活性和高性能。而由于当时机器学习的SOTA就是CNN和DNN，种类有限，所以可以这类计算设计出有针对性的加速器</p><p>当前针对CNN或MLP的加速器都专注于神经网络中计算原语如卷积的有效实现，如矩阵乘法、向量计算方面的优化。但是忽略了对性能影响同样巨大的访存部分</p><p>由于阿姆达尔定律，即使计算原件做大量优化，整体性能依旧会受制于内存传输部分，并且在机器学习领域，为了实现更高的精度和功能，有一个必然的趋势就是提高神经网络的规模，而这也证明了访存优化对于设计加速卡的重要性</p><p>主要贡献：高吞吐、高能耗比、侧重内存性能的加速卡设计</p><h3 id="最新机器学习技术入门"><a href="#最新机器学习技术入门" class="headerlink" title="最新机器学习技术入门"></a>最新机器学习技术入门</h3><p>当前的市场来说，我们的加速器应该聚焦于网络的前馈而非反馈。这是由于在许多业务场景下离线学习都是主流，网路可以周期性的进行离线学习，而拿到客户手中只需要高效的前向推理。并且由于反向传播和正向路径原理类似，我们在之后也会针对反向传播做出优化工作</p><h3 id="基于处理器的（大型）神经网络实现"><a href="#基于处理器的（大型）神经网络实现" class="headerlink" title="基于处理器的（大型）神经网络实现"></a>基于处理器的（大型）神经网络实现</h3><table><thead><tr><th align="center"><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image.png" alt=" "></th><th align="center"><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-1.png" alt=" "></th><th align="center"><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-2.png" alt=" "></th></tr></thead></table><ol><li><p>classifier层</p><ul><li>就是全连接层，在一般的理解中，每一个output相当于所有input的加权求和在Sigmoid出的概率值，如图所示，synapses矩阵对于每个output来说该input的权重</li><li>首先展示的是针对全连接层的优化，一般比较符合直观思维的模式是逐行运算，每次输出一个output。这种方式在遇到大规模神经网络（i&#x2F;o神经元数量大）时有一个问题：对带宽要求高，内存运输总数&#x3D; inputs loaded + synapses loaded + outputs written &#x3D; Ni x Nn + Ni x Nn + Nn</li><li>改进方式：将输入神经元进行tile loop，在L1缓存不够大的情况下，平铺再分块，类似卷积层，这样做的好处是对input神经元的数据做了复用，大大降低了输入神经元的内存带宽需求，略微增加了输出带宽需求（因为不再是一次出结果，需要written多次）</li><li>同时如果将synapses矩阵存入L2缓存（对于当时的神经网络来说，权重总数在百万数量级，仍在L2的范围内），可以进一步减少所需带宽</li></ul></li><li><p>卷积层</p><ul><li>有两种数据复用可能：滑动窗口（就是卷积核权重，滑动过程中卷积核不变）以及跨通道的输出复用</li><li>简单来说就是每次不再都整个input feature map，而是在map上截取一片Tx*Ty，每次不同的卷积核在上面卷积运算之后送给output，再最后加权求和</li><li>无非是由于缓存空间限制而对feture maps、Input channel、output channel做出截取，让运算时数据都在缓存中，降低内存带宽</li><li>针对卷积核权值共享做了优化，文章倾向于共享卷积核</li></ul></li><li><p>池化层:</p><ul><li>重用机会少，对于增加Tx，Ty效果不显著</li></ul></li></ol><h3 id="小型神经网络加速器"><a href="#小型神经网络加速器" class="headerlink" title="小型神经网络加速器"></a>小型神经网络加速器</h3><ul><li>对于小型网络，可假设所有神经元和突触都由硬件实现，内存仅用于I&#x2F;O</li><li>对于小型神经网络可以大幅提高能耗，但随神经元数量增多面积、能量和延迟呈二次方增长</li></ul><h3 id="大型神经网络加速器"><a href="#大型神经网络加速器" class="headerlink" title="大型神经网络加速器"></a>大型神经网络加速器</h3><p><img src="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-3.png" alt=" "></p><p>总共三大部件：</p><ul><li>运算部分：NFU</li><li>存储部分：输入缓存NBin、输出缓存NBout、突触权重SB</li><li>控制逻辑：CP</li></ul><p>NFU：</p><ul><li>按照第三节，将每一层分解为Ti和Tn的计算块</li><li>流水线：layer都可以分为若干个规范计算单元的组合，将整个计算规范化，流程化<ol><li>NFU-1：乘法单元</li><li>NFU-2：加法树</li><li>NFU-3：激活单元</li></ol></li></ul><p>全连接层：突出*输入；乘积求和；激活函数sigmoid；<br>卷积层：计算阶段相同，只是激活函数可能不同；<br>池化层：没有乘积的操作，可以是求最大池化和平均池化</p><h2 id="DaDianNao-A-Machine-Learning-Supercomputer"><a href="#DaDianNao-A-Machine-Learning-Supercomputer" class="headerlink" title="DaDianNao: A Machine-Learning Supercomputer"></a>DaDianNao: A Machine-Learning Supercomputer</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>在多核芯片中，由于CNNs和DNNs所需内存并未超过其片上存储空间，结合CNN&#x2F;DNN算法自身特点，会导致高内部带宽和低外部通信这一情况，从而能在合理的区域成本下实现高并发。</p><h3 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h3><p>前人工作缺陷：加速芯片要么有神经网络大小限制，要么对于大型网络，神经元和突触必须存储在内存中</p><p>神经网络性能瓶颈：内存访问</p><h3 id="The-GPU-Option-The-Accelerator-Option"><a href="#The-GPU-Option-The-Accelerator-Option" class="headerlink" title="The GPU Option&#x2F; The Accelerator Option"></a>The GPU Option&#x2F; The Accelerator Option</h3><p>CPU与GPU与DianNao的一些比较；DianNao的介绍，现有DianNao的不足</p><p>主要的限制来源于两种重要层的内存带宽需求：私有内核的卷积层(用于dnn)和全连接层</p><h3 id="A-Machine-Learning-Supercomputer"><a href="#A-Machine-Learning-Supercomputer" class="headerlink" title="A Machine-Learning Supercomputer"></a>A Machine-Learning Supercomputer</h3><p>权重存储在将使用它们的神经元附近，最大限度地降低数据移动，节省时间和能量;架构是完全分布式的，没有主存</p><p>非对称的体系结构，其中每个节点占用的空间大量偏向于存储而不是计算</p><p>传递神经元值而不是权重，因为在两个典型层中，神经元值比权重数量级小，需要相对较少的外部(跨芯片)带宽</p><p>通过将本地存储分解成许多块来实现高的内部带宽</p><p>将SRAM更换为eDRAM，缩小面积，但需要周期性刷新、延迟高</p><p>NFU无法简单扩大规模：数据布线面积占用过多，可扩展性差</p><p>对NFU进行分片</p><h2 id="Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks"><a href="#Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks" class="headerlink" title="Cambricon: An Instruction Set Architecture for Neural Networks"></a>Cambricon: An Instruction Set Architecture for Neural Networks</h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>传统神经网络往往在CPU、GPGPU这样的的通用平台执行，通常来说不够节能，因为这种平台主要是为了灵活支持各类型工作</p><p>最近的一些硬件加速器，这类加速器通常采用高级指令直接控制高级功能块。但是，当需要灵活支持各种不同的NN时，这种直接控制块的方式就不太行</p><p>思路 a. 分解大块的操作成一个个小的指令，获得更大的灵活性。用户可以用低层次的操作组合成高层次的功能。b. 简单的短指令可以大幅降低设计验证的复查度和解码器的功耗和面积。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DianNao</tag>
      
      <tag>论文笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>00：个人网站部署</title>
    <link href="/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/"/>
    <url>/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>花了两三天时间搭了一个用于笔记的个人博客，部署在了 github 上，这里记录一下部署的过程。</p><span id="more"></span><h2 id="选型"><a href="#选型" class="headerlink" title="选型"></a>选型</h2><p>网站的建立主要是为了搭一个在公司和家里都能访问的博客环境，对工作和学习做一些记录，所以直接放弃传统的带前端后端的动态页面，时间成本太高，整一个可以一键上传 markdown 的静态页面就挺ok。前端框架采用 hexo，UI选择 fluid，代码放在 github 上，并使用 github action 进行持续集成，部署到 github pages 后，后续写作只需要一次 git push 就可以自动将文章更新到目标网站上。</p><h2 id="框架搭建"><a href="#框架搭建" class="headerlink" title="框架搭建"></a>框架搭建</h2><p><a href="https://hexo.io/zh-cn/docs/">https://hexo.io/zh-cn/docs/</a></p><h2 id="UI设置"><a href="#UI设置" class="headerlink" title="UI设置"></a>UI设置</h2><p><a href="https://hexo.fluid-dev.com/docs/guide/">https://hexo.fluid-dev.com/docs/guide/</a></p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="快速部署"><a href="#快速部署" class="headerlink" title="快速部署"></a>快速部署</h3><p><a href="https://hexo.io/zh-cn/docs/one-command-deployment">https://hexo.io/zh-cn/docs/one-command-deployment</a></p><p>适用于希望源代码保存在本地而不用上传的情况，相当于本地构建完再将构建好的网页直接推给gh page</p><p>需要在 _config.yml 中配置 gh page 的仓库地址和分支，推送后，hexo 会将 public 目录中的文件推送至_config.yml 中指定的分支中，并且完全覆盖该分支下的已有内容。</p><p>这就导致了一个问题，由于是只传 public 目录，域名映射需要的CNAME文件只能放到 public 下，这样每次 hexo clean 后会清空 public，还得再编辑一次CNAME，但是好处在于刨除了云端构建的不稳定性，每次可以本地看看网站效果，再直接放到 gh page 中</p><h3 id="gh-actions-持续集成"><a href="#gh-actions-持续集成" class="headerlink" title="gh actions 持续集成"></a>gh actions 持续集成</h3><p><a href="https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html">https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html</a></p><p>源代码放到 user.github.io 仓库中后（仓库名只能设为这个，否则生成网页会变成 user.github.io 的子页），CNAME 放在 source 中，然后在 .github&#x2F;workflws 中定义 gh actions 的详细配置</p><p>采用的hexo官方文档中的配置，最后一步使用 peaceiris&#x2F;actions-gh-pages@v3 咱也不太懂，参考知乎上的其他配置，大概相当于安装 hexo 完了在将 main 分支的源码 deploy 到 gh-pages 分支上，之后在设置时选择这个分支即可</p><p>主要问题在于每次 push 都要重新 build，推测后期内容增多后网站更新会十分不及时，可能需要看看别人的追加更新是咋弄的</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>属于我的第0篇博客，大概，能在网站上正常显示，证明基本功能已经ok</p><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>前端</tag>
      
      <tag>流程记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
