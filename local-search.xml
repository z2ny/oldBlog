<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>TVM源码学习-00</title>
    <link href="/2023/11/16/TVM%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-00/"/>
    <url>/2023/11/16/TVM%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-00/</url>
    
    <content type="html"><![CDATA[<p>TVM v0.8 源码学习</p><span id="more"></span><h2 id="代码拉取">代码拉取</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --recursive https://github.com/apache/tvm.git<br>git checkout v0.8<br></code></pre></td></tr></table></figure><h2 id="编译安装">编译安装</h2><h3 id="编译常识">编译常识</h3><h4 id="GCC">GCC</h4><p>对于小项目来说，文件数量较少，使用GCC直接进行编译即可</p><ol><li>g++:将源文件编译成.out可执行文件</li><li>g++ -c:将源文件编译成中间文件 而不进行链接，即编译成.o文件</li><li>g++ -o:将源文件或者.o文件进行编译+链接或链接，生成.out文件</li><li>对于包含多个源文件的项目，可以将源文件分别gcc -c，再将产生的文件链接到一起，如<code>g++ -o myprogram file1.o file2.o</code></li></ol><h4 id="make-makefile">make &amp; makefile</h4><p>然而随着计算机的发展，一个软件工程包含的源文件越来越多，手动逐个编译完全不可行，于是有个make和makefile。</p><p>Make 是一个批处理工具，它根据 Makefile 文件中的规则来构建项目。Make 可以确定哪些文件需要重新编译，哪些文件已经是最新的，从而只编译需要编译的文件。</p><p>Makefile：Makefile 是 Make 的配置文件，它包含了一系列的规则，用于指定如何构建项目。Make 通过读取 Makefile 文件来构建项目。</p><p>在这一阶段，工程师可以手写项目的makefile文件，再使用make指令统一构建整个项目</p><h4 id="Cmake-CMakeLists">Cmake &amp; CMakeLists</h4><p>makefile在一些简单的工程下，完全可以人工手写，但是问题又来了，工程非常大的时候，连 makefile 的手写也非常麻烦，这时就需要一个工具可以自动生成 makefile ，这个工具就是cmake。</p><p>CMakeLists 是 Cmake 的配置文件。还是需要手写。</p><p>Cmake 会根据 CMakeLists 自动生成项目的 makefile 文件，然后再使用 make 构建项目。</p><p>Cmake 有不同的生成器，可以生成不同平台下的 makefile 文件，比如 Unix Makefile、Visual Studio、Ninja、Nmake等等，可以生成不同平台下的makefile，生成后再进行make就可以将项目构建在不同的平台下。</p><p>ninja是一种注重速度的生成器，使用ninja生成会产生一个build.ninja文件，然后使用ninja而非make进行构建</p><h3 id="编译TVM">编译TVM</h3><p>首先在 <a href="https://winlibs.com/">https://winlibs.com/</a> 拿到带 LLVM 库的 GCC 包，安装并加入环境变量，在cmd中可以使用 <code>llvm-config --libdir</code> 验证</p><p>创建build目录并<code>cp cmake/config.cmake build</code>并自定义配置，把USE LLVM打开</p><p>Conda创建tvm的虚拟环境，可以直接 <code>conda env create --file conda/build-environment.yaml</code> 但是使用该环境 git 会莫名奇妙出bug，不如手动创环境下载该文件中提到的依赖包</p><p>安装visual studio，把桌面C++开发组件勾上，安装完成后在cmd中使用<code>cl</code>验证（有没有可能除了装一个vs上述步骤全部不需要，但是不管了，官方文档安装部分相当混乱）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> build<br><span class="hljs-built_in">cd</span> build<br>cmake -A x64 -Thost=x64 ..<br><span class="hljs-built_in">cd</span> ..<br>cmake --build build --config Release -- /m<br></code></pre></td></tr></table></figure><h3 id="安装python包">安装python包</h3><p>在import tvm时如果未找到包，vscode会自动在工作区下创建配置帮你把tvm的路径加到 <code>python.analysis.extraPaths</code> 但实测虽然变绿且鼠标可以左键跳转了，但解释器还是找不到，可能需要改全局的python路径啥的</p><p>不如直接安装，环境变量引入包的好处在于源码更改后，引入可以立即感知；而install的包在每次源码更改后要重新install才能生效。但是我使用的v0.8的源码，已经没有更新了，所以直接安装也ok</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> python<br>python setup.py install<br></code></pre></td></tr></table></figure><h2 id="用户手册">用户手册</h2><p>tvm/gallery</p><h3 id="introduction-py"><a href="http://introduction.py">introduction.py</a></h3><p><img src="image.png" alt="Alt text"></p><p>TVM编译步骤：</p><ol><li><p>从 TensorFlow、PyTorch 或 ONNX 等框架导入模型。在导入阶段中，TVM 可以从其他框架（如 TensorFlow、PyTorch 或 ONNX）中提取模型。 TVM 为前端提供的支持水平会随着我们不断改进这个开源项目而变化。如果在将模型导入 TVM 时遇到问题，可以将其转换为 ONNX。</p></li><li><p>翻译成 TVM 的高级模型语言 Relay。已导入 TVM 的模型在 Relay 中表示。Relay 是神经网络的功能语言和中间表示（IR）。Relay 应用图级优化 pass 来优化模型。</p></li><li><p>降级为张量表达式（TE）表示。降级是指将较高级的表示转换为较低级的表示。应用了高级优化之后，Relay 通过运行 FuseOps pass，把模型划分为许多小的子图，并将子图降级为 TE 表示。张量表达式（TE）是一种用于描述张量计算的领域特定语言。 TE 还提供了几个 schedule 原语来指定底层循环优化，例如循环切分、矢量化、并行化、循环展开和融合。为将 Relay 表示转换为 TE 表示，TVM 包含了一个张量算子清单（TOPI），其中包含常用张量算子的预定义模板（例如，conv2d、transpose）。</p></li><li><p>使用 auto-tuning 模块 AutoTVM 或 AutoScheduler 搜索最佳 schedule。schedule 为 TE 中定义的算子或子图指定底层循环优化。auto-tuning 模块搜索最佳 schedule，并将其与 cost model 和设备上的测量值进行比较。TVM 中有两个 auto-tuning 模块，AutoTVM（有模板）和Ansor（无模板）。</p></li><li><p>为模型编译选择最佳配置。调优后，auto-tuning 模块会生成 JSON 格式的调优记录。此步骤为每个子图选择最佳 schedule。</p></li><li><p>降级为张量中间表示（TIR，TVM 的底层中间表示）。基于调优步骤选择最佳配置后，所有 TE 子图降级为 TIR 并通过底层优化 pass 进行优化。接下来，优化的 TIR 降级为硬件平台的目标编译器。这是生成可部署到生产的优化模型的最终代码生成阶段。</p></li><li><p>编译成机器码。compiler-specific 的生成代码最终可降级为机器码。 TVM 可将模型编译为可链接对象模块，然后轻量级 TVM runtime 可以用 C 语言的 API 来动态加载模型，也可以为 Python 和 Rust 等其他语言提供入口点。或将 runtime 和模型放在同一个 package 里时，TVM 可以对其构建捆绑部署。</p></li></ol><h3 id="autotvm-relay-x86-py">autotvm_relay_x86.py</h3><p>使用TVM的Python API编译、训练和调优与训练的模型。</p><h4 id="导入依赖，并下载和加载ONNX模型">导入依赖，并下载和加载ONNX模型</h4><p>使用ResNet-50 v2，一个50层的用于图像分类的卷积神经网络，可以使用<a href="https://netron.app/">Netron</a>检查模型结构。输入图像为224×224，模型已经预训练好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> onnx<br><span class="hljs-keyword">from</span> tvm.contrib.download <span class="hljs-keyword">import</span> download_testdata<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tvm.relay <span class="hljs-keyword">as</span> relay<br><span class="hljs-keyword">import</span> tvm<br><span class="hljs-keyword">from</span> tvm.contrib <span class="hljs-keyword">import</span> graph_executor<br><br>model_url = (<br>    <span class="hljs-string">&quot;https://github.com/onnx/models/raw/main/&quot;</span><br>    <span class="hljs-string">&quot;vision/classification/resnet/model/&quot;</span><br>    <span class="hljs-string">&quot;resnet50-v2-7.onnx&quot;</span><br>)<br><br>model_path = download_testdata(model_url, <span class="hljs-string">&quot;resnet50-v2-7.onnx&quot;</span>, module=<span class="hljs-string">&quot;onnx&quot;</span>)<br>onnx_model = onnx.load(model_path)<br><br><span class="hljs-comment"># 为 numpy 的 RNG 设置 seed，保证每次复现得到一致的结果</span><br>np.random.seed(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="下载和预处理图像">下载和预处理图像</h4><p>将图像转为Numpy数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">img_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&quot;</span><br>img_path = download_testdata(img_url, <span class="hljs-string">&quot;imagenet_cat.png&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-comment"># 重设大小为 224x224</span><br>resized_image = Image.<span class="hljs-built_in">open</span>(img_path).resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br>img_data = np.asarray(resized_image).astype(<span class="hljs-string">&quot;float32&quot;</span>)<br><br><span class="hljs-comment"># 输入图像是 HWC 布局，而 ONNX 需要 CHW 输入，所以转换数组</span><br>img_data = np.transpose(img_data, (<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br><br><span class="hljs-comment"># 根据 ImageNet 输入规范进行归一化</span><br>imagenet_mean = np.array([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>]).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>imagenet_stddev = np.array([<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>norm_img_data = (img_data / <span class="hljs-number">255</span> - imagenet_mean) / imagenet_stddev<br><br><span class="hljs-comment"># 添加 batch 维度，期望 4 维输入：NCHW。</span><br><span class="hljs-comment"># N:number of samples，即batch size C:channels H:height W:width</span><br>img_data = np.expand_dims(norm_img_data, axis=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="使用Relay编译模型">使用Relay编译模型</h4><p>将模型转为Relay中间表示（IR）。首先用 from_onnx 导入器将模型导入到 Relay 中。然后，用标准优化，将模型构建到 TVM 库中，最后从库中创建一个 TVM 计算图 runtime 模块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入名称可能因模型类型而异</span><br><span class="hljs-comment"># 可用 Netron 工具检查输入名称</span><br>input_name = <span class="hljs-string">&quot;data&quot;</span><br>target = <span class="hljs-string">&quot;llvm&quot;</span><br><br><span class="hljs-comment"># 定义输入的形状字典</span><br>shape_dict = &#123;input_name: img_data.shape&#125;<br><br><span class="hljs-comment"># 将ONNX模型转换为Relay中间表示（IR）</span><br>mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)<br><br><span class="hljs-comment"># 使用优化级别为3的PassContext进行编译</span><br><span class="hljs-keyword">with</span> tvm.transform.PassContext(opt_level=<span class="hljs-number">3</span>):<br>    <span class="hljs-comment"># 使用Relay构建模型并编译为目标为&quot;llvm&quot;的库</span><br>    lib = relay.build(mod, target=target, params=params)<br><br><span class="hljs-comment"># 创建&quot;llvm&quot;设备</span><br>dev = tvm.device(target, <span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 创建GraphModule并加载编译好的模块</span><br>module = graph_executor.GraphModule(lib[<span class="hljs-string">&quot;default&quot;</span>](dev))<br></code></pre></td></tr></table></figure><h4 id="使用模型进行推理">使用模型进行推理</h4><p>直接运行模型进行预测，并将输出转为可读形式。并收集此时还未优化过的模型基本性能数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 运行模型</span><br>dtype = <span class="hljs-string">&quot;float32&quot;</span><br>module.set_input(input_name, img_data)<br>module.run()<br>output_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br>tvm_output = module.get_output(<span class="hljs-number">0</span>, tvm.nd.empty(output_shape)).numpy()<br><br><span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> softmax<br><br><span class="hljs-comment"># 下载标签列表</span><br>labels_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;</span><br>labels_path = download_testdata(labels_url, <span class="hljs-string">&quot;synset.txt&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(labels_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    labels = [l.rstrip() <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> f]<br><br><span class="hljs-comment"># 打开输出文件并读取输出张量</span><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br><br><span class="hljs-comment"># 运行时间评估</span><br><span class="hljs-keyword">import</span> timeit<br>timing_number = <span class="hljs-number">10</span><br>timing_repeat = <span class="hljs-number">10</span><br>unoptimized = (<br>    np.array(timeit.Timer(<span class="hljs-keyword">lambda</span>: module.run()).repeat(repeat=timing_repeat, number=timing_number))<br>    * <span class="hljs-number">1000</span><br>    / timing_number<br>)<br>unoptimized = &#123;<br>    <span class="hljs-string">&quot;mean&quot;</span>: np.mean(unoptimized),<br>    <span class="hljs-string">&quot;median&quot;</span>: np.median(unoptimized),<br>    <span class="hljs-string">&quot;std&quot;</span>: np.std(unoptimized),<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="输出后处理">输出后处理</h4><p>用专为该模型提供的查找表，运行一些后处理（post-processing），从而使得 ResNet-50 v2 的输出形式更具有可读性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> softmax<br><br><span class="hljs-comment"># 下载标签列表</span><br>labels_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;</span><br>labels_path = download_testdata(labels_url, <span class="hljs-string">&quot;synset.txt&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(labels_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    labels = [l.rstrip() <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> f]<br><br><span class="hljs-comment"># 打开输出文件并读取输出张量</span><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br></code></pre></td></tr></table></figure><h4 id="进行模型调优">进行模型调优</h4><p>用编译的模块推理，有时可能无法获得预期的性能。在这种情况下，可用自动调优器更好地配置模型，从而提高性能。 TVM 中的调优是指，在给定 target 上优化模型，使其运行得更快。与训练或微调不同，它不会影响模型的准确性，而只会影响 runtime 性能。作为调优过程的一部分，TVM 实现并运行许多不同算子的变体，以查看哪个性能最佳。这些运行的结果存储在调优记录文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tvm.auto_scheduler <span class="hljs-keyword">as</span> auto_scheduler<br><span class="hljs-keyword">from</span> tvm.autotvm.tuner <span class="hljs-keyword">import</span> XGBTuner<br><span class="hljs-keyword">from</span> tvm <span class="hljs-keyword">import</span> autotvm<br><br>number = <span class="hljs-number">10</span><br>repeat = <span class="hljs-number">1</span><br>min_repeat_ms = <span class="hljs-number">0</span>  <span class="hljs-comment"># 调优 CPU 时设置为 0</span><br>timeout = <span class="hljs-number">10</span>  <span class="hljs-comment"># 秒</span><br><br><span class="hljs-comment"># 创建 TVM 运行器</span><br>runner = autotvm.LocalRunner(<br>    <span class="hljs-comment"># 将要测试的不同配置的数量</span><br>    number=number,<br>    <span class="hljs-comment"># 每个配置重复次数</span><br>    repeat=repeat,<br>    <span class="hljs-comment"># 每次测试运行时间上限</span><br>    timeout=timeout,<br>    <span class="hljs-comment"># 指定运行配置测试需要多长时间，如果重复次数低于此时间，则增加其值</span><br>    min_repeat_ms=min_repeat_ms,<br>    enable_cpu_cache_flush=<span class="hljs-literal">True</span>,<br>)<br><br>tuning_option = &#123;<br>    <span class="hljs-string">&quot;tuner&quot;</span>: <span class="hljs-string">&quot;xgb&quot;</span>,<br>    <span class="hljs-comment"># 试验次数，CPU上推荐1500，GPU推荐3000-4000，此处仅作展示用</span><br>    <span class="hljs-string">&quot;trials&quot;</span>: <span class="hljs-number">20</span>,<br>    <span class="hljs-comment"># 使搜索提前停止的实验最小值</span><br>    <span class="hljs-string">&quot;early_stopping&quot;</span>: <span class="hljs-number">100</span>,<br><br>    <span class="hljs-string">&quot;measure_option&quot;</span>: autotvm.measure_option(<br>        builder=autotvm.LocalBuilder(build_func=<span class="hljs-string">&quot;default&quot;</span>),<br>        runner=runner<br>    ),<br>    <span class="hljs-comment"># 调优数据保存的文件名</span><br>    <span class="hljs-string">&quot;tuning_records&quot;</span>: <span class="hljs-string">&quot;resnet-50-v2-autotuning.json&quot;</span>,<br>&#125;<br></code></pre></td></tr></table></figure><p>在 <code>extract_from_program</code> 时卡了很久，还未发现原因</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先从 onnx 模型中提取任务</span><br>tasks = autotvm.task.extract_from_program(mod[<span class="hljs-string">&quot;main&quot;</span>], target=target, params=params)<br><br><span class="hljs-comment"># 按顺序调优提取的任务</span><br><span class="hljs-keyword">for</span> i, task <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tasks):<br>    prefix = <span class="hljs-string">&quot;[Task %2d/%2d] &quot;</span> % (i + <span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(tasks))<br><br>    <span class="hljs-comment"># choose tuner</span><br>    tuner = <span class="hljs-string">&quot;xgb&quot;</span><br><br>    <span class="hljs-comment"># create tuner</span><br>    <span class="hljs-keyword">if</span> tuner == <span class="hljs-string">&quot;xgb&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;reg&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_knob&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;reg&quot;</span>, feature_type=<span class="hljs-string">&quot;knob&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_itervar&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;reg&quot;</span>, feature_type=<span class="hljs-string">&quot;itervar&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_curve&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;reg&quot;</span>, feature_type=<span class="hljs-string">&quot;curve&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank_knob&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank&quot;</span>, feature_type=<span class="hljs-string">&quot;knob&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank_itervar&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank&quot;</span>, feature_type=<span class="hljs-string">&quot;itervar&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank_curve&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank&quot;</span>, feature_type=<span class="hljs-string">&quot;curve&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank_binary&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank-binary&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank_binary_knob&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank-binary&quot;</span>, feature_type=<span class="hljs-string">&quot;knob&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank_binary_itervar&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank-binary&quot;</span>, feature_type=<span class="hljs-string">&quot;itervar&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;xgb_rank_binary_curve&quot;</span>:<br>        tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank-binary&quot;</span>, feature_type=<span class="hljs-string">&quot;curve&quot;</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;ga&quot;</span>:<br>        tuner_obj = GATuner(task, pop_size=<span class="hljs-number">50</span>)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;random&quot;</span>:<br>        tuner_obj = RandomTuner(task)<br>    <span class="hljs-keyword">elif</span> tuner == <span class="hljs-string">&quot;gridsearch&quot;</span>:<br>        tuner_obj = GridSearchTuner(task)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Invalid tuner: &quot;</span> + tuner)<br><br>    tuner_obj.tune(<br>        n_trial=<span class="hljs-built_in">min</span>(tuning_option[<span class="hljs-string">&quot;trials&quot;</span>], <span class="hljs-built_in">len</span>(task.config_space)),<br>        early_stopping=tuning_option[<span class="hljs-string">&quot;early_stopping&quot;</span>],<br>        measure_option=tuning_option[<span class="hljs-string">&quot;measure_option&quot;</span>],<br>        callbacks=[<br>            autotvm.callback.progress_bar(tuning_option[<span class="hljs-string">&quot;trials&quot;</span>], prefix=prefix),<br>            autotvm.callback.log_to_file(tuning_option[<span class="hljs-string">&quot;tuning_records&quot;</span>]),<br>        ],<br>    )<br></code></pre></td></tr></table></figure><h4 id="使用生成的调优数据优化编译模型">使用生成的调优数据优化编译模型</h4><p>获取上述存储在<code>resnet-50-v2-autotuning.json</code> 中的调优记录，并使用该结果为指定target上的模型生成高性能代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> autotvm.apply_history_best(tuning_option[<span class="hljs-string">&quot;tuning_records&quot;</span>]):<br>    <span class="hljs-keyword">with</span> tvm.transform.PassContext(opt_level=<span class="hljs-number">3</span>, config=&#123;&#125;):<br>        lib = relay.build(mod, target=target, params=params)<br><br>dev = tvm.device(<span class="hljs-built_in">str</span>(target), <span class="hljs-number">0</span>)<br>module = graph_executor.GraphModule(lib[<span class="hljs-string">&quot;default&quot;</span>](dev))<br><br><span class="hljs-comment"># 验证模型是否产生相同的结果</span><br>dtype = <span class="hljs-string">&quot;float32&quot;</span><br>module.set_input(input_name, img_data)<br>module.run()<br>output_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br>tvm_output = module.get_output(<span class="hljs-number">0</span>, tvm.nd.empty(output_shape)).numpy()<br><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br></code></pre></td></tr></table></figure><h4 id="比较调优前后的模型性能">比较调优前后的模型性能</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> timeit<br><br>timing_number = <span class="hljs-number">10</span><br>timing_repeat = <span class="hljs-number">10</span><br>optimized = (<br>    np.array(timeit.Timer(<span class="hljs-keyword">lambda</span>: module.run()).repeat(repeat=timing_repeat, number=timing_number))<br>    * <span class="hljs-number">1000</span><br>    / timing_number<br>)<br>optimized = &#123;<span class="hljs-string">&quot;mean&quot;</span>: np.mean(optimized), <span class="hljs-string">&quot;median&quot;</span>: np.median(optimized), <span class="hljs-string">&quot;std&quot;</span>: np.std(optimized)&#125;<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;optimized: %s&quot;</span> % (optimized))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;unoptimized: %s&quot;</span> % (unoptimized))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TVM</tag>
      
      <tag>编译</tag>
      
      <tag>cmake</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI编译器论文阅读-01</title>
    <link href="/2023/11/13/AI%E7%BC%96%E8%AF%91%E5%99%A8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-01/"/>
    <url>/2023/11/13/AI%E7%BC%96%E8%AF%91%E5%99%A8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-01/</url>
    
    <content type="html"><![CDATA[<p>TVM以及autoTVM的原文，一作都是陈天奇。</p><p><em>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning ——2018</em></p><p><em>Learning to Optimize Tensor Programs ——2019</em></p><span id="more"></span><h1><em>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</em></h1><h2 id="Abstract">Abstract</h2><p>人工智能的需求越来越高，机器学习的应用场景越来越广blabla</p><p>我们提供了一种AI编译器TVM。TVM向外界暴露出图级和算子级的优化工作，以在不同的硬件平台为深度学习工作负载提供性能可移植性。</p><p>TVM解决了深度学习特有的优化难题，如算子融合、硬件原语映射、内存延迟隐藏等。他还采用了一种新型的、基于继续学习的成本建模方法来探索自动调优问题。实验表明效果很好，已经有大公司在使用。</p><h2 id="1-Intro">1. Intro</h2><p>现有的DL框架如TensorFlow、Pytorch等都是依靠计算图的中间表示来优化，如自动微分和动态内存管理等。但是图级优化过于高级，无法看到和处理特定硬件后端的算子级别优化。大多数后端的优化都是依赖于高度定制化的算子库，当前，在各种DL框架中为各种硬件后端提供支持需要大量的工程努力。</p><blockquote><p>即使对于已经支持的后端，框架也必须在一下两种方案之间做出艰难选择：1. 避免产生不在预定义算子库的算子而实现图优化 2. 使用包括新算子但尚未优化的图 （对高速发展的机器学习来说，新算子是不可避免的）</p></blockquote><p>为了对不同的硬件后端启用图级和算子级的优化，作者采用了一种完全不同的端到端的方法。作者构建了 TVM，这是一个编译器，它从现有框架中获取深度学习程序的高级规范，并为各种硬件后端生成低级优化代码。</p><p>简单来说，TVM要解决的问题是<strong>适配</strong>。将模型部署到不同的硬件上依赖硬件厂商的定制化库，需要大量的手工优化。而TVM这种端到端的编译器方案解决了不同框架下的不同模型部署到不同硬件平台上的适配问题。</p><p>方案的核心包括三大问题：</p><ol><li>如何进行高级别的图优化</li><li>如何进行硬件/算子级别的优化</li><li>如何解决组合搜索空间的问题</li></ol><h2 id="2-Overview">2. Overview</h2><p>TVM组件</p><p><img src="image.png" alt="Alt text"></p><p>TVM系统首先将现有框架模型作为输入，并转为计算图表示，然后执行高级数据流重写以生成优化图。在算子级优化层面，TVM为给定硬件目标的算子识别一组可能的代码优化。并采用机器学习的代价模型来寻找优化的算子，最后进行代码生成。</p><h2 id="3-Optimizing-Computational-Graphs">3. Optimizing Computational Graphs</h2><p>计算图blablabla，实现了一些图级优化，如算子融合、常量折叠、静态内存规划、数据布局转换等</p><h2 id="4-Generating-Tensor-Operations">4. Generating Tensor Operations</h2><p>TVM 通过在每个硬件后端生成许多有效的实现并选择优化的实现，为每个算子生成高效的代码。这个过程建立在 Halide 将描述和计算规则 (或调度优化) 解耦的想法的基础上，并将其扩展为支持新的优化 (嵌套并行、张量和延迟隐藏) 和 广泛的硬件后端。</p><h3 id="4-1-张量表达与调度空间">4.1 张量表达与调度空间</h3><p>TVM引入张量表达式语言来支持自动代码生成，每个计算操作都指定输出的形状，和描述如何计算其中每个元素的表达式。这种语言不指定循环结构或其他执行细节，为后端添加硬件感知优化的灵活性。</p><p>通过调度原语来构建调度。</p><h3 id="4-2-协作式嵌套并行">4.2 协作式嵌套并行</h3><p>并行性是提高效率的关键，大多数现有的方案都采用一种称为嵌套并行的模型，这种模型使用并行调度原语来实现并行，每个任务递归地细分为子任务并分配到线程上。是一种无共享嵌套并行，因为工作线程无法在同一阶段查看兄弟线程的数据。</p><p>有一种无共享方式的替代方法。线程组可以协作获取他们都需要的数据放入共享内存空间中。这种优化利用GPU内存层次结构，实现跨线程的数据重写。TVM使用调度原语来支持这种优化。</p><p>在没有显示声明内存范围时，将自动将计算阶段标记为thread-local。共享任务必须计算组内所有工作线程的依赖关系，还必须正确插入内存同步屏障，以保证数据对消费者可见。</p><h3 id="4-3-张量化">4.3 张量化</h3><p>DL工作负载具有较高的计算强度，通常可以分解成张量算子，如矩阵乘法或一维卷积。这种自然分解引发了添加张量计算基元（原语）的趋势。调度原语的增加为基于调度的编译增加了挑战性，使用他们可以提高性能，但是编译框架必须无缝集成所有使用的调度原语。指令往往是可变长的，每个都具有不同的数据布局，此外由于新的硬件在源源不断的出现，我们需要一个可扩展的调度原语方案。</p><p>我们通过张量内在声明的机制使目标硬件的内在特性与调度分离，使TVM可以轻松支持新的硬件架构。这样就可以使用相同的张量表达式语言来声明每个新硬件的底层行为。此外，还引入了一种调度原语，用相应的内部函数来替换计算单元。编译器将计算模式与硬件的匹配降低到了相应的硬件内部。</p><h3 id="4-4-显式内存延迟隐藏">4.4 显式内存延迟隐藏</h3><p>延迟隐藏是指将内存操作与计算重叠以最大限度地利用内存和计算资源的过程。它需要不同的策略，具体取决于目标硬件后端。在 CPU 上，内存延迟隐藏是通过同步多线程 或 硬件预取 来隐式实现的。</p><p>GPU 依赖于许多线程的快速上下文切换。相比之下，诸如 TPU 之类的专用 DL 加速器通常倾向于使用解耦访问执行 (DAE) 架构记性更加精简的控制，并将细粒度同步的问题转移给软件。</p><h2 id="5-Automating-Optimization">5. Automating Optimization</h2><p>鉴于丰富的调度原语集合，剩下的问题是为 DL 模型的每一层找到最佳的算子实现。在这里，TVM 为与每一层关联的特定输入形状和布局创建了一个专门的算子。这种专门的优化提供了显著的性能优势 (与针对较小的形状和布局多样性的手工代码相比)，但它也提出了自动化的挑战。系统需要选择调度优化：例如修改循环顺序或优化内存层次结构，以及特定于调度的参数，例如切片大小和循环展开因子。这样的组合选择为每个硬件后端创建了算子实现的大的搜索空间。为了应对这一挑战，构建了一个具有两个主要组件的自动调度优化器：一个新配置的调度搜索器 以及 一个预测给定配置性能的机器学习成本模型。</p><p><img src="image-1.png" alt="Alt text"></p><h3 id="5-1-调度搜索空间的规范">5.1 调度搜索空间的规范</h3><p>我们构建了一种调度模板的规范API，来让开发者在配置空间中声明knob。这一规范允许开发者结合特定领域的知识，来指定可能的调度。同时我们还为每个硬件后端创建了一个通用主模板，该模板可以根据张量表达式语言表达的计算描述自动提取可能的knob。对上层来说，我们希望可以有尽可能多的配置选择，并让优化器来管理这种巨大配置空间造成的计算负担。优化器必须可以搜索数十亿可能的配置。</p><h3 id="5-2-基于机器学习的代价模型">5.2 基于机器学习的代价模型</h3><p>从大型配置空间中找到最佳调度有以下几种方法：</p><ol><li><p>黑盒优化，即自动调整。该方法用于调优高性能计算库。然而，自动调整需要许多实验来确定一个好的配置。</p></li><li><p>构建一个预定义的成本（代价）模型来指导特定硬件后端的配置搜索。理想情况下，一个完美的成本模型会考虑影响性能的所有因素：内存访问模式、数据重用、管道依赖性和线程模式等。不幸的是，由于现代硬件日益复杂，这种方法很繁琐。此外，每个新的硬件目标都需要一个新的 (预定义的) 成本模型。</p><p>而我们则采用统计的方法解决成本模型问题。这种方式的搜索器每次会输出能够提高算子性能的配置。对于每个调度配置，使用一个ML模型预测其运行时间。这种模型使用搜索期间手机的运行时测量数据进行训练，而不需要详细的硬件信息。在优化过程中，会定期更新模型，以提高后续的准确性。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>TVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI编译器论文阅读-00</title>
    <link href="/2023/10/31/AI%E7%BC%96%E8%AF%91%E5%99%A8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-00/"/>
    <url>/2023/10/31/AI%E7%BC%96%E8%AF%91%E5%99%A8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-00/</url>
    
    <content type="html"><![CDATA[<p>事已至此，先看论文吧</p><p><em>The Deep Learning Compiler: A Comprehensive Survey</em><br>2020的一篇有关AI编译器的综述，来自北航和清华</p><span id="more"></span><h1><em>The Deep Learning Compiler: A Comprehensive Survey</em></h1><h2 id="Abstract">Abstract</h2><p>由于AI芯片的高度定制化，使得在不同硬件上部署各种深度学习模型变得十分困难，这也推动了深度学习编译器的研究。业界因此推出了一些深度学习编译器，如Tensorflow XLA和TVM。与传统编译器类似的是，深度学习编译器将不同的深度学习框架中描述的深度学习模型作为输入，然后为不同的硬件生成优化后的代码作为输出。</p><p>然而现有的文章都没有全面分析深度学习编译器独特的设计架构。本文将对现有DL编译器做全面剖析，重点在DL的多级IR及前/后端优化。</p><p>这是第一篇关于DL编译器设计体系结构的综述性论文。</p><h2 id="1-Introduction">1. Introduction</h2><p>先讲深度学习对于各个领域的深远影响blablabla</p><p>然后说当前业界的几种主流框架，如TensorFlow、PyTorch，MXNet和CNTK等，同时提出这些框架如果需要支持新的模型，interoperability（互操作性，复用性？）会变得非常重要。为了提供复用性，我们又提出了ONNX，定义了一种DL模型的统一格式，以促进不同框架间模型的相互转换。</p><p>然后说各大公司对开发DL专用硬件的巨大投入。在可预见的未来，深度学习芯片的设计会变得越来越多样化。</p><p>要包容硬件的多样性，就必须将计算高效的映射到各种硬件上。在通用硬件上，一些专用的依赖库可以实现DL模型的高效计算，在许多专用的DL芯片上也有类似的库。但是依赖库的缺点是，库的开发和更新通常跟不上DL模型的飞速发展，无法真正有效的利用DL芯片。</p><p>为了解决DL库和其他工具的类似缺点，并减轻DL芯片上每个模型都需要手动优化的负担，DL社区开始发展DL编译器。针对不同的模型和硬件架构，DL编译器对模型定义到特定代码实现之间的转换进行的高度优化。同时还利用通用编译器如LLVM的成熟工具链，在各种硬件架构间提供更好的可移植性。与传统编译器类似，DL编译器也采用分层设计，包括前端、多级IR和后端。</p><p>本文贡献：blablabla</p><h2 id="2-Background">2. Background</h2><h3 id="2-1-深度学习框架">2.1 深度学习框架</h3><p>介绍目前流行的几种框架</p><h3 id="2-2-深度学习硬件">2.2 深度学习硬件</h3><p>当前AI芯片的粗略分类：</p><ol><li>通用AI硬件：GPGPU，如Nvidia的Volta架构，辅以深度学习加速库如cuDNN，已经一些TensorRT之类的库</li><li>专用AI硬件：如Google TPU</li><li>Neuromorphic Hardware：略略略</li></ol><h3 id="2-3-硬件相关的DL代码生成器">2.3 硬件相关的DL代码生成器</h3><p>讲FPGA，略略略</p><h2 id="3-COMMON-DESIGN-ARCHITECTURE-OF-DL-COMPILERS">3. COMMON DESIGN ARCHITECTURE OF DL COMPILERS</h2><p><img src="image.png" alt="Alt text"></p><p>DL编译器的通用架构一般包含两部分：编译器前端和后端，中间表示IR在前后端之间，处理优化工作。其中IR都分多级，高级IR用于前端，偏向硬件无关的转换和优化；低级IR用于后端，偏向硬件相关的转换和优化、代码生成、编译等</p><p>高阶IR即图IR，用于表示硬件无关的计算和控制流，设计难点在于计算和控制流的抽象。有了这种抽象能力就可以捕获和表示各种DL模型。其目标在于建立控制流以及算子与数据之间的依赖关系，并为图级优化提供接口。此外高阶IR还需要包含编译所需的语义信息，并为自定义算子提供可扩展性。</p><p>低阶IR用于硬件相关的优化和代码生成。因此，低阶IR更注重细节，反映硬件特性，准确表示硬件相关优化。并且应该能够在后端使用成熟的第三方工具链。</p><p>前端从DL框架获取模型作为输入，然后将其转化为计算图表示形式。为了支持不同的框架，前端需要支持并实现不同格式的相互转换。计算图的优化可以分为节点级别（消除nop和零维张量）、块级别（代数简化、算子融合）、数据流级别（CSE、DCE、静态内存规划）。生成的优化计算图会传递给后端。</p><p>后端收到高阶IR（计算图）后，将高阶IR转换为LLVM IR等第三方工具链，这样就可以利用已有工具完成通用优化和代码生成。此外，后端还可以利用DL模型和硬件特性的先验知识来优化代码生成，如硬件固有映射、内存分配、内存延迟隐藏、并行化、循环优化等。为了在大的优化空间中确定最佳参数，现有的DL编译器广泛采用两种方法，<strong>Auto-Tuing（如AutoTVM）和Auto-Scheduling（如AutoScheduler）</strong>。优化后的低阶IR再经过JIT或AOT编译，生成面向不同硬件目标的机器码。</p><h2 id="4-KEY-COMPONENTS-OF-DL-COMPILERS">4. KEY COMPONENTS OF DL COMPILERS</h2><h3 id="4-1-高阶IR">4.1 高阶IR</h3><p>传统编译器中采用的IR表示能力限制了DL模型中复杂计算的表达，现在的DL编译器都会采用高阶IR（称为图IR）和一些特殊设计，以求达到高效的代码优化。为了更好地理解DL编译器中使用的图IR，以下描述了图IR的表示和实现。</p><h4 id="4-1-1-图IR的表示形式">4.1.1 图IR的表示形式</h4><blockquote><p>图IR的表示方式决定了DL编译器分析图IR的方式</p></blockquote><p><strong>基于DAG的IR</strong></p><p>DAG即有向无环图，在DL编译器中，DAG的节点表示算子，边表示张量。通用编译器一般使用DDG即数据依赖图完成如公共子表达式消除CSE和死代码消除DCE之类的优化，而借助DAG，DL编译器同样可以实现这些。</p><p>基于DAG的IR由于表达方式的简单，便于编程和编译，但由于基于DAG的IR缺少计算范围定义，因而存在诸如语义二义性这类缺陷。</p><p><strong>基于Let-binding的IR</strong></p><p>基于Let绑定的IR-Let绑定是一种解决语义歧义的方法，它为Javascript等许多高级编程语言使用的某些范围有限的函数提供Let表达式。当使用let关键字定义表达式时，会生成一个let节点，然后它指向表达式中的运算符和变量，而不仅仅是将变量之间的计算关系构建为DAG。在基于DAG的编译器中，当进程需要获得一个表达式的返回值时，它首先访问相应的节点并搜索相关的节点，也称为递归下降技术。相反，基于let绑定的编译器计算出let表达式中变量的所有结果，并构建变量映射。当需要特定的结果时，编译器会查找此映射来决定表达式的结果。在DL编译器中，TVM的Relay IR同时采用了基于DAG的IR和基于let绑定的IR，以获得两者的好处。</p><p><strong>张量的计算表示</strong></p><p>不同的图IR表示张量计算的方式也不同</p><ol><li>基于函数的表示：XLA、nGraraph</li><li>Lambda表示：TVM</li><li>爱因斯坦符号表示</li></ol><h4 id="4-1-2-图IR的实现">4.1.2 图IR的实现</h4><p>DL编译器中的数据通常以张量的形式进行组织，也即多维数组。编译器可以通过内存指针访问张量，或通过更灵活的占位符方式表示。占位符需要包含张量每个维度的大小，有时也可以标记成未知。此外，出于优化的原因，DL编译器需要数据布局信息，也应该可以根据占位符推断迭代器的边界。</p><p><strong>占位符</strong></p><p>占位符广泛应用于符号编程，如Tensorflow。占位符用于提供数据给计算图，是一种具有明确形状信息的变量。无初始值，声明只分配必要内存。有助于计算与编译器执行的分离。</p><p><strong>未知形状表示</strong></p><p>与Tensorflow类似，TVM使用any表示未知维度。未知形状的表示对动态模型必不可少，但是要完全支持动态模型，还要放宽边界推断和维度检查，以及额外的机制来确保内存有效性。</p><p><strong>数据布局</strong></p><p>张量都是放在特定数据布局和形状下。数据布局用来描述张量在内存中的组织方式，一般是一个从逻辑索引到内存索引的映射方式。合适的数据布局对性能提升非常关键，特别是对于深度学习模型之类内存密集型的应用而言。数据布局通常包括维度顺序（NCHW or NHWC）、平铺（tiling）、填充（padding）、跨距（striding）等。</p><p>TVM和GLOW将数据布局当作一种算子的参数，因为计算和优化需要这些参数信息。</p><p><strong>边界推断</strong></p><p>尽管DL编译器使用张量的数据表示可以很方便的描述输入和输出，但这种方式在推断迭代器边界时会有困难。边界推断通常根据计算图和已知占位符，以迭代或递归的方式执行</p><p><strong>算子支持</strong></p><p>算子就是计算图中的节点，通常包含代数算子（加减乘除等）、神经网络算子（卷积、池化等）、张量算子（reshape、resize等）、广播\规约算子（min、argmin等），以及控制流算子（条件和循环等），下面对三个特定的算子进行说明：</p><ol><li>广播算子：。。。</li><li>控制流算子：任何控制流都可以通过递归和模式来实现，正因为这一点，Relay可通过函数式编程来描述复杂的深度神经网络。</li><li>Derivative算子：略略略 看不明白</li></ol><p><strong>算子定制</strong></p><p>程序员可以定制算子。</p><h4 id="4-1-3-讨论">4.1.3 讨论</h4><p>几乎所有的DL编译器都有其独特的高阶IR，重要的是高阶IR与硬件无关。</p><h3 id="4-2-低阶IR">4.2 低阶IR</h3><h4 id="4-2-1-实现">4.2.1 实现</h4><p>与高阶IR相比，低阶IR用更细的粒度描述了DL模型的计算，并提供计算调优和内存访问接口，实现目标相关的优化。本节将低阶IR分为三类：基于Halide的IR，基于多面体的IR和其他IR。</p><p><strong>基于Halide的IR</strong></p><p>TVM将Halide IR改进为独立的符号IR，主要的两点：首先，TVM消除了对LLVM的依赖，并重构了项目模块和Halide IR设计的结构，优化了代码组织，使得图IR和前端语言（如Python）更易于理解，并且通过运行时分发机制，可以方便地添加自定义算子。如此一来，可重用性也得到了改善。 其次，TVM还简化了从字符串匹配到指针匹配的变量定义，确保每个变量都具有一个定义位置，即，静态单赋值（Static Single-Assignment, SSA）。</p><p><strong>基于多面体的IR</strong></p><p>多面体（Polyhedral-based）模型是在某些DL编译器采用的一项重要技术。多面体模型使用线性编程、仿射变换和其它数学方法来优化基于loop循环的代码，代码可以具有边界和分支的静态控制流。与Halide相比，内存引用和循环嵌套的边界可以是多面体模型中具有任何形状的多面体。这种灵活性使多面体模型在通用编译器中得到广泛使用。但是，这种灵活性也妨碍了多面体模型与调优机制的集成。但是，由于能够处理深度嵌套的循环，很多DL编译器（例如TC和PlaidML）都采用了多面体模型作为其低阶IR。基于多面体的IR可以很容易地应用于各种多面体转换（例如融合、平铺、下沉和映射），不论是设备相关还是设备不相关的优化。基于多面体的编译器兼容很多其它工具链，例如isl、Omega、PIP、Polylib和PPL。</p><p>TC的低阶IR设计较为独特，其设计结合了Halide和多面体模型，也就是用基于Halide的IR表示计算，用基于多面体的IR表示loop结构。 TC通过抽象实例表示表达式，并引入了特定的节点类型。简而言之，TC用域节点来指定索引变量的范围，用上下文节点描述与硬件相关的、新的迭代变量，用band节点确定迭代顺序，用过滤器节点表示与语句实例结合的迭代器，用Set和sequence是指定过滤器的执行类型（并行和串行执行），用扩展节点来描述代码生成所需的其它必要指令，例如内存搬移。</p><p>PlaidML使用基于多面体的IR（称为Stripe）来表示张量操作，并且通过将并行多面体块的嵌套挤结构扩展到多个级别，创建可并行化代码的层次结构。PlaidML可以将嵌套的多面体分配给嵌套的内存单元，做到计算与存储层次结构的匹配。Stripe中的硬件配置独立于内核代码。 Stripe中的标记tag（在其他编译器中称为pass）不会更改内核结构，但会为优化pass提供硬件目标的其它信息。 Stripe将DL算子拆分为适合本地硬件资源的块（tile）。</p><p><strong>其他IR</strong></p><p>有些DL编译器无需使用Halide和多面体模型就可实现定制低阶IR。定制低阶IR将硬件相关的优化和降级操作应用到LLVM IR上。</p><p>Glow中的低阶IR是基于指令的表达式，可对通过地址引用的张量进行操作。Glow低阶IR中有两种基于指令的函数：声明和程序。声明用于表示在程序的整个生命周期都存在的常量内存区域数量（例如，输入、权重、偏置等）。程序是一系列本地分配的区域，包括函数（比如conv和pool）和临时变量。指令可以在全局内存区域或本地分配的区域上运行。此外，每个操作数都用一个限定符修饰。@in表示操作数从缓冲区中读取；@out表示操作数写入缓冲区；@inout表示操作数读取和写入缓冲区。这些指令和操作数限定符可帮助Glow确定在什么时候可以执行什么样的内存优化。</p><p>MLIR受LLVM的影响很大，但MLIR是一个比LLVM更纯粹的编译器基础结构。MLIR复用了LLVM中的许多想法和接口，其定位在模型表示和代码生成之间。MLIR有灵活的类型系统和多层抽象级别，并引入了dialect来表示这些抽象级别。每个dialect都由一组定义好的不可变操作组成。MLIR的当前dialect包括TensorFlow IR、XLA HLO IR、多面体IR、LLVM IR和TensorFlow Lite，并支持dialect之间的转换。此外，MLIR可以创建新的dialect连接到低阶编译器。</p><p>XLA的HLO IR既可以被视为高阶IR，也可以被视为低阶IR，因为HLO的粒度足以表示硬件信息。此外，HLO支持硬件相关优化，并可用于生成LLVM IR。</p><h4 id="4-2-2-基于低阶IR的代码生成">4.2.2 基于低阶IR的代码生成</h4><p>大多数DL编译器采用的低阶IR最终可以降级到LLVM IR，并利用LLVM成熟的优化器和代码生成器产生机器码。此外，LLVM可以从0开始，为专用加速器设计定制指令集。但是，如果直接将低阶IR转换为LLVM IR，传统的编译器生成的代码质量可能较差。为了避免这种情况，DL编译器采用了两种方法来实现硬件相关优化：</p><ol><li>在LLVM的上层IR（例如，基于Halide的IR和基于多面体的IR）中执行目标相关的loop循环转换</li><li>为优化pass提供更多硬件目标相关信息，帮助改善优化效果。</li></ol><p>大多数DL编译器都应用这两种方法，但是侧重点有所不同。通常，偏向前端用户的DL编译器（例如TC、TVM、XLA和nGraph）可能专注于第一点，而偏向后端开发人员的DL编译器（例如Glow、PlaidML和MLIR）可能专注于第二点。</p><p>DL编译器中的编译方案主要可分为两类：JIT（Just-In-Time）和AOT（Ahead-Of-Time）。JIT编译器可以即时生成可执行代码，并且可以利用运行时信息优化代码。 AOT编译器首先生成所有可执行二进制文件，然后再执行。因此， AOT编译器的静态分析范围比JIT编译器更大。此外，AOT方法可以用于嵌入式平台的交叉编译器，并可以在远程计算机和定制加速器上执行。</p><h4 id="4-2-3-讨论">4.2.3 讨论</h4><p>在DL编译器中，低阶IR是DL模型的细粒度表示，主要描述了DL模型移植到各种不同硬件上所需的详细信息。低阶IR包括基于Halide IR、基于多面体IR和其它IR。尽管这些低阶IR在设计上有所不同，但是都通过已有的成熟编译器工具链和基础结构，提供硬件相关的优化和代码生成接口。低阶IR的设计也会影响到DL加速器的设计（例如TVM Halide IR和Inferentia，以及XLA HLO和TPU）。</p><h3 id="4-3-前端优化">4.3 前端优化</h3><p>构造计算图后，编译器前端就可以开始执行图级优化。由于计算图提供了计算的全局视图，因此很容易在图这个级别识别判断并执行各种优化方法。这些优化方法与硬件无关，仅适用于计算图，不适用于后端实现。</p><p>前端优化通常通过pass来定义，用于遍历计算图的节点并执行图转换。前端功能主要包括两个：第一，从计算图捕获特定特征；第二，重写图以便进行优化。除了预定义pass，开发者也可以在前端定制pass。DL模型导入并转换为计算图后，大多数DL编译器就可以确定操作的输入输出张量的形状</p><p>在本节中，我们将前端优化分为三类：节点级优化、块级（窥孔、局部）优化和数据流级（全局）优化。</p><h4 id="4-3-1-节点级">4.3.1 节点级</h4><p>节点级优化包括消除不必要的节点（即节点消除），以及将节点替换为其它低成本节点（即节点替换），如消除只有一个输入的求和节点，或某个输入为零维张量的求和节点，以及一些零填充宽度的填充节点。</p><h4 id="4-3-2-块级优化">4.3.2 块级优化</h4><p><strong>代数简化</strong></p><p>包括代数识别、强度消减和常量折叠。</p><p>代数识别：利用不同类型节点的交换律、结合律和分配律来简化计算</p><p>强度消减：用便宜的算子代替更昂贵的算子</p><p>常量折叠：使用常量代替常量表达式</p><p><strong>算子融合</strong></p><p>核心思想是将多个算子合并为一个内核，这样无需将中间结果写回内存，减少了中间变量的分配时间。</p><p><strong>算子下沉</strong></p><p>这种优化方法将诸如转置之类的运算下沉到批标准化（batch normalization）、ReLU、Sigmoid和通道混洗（shuffle）之类的运算之下。通过这种优化，许多彼此相似的操作相互靠近，从而为代数简化优化创造更多机会。</p><h4 id="4-3-3-数据流级优化">4.3.3 数据流级优化</h4><p><strong>CSE：公共子表达式消除</strong></p><p>如果某表达式的值已经得到，并且在后续计算中不会再改变，就称为公共子表达式。DL编译器会在整个计算图中搜索该公共子表达式，并使用已经计算好的值替换，避免重复计算。</p><p><strong>DCE：死代码消除</strong></p><p>如果代码的计算结果未被使用，或代码无法执行和访问，则可将这些代码视为死代码。死代码消除是一种常见编译优化技术，目的是移除对程序运行结果没有影响的死代码，减少最终生成代码的大小。</p><p>传统编译器基于活跃变量分析进行死代码消除优化。DL计算图中的死代码通常是由其它图优化造成的。因此，在其它图优化之后还应执行死代码消除和公共子表达式消除。因为不同于传统编译器的低阶IR，DL编译器中的死代码消除的操作对象是高级图IR。因此DL编译器中的死代码消除会有一些传统编译器不具备的操作。例如，如果存储操作的目的张量在后续计算中不再使用，则存储操作可以被删除。这也属于死代码消除。</p><p><strong>静态内存规划</strong></p><p>静态内存规划的目的是尽可能重用内存缓冲区。静态内存规划通常有两种：就地（in-place）内存共享和标准内存共享。对于采用就地内存共享的操作，其输入和输出占用相同的内存，并在计算之前仅分配一个内存副本。标准内存共享可重复使用先前操作的内存，而不会出现重叠。静态内存规划离线执行，这样就可以采用更复杂的规划算法。</p><p><strong>布局转换</strong></p><p>传统编译器中有成熟且使用广泛的结构数据布局优化（Structure Data Layout Optimizations），目的是提高数据局部性，减少缓存未命中率。</p><p>AI编译器的布局转换，其目的是优化计算图中张量的数据布局，并在途中出插入布局转换节点。注意这时尚未执行实际的转换，而是待到编译器后端评估计算图时才执行实际的转换。</p><p>同一种算子在不同的数据布局上性能是不同的，而不同硬件的最佳数据布局也不同.例如，在GPU上，NCHW格式的操作通常运行速度更快，因而其它格式的张量可以先转换为NCHW格式。有些DL编译器依赖于硬件相关的库来实现更高的性能，而这些库可能对布局有要求。此外，边缘设备通常配备异构计算单元，不同类型的单元可能要求不同的数据布局。因此，AI芯片编译器需要提供在各种硬件之间做布局转换的优化方法。</p><p>布局转换本身的开销也很可观。</p><h4 id="4-3-4-讨论">4.3.4 讨论</h4><p>前端是DL编译器中最重要的组件之一，负责从DL模型到计算图高​​级IR的转换，以及基于高阶IR的硬件无关优化。虽然不同AI芯片编译器前端实现在高阶IR的数据表示和算子定义上有所不同，但与硬件无关的优化都可以分为三个层次：节点级、块级和数据流级。每个层次的优化方法都利用了DL特有方法和常规的编译优化技术，从而减少了计算冗余，提高了DL模型在计算图层次的性能。</p><h3 id="4-4-后端优化">4.4 后端优化</h3><p>DL编译器后端通常包括各种硬件相关优化、自动调优技术和优化的内核库。硬件相关优化可以针对不同硬件目标实现高效的代码生成，自动调优技术可以极大减轻推导最佳参数配置的手动工作量，高度优化的内核库广泛用于通用处理器和定制DL加速器。</p><p>在代码生成方面，传统编译器的目标是生成优化的通用代码，<strong>而AI编译器的目标是为特定算子（如卷积，矩阵乘等）生成性能达到或超过手动调优算子的代码实现。作为代价，AI编译器可能要牺牲编译时间去搜索最优配置。</strong></p><h4 id="4-4-1-硬件相关优化">4.4.1 硬件相关优化</h4><p>硬件相关优化（或目标相关优化）用于获得针对特定硬件的高性能代码。某一个途径是将低阶IR转为LLVM IR，以利用LLVM来生成优化的CPU/GPU代码；另一个途径是利用DL知识来设计定制的优化。</p><p>文章介绍了几个比较经典的硬件相关优化方法：</p><p><strong>硬件固有映射</strong></p><p>硬件固有映射可以将某组低级IR指令转换为已经在硬件上高度优化的内核。</p><p>在TVM中，硬件固有映射是用可扩展张量化的方法实现的，它可以声明硬件固有映射的行为和固有映射的下降规则。这种方法使编译器后端能够将硬件实现以及高度优化的手工微内核应用于特定的操作模式，从而显著提高性能。</p><p><strong>内存分配和获取</strong></p><p>对于GPU和定制加速器，内存分配是代码生成中的一个主要难点。例如，GPU主要包含共享内存空间（容量小但访问延迟较低）和本地内存空间（大容量但访问延迟较高）（3global，2shared，1local，0reg）。这样的内存层次结构需要高效的内存分配和获取技术才能改善数据局部性。为了实现内存分配和获取优化，TVM引入了内存作用域调度（TVM小节中应增加这部分内容？）的概念。内存作用域调度原语可以将某个计算阶段标记为共享或线程本地（thread-local）。对于标记为共享的计算阶段，TVM通过共享内存分配和协作数据获取方式生成代码，这种方式会在适当的代码位置插入内存栅栏（barrier）以保证正确性。而TC通过扩展PPCG编译器提供了类似的功能（称为内存提升）。但是，TC仅支持有限的预定义规则。特别是，TVM可以通过内存作用域调度原语在加速器中实现了特殊缓冲功能。</p><p><strong>内存延迟隐藏</strong></p><p>不论是何种处理器，延迟都包括两种：计算延迟和内存访问延迟。与计算速度相比，内存访问速度要慢得多，并且能耗要大得多。为了提高性能，处理器要花费大量资源来隐藏和减少内存延迟，内存延迟隐藏通过重叠内存计算操作，使内存利用率和计算效率最大化，是后端中使用的一项重要技术。</p><p>为了隐藏等待时间，处理器使用乱序执行，将内存访问与其它工作重叠起来，并使用指令投机调度（speculative instruction scheduling）执行相关指令。为了减少延迟，处理器使用了多级缓存，使常用数据更靠近处理器。但是，这些优化措施各有代价。乱序执行需要昂贵的处理器资源，指令投机调度必须在推测错误时重新执行指令，而多级缓存则需要额外的计算和延迟来搜索缓存。</p><p>由于大多数DL编译器都支持CPU和GPU上的并行化执行，因此可以通过芯片硬件实现内存延迟隐藏。但是，针对不同的硬件，需要采用不同的延迟隐藏策略。例如，GPU通过线程束调度器调度线程束参与指令（流水线）的执行，通过快速的切换线程束，最大化利用功能单元。如果有足够的并发活跃线程束，线程束调度器可以让GPU在每个流水线阶段都处于忙碌状态，GPU的内存指令延迟就可以被其它线程束的计算隐藏。但是对于具有解耦访问执行（DAE, Decoupled Access-Execute）架构的类似TPU的加速器，编译器后端需要执行调度和细粒度的同步才能生成正确且高效的代码。为了获得更好的性能，减少编程负担，TVM引入了虚拟线程调度原语，用户可以在虚拟化多线程体系结构上指定数据并行性。然后，TVM通过插入必要的内存栅栏指令来降低虚拟并行线程的数量，并将这些线程的操作交织到一个指令流中，形成更合理的线程执行流水线，这样就可以隐藏内存访问延迟。</p><p><strong>面向循环的优化</strong></p><p><strong>并行化</strong></p><p>并行化是提高深度学习模型中计算密集操作效率的关键。由于现代处理器通常都支持多线程和SIMD并行性，因此，编译器后端可以利用并行性来提高硬件利用率，实现性能最优化。 Halide使用并行调度原语来并行化计算任务，为线程级并行指定循环的并行化维度。其中的每个并行任务可以进一步递归地细分为子任务，以便充分利用目标体系结构上的多级线程层次结构。Halide可以用向量语句替换循环，然后通过硬件intrinsic映射将向量语句映射到硬件相关的SIMD操作码。 Stripe将多面体模型扩展为嵌套多面体模型，引入了并行多面体块这个概念作为迭代的基本执行元素。扩展之后，嵌套多面体模型可以检测平铺和跨度级别之间的层次结构并行化。另外，有些DL编译器依赖于手工库，例如Glow或由硬件厂商提供的优化数学库（在第4.4.3节中讨论）。同时，Glow将向量化处理放到LLVM中完成，因为只要有张量尺寸和循环轮次信息，LLVM自动向量化功能就完全可以正常工作。</p><h4 id="4-4-2-自动调优">4.4.2 自动调优</h4><p>由于在硬件相关优化中用于参数调优的搜索空间巨大，因此有必要利用自动调优来确定最佳参数配置。在常用DL编译器中，TVM、TC和XLA支持自动调优。</p><p>自动调优实现通常包括四个关键部分：参数化、成本模型、搜索技术和加速。</p><p><strong>参数化</strong></p><p>数据和目标：数据参数描述数据的规格，例如输入形状。目标参数描述了在优化调度和代码生成过程中要考虑的硬件相关的特性和约束。对于GPU，需要指定硬件参数，如共享内存和寄存器大小。</p><p>优化选项：优化选项包括优化调度方法和相应的参数，例如面向循环的优化和图块大小。在TVM中，考虑了预定义调度和用户自定义调度以及其参数。</p><p><strong>成本模型</strong></p><ol><li>黑盒模型：TC采用了这种模型。此模型仅考虑最终执行时间，而不考虑编译任务的特征。建立黑匣子模型很容易，但是在没有任务特征指导的情况下，最终得到的可能是次优的解决方案。</li><li>基于机器学习的成本模型：基于机器学习的成本模型是一种使用机器学习方法预测性能的统计方法。这种模型可以在探索新配置时进行更新，从而有助于实现更高的预测精度。TVM和XLA采用了这种模型。</li><li>预定义成本模型：预定义成本模型是基于编译任务特征的模型，可以用于评估编译任务的整体性能。与基于ML的模型相比，预定义的模型在实际应用中产生的计算开销较小，但是需要大量的工作量才能在每个新DL模型和硬件上重建模型。</li></ol><p><strong>搜索技术</strong></p><ol><li>初始化和确定搜索空间：初始选项可以随机设置，或者是根据经验配置。一般情况下，应在自动调优之前就指定搜索空间。TVM中，开发人员使用其特定域知识指定搜索空间，并基于计算描述提取每个硬件目标的自动搜索空间。而TC依赖于编译缓存和预定义规则获得搜索空间。</li><li>遗传算法（Genetic Algorithm）：遗传算法是受自然选择过程启发的一种元启发法，属于自然进化算法大类。遗传算法通过一组候选解（称为个体，生物或表型）的交叉、变异和选择来解决优化问题。在搜索技术中，遗传算法将每个调优参数视为基因，并将每个配置视为候选解。根据适应度值，通过交叉、变异和选择来迭代生成新的候选配置。最后，得出最佳候选解。交叉，变异和选择的速率用于控制探索和开发之间的权衡。TC的自动调整技术采用了遗传算法。</li><li>模拟退火算法（Simulate Anneal，SA）：模拟退火算法是一种通用概率算法，也是受物理退火过程启发的元启发法，用来在一个大的搜寻空间内寻找问题的最优解 ，并能够以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。TVM的自动调整技术采用了模拟退火算法。</li><li>强化学习（Reinforcement Learning, RL）：强化学习算法是机器学习的一个分支，算法的目的是在不断尝试的过程中，学习到在特定的情境下选择哪种行为可以得到最大的回报。强化学习的学习过程是在探索与开发之间的权衡取舍，在给定环境的情况下使回报最大化。基于TVM构建的Chameleon在其自动调整技术中采用了强化学习。</li></ol><p><strong>加速</strong></p><ol><li>并行化：并行化是加速自动调优的方向之一。考虑到遗传算法需要评估每一代中的所有候选配置，TC提出了一种多线程、多GPU的并行化策略。首先，将候选配置入队，并在多CPU线程上对其进行编译。生成的代码在GPU上并行评估，并且每个候选者都有其父选择步骤使用的适应度。完成整个评估后，将生成新的候选配置，并将新的编译任务入队，等待在CPU上进行编译。同样，TVM支持交叉编译和RPC，允许用户在本地计算机上编译，并在多个目标上以不同的自动调优配置运行程序。</li><li>配置重用：加速自动调优的另一个方向是重用以前的自动调优配置。 TC会在编译缓存中存储某个配置的最快生成代码版本。在编译过程中，每次内核优化之前，编译器都会先检索编译缓存，如果编译缓存未命中，才会触发自动调优。与此类似，TVM会生成一个日志文件，其中存储了所有调度算子的最佳配置，并在编译过程中，从日志文件检索最佳配置。TVM在Halide IR中为每个算子做自动调优，并为每个算子确定各自的最佳配置。</li></ol><h4 id="4-4-3-优化的内核库">4.4.3 优化的内核库</h4><h4 id="4-4-4-讨论">4.4.4 讨论</h4><h2 id="5-DL编译器的分类">5. DL编译器的分类</h2><p>后面实验不用看了</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>TVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI编译器相关</title>
    <link href="/2023/10/16/AI%E7%BC%96%E8%AF%91%E5%99%A8%E7%9B%B8%E5%85%B3/"/>
    <url>/2023/10/16/AI%E7%BC%96%E8%AF%91%E5%99%A8%E7%9B%B8%E5%85%B3/</url>
    
    <content type="html"><![CDATA[<p>参考:</p><p><a href="https://github.com/BBuf/tvm_mlir_learn">TVM学习仓库</a></p><p><a href="https://space.bilibili.com/517221395/channel/collectiondetail?sid=857162">B站-zomi酱</a></p><span id="more"></span><h2 id="1-编译器相关">1. 编译器相关</h2><p>编译器（Compiler）和解释器（Interpreter）：</p><ul><li>编译器：将源代码整体编译为可执行文件（机器码），（可能经过预编译、编译、汇编、链接等环节，统一视作编译器的流程）最后由机器执行，会产生可以重复使用的中间文件和可执行文件</li><li>解释器：将源代码逐行解释成字节码并直接交由机器执行，不产生其他文件</li></ul><p>编译器编译方式：JIT 和 AOT</p><ul><li>AOT：AheadOfTime 即静态编译，源代码先统一编译成机器码，再执行</li><li>JIT：JustInTime 即动态编译，相比于传统 AOT，JIT 可以在程序运行过程中变运行边编译，具体流程可以参考 java。注意 JIT 与解释器的区别，解释器的粒度为一行源代码，而 JIT 的粒度为一个函数，JIT 编译的函数可以重复使用，而解释器每次都要重新解释一遍。</li></ul><p>一个 GCC 的标准编译流程：</p><ol><li>预处理：处理宏定义、文件包含、条件编译等信息，生成 .i 文件</li><li>编译：对 .i 文件进行语法分析，优化后生成 .s 汇编文件</li><li>汇编：将 .s 汇编文件汇编为机器码 .o 文件</li><li>链接：将程序运行所需要的目标文件、依赖库文件等统一打包链接成一个可执行文件</li></ol><p>LLVM 在 GCC 的基础上发展而来，早期苹果使用 GCC ，后来由于 GCC 证书以及苹果的商用需要，只能放弃 GCC 而单独发展出 LLVM，LLVM 本质算一个编译器的框架系统，使用模块化的方式，将编译器的前端、优化器、后端等模块分开，可以根据需要进行组合，比如目前主流的 Clang 就是 LLVM 的前端，而 LLVM 的后端可以生成多种平台的机器码，LLVM 的优化器也可以单独使用，这样就可以根据需要进行组合，而不是像 GCC 那样，前端、优化器、后端都是一体的，不可分割。</p><p>PASS：编译器对源代码进行完整的扫描，进行优化和分析的步骤<br>IR：Intermediate Representation 中间表达</p><p>编译器基本结构（主要是 LLVM，GCC 分的没有这么明确）</p><ul><li>Front End：词法分析、语法分析，将源代码转换为抽象语法树（AST），LLVM 使用 Clang 作为前端</li><li>Optimizer：优化，将 IR 进行优化，使代码更高效（PASS 在这个地方）</li><li>Back End：代码生成，将 IR 转换为目标代码（机器码）</li></ul><p><img src="image.png" alt="Alt text"></p><blockquote><p>相关 <strong>Chris Lattner：The Golden Age of Compilers</strong></p></blockquote><p>AI 编译器是介于机器学习框架与硬件中间的一层，用于解决众多框架与多种硬件之间的适配问题，主要架构</p><ul><li>Front-end：计算图转换，将不同框架下的源代码输出为 Graph IR 等高阶 IR（HLIR），重点在于抽象出硬件无关的计算和控制流程，以及数据张量、算子的支持</li><li>Optimizer：对计算图进行一些算子融合、自动微分、并行切分、剪枝量化等优化，IR 间的相互转化，将高阶 IR 转换为低阶 IR（LLIR）</li><li>Back-end：针对特定的机器，将低级 IR 转换为 LLVM IR，再利用 LLVM 基础结构生成优化的机器码</li></ul><h2 id="2-TVM">2. TVM</h2><p>参考：</p><p><a href="https://tvm.hyper.ai/docs/arch/">TVM官方文档</a></p><p><a href="https://zhuanlan.zhihu.com/p/560210215">TVM学习指南</a></p><p>为什么使用 TVM：在模型部署时，众多的机器学习框架（Pytorch、TF、ONNX）与众多的平台（x86、arm、GPU）产生了众多不同的部署场景，而同一个模型在这些不同的场景之间是无法无缝切换的。TVM 的目标就是将这些不同的框架与平台进行统一，使得模型部署更加简单。</p><p>TVM 想要解决的问题：模型部署的可移植性问题、特定平台的硬件优化问题、软件栈的支持问题</p><h3 id="编译流程">编译流程</h3><p><img src="image1.png" alt="Alt text"></p><p>Relay IR：如 relay.Function，TVM 为了兼容上层的机器学习框架而引入的中间表达，一种高阶的图结构，包含了计算图和控制流的信息，这样的设计使得 TVM 可以对模型进行更加全面的优化。Relax 是下一代 Relay（Relay Next）</p><p>Tensor IR：如 tir.PrinFunc，TVM 为了兼容不同的硬件而引入的中间表达，一种低阶的图结构，包含了数据张量和算子的信息，这样的设计使得 TVM 可以对硬件进行更加全面的优化。</p><p>IRModule：是TVM堆栈中的主要数据结构，它是TVM编译的最小完整单元。在 Relay Pass 后包含一组 relay.Function。一个 RelayFunc 通常对应一个端到端的模型（可见MLC）。在经过 TIR Pass 后一个 RelayFunc 可降级为多个 tir.PrinFunc 即元张量函数，这些函数可以被 TVM 优化器进行优化，最后转化为机器码。</p><h4 id="Pass-转换">Pass 转换</h4><p>TVM转换流程的目的：优化（如常量折叠、死码消除，针对特定张量的布局转换、scale因子折叠），以及降级（将代码逐渐转化成更接近硬件的低级表示。</p><p>在 relay/transform 流程的后期，FuseOps 将端到端的函数（即 relay.Function）转化为一个个的算子（即 tir.PrinFunc），这个过程帮助将原始的编译问题分为了两个子问题：</p><ol><li>算子的编译和优化</li><li>整体的执行流程：对生成的算子进行的调用</li></ol><p>tir/transform 流程主要处理 tir.PrimFunc 的降级，例如有些 pass 将多维访问展平为一维指针访问，将内联函数扩展至特定硬件的函数等。也有一些pass的目的仍是优化，如访问索引简化和死码消除。</p><h4 id="AutoTVM：搜索空间和基于学习的转换">AutoTVM：搜索空间和基于学习的转换</h4><p>上述的转换都是确定且基于某一规则的。TVM的目标之一是支持不同硬件平台的高性能代码优化，因此往往要研究尽可能多的优化选择，包括多维张量访问、循环分块策略、特殊加速器内存。</p><p>首先定义一组用来转换程序的操作，包括循环转换、内联、向量化等，称为调度原语，这种原语组成的集合定义了可用于程序优化的搜索空间。接下来，系统搜索不同的可能调度序列，找到最佳（极佳）的调度组合。</p><p>AutoTVM和AutoScheduler是TVM中的两个自动调度器，AutoTVM是基于遗传算法的调度器，AutoScheduler是基于机器学习的调度器。在官方文档中似乎统一为AutoTVM介绍了。</p><blockquote><p><em>使用基于搜索的优化来处理初始 tir 函数生成问题。</em></p></blockquote><p>AutoTVM是在tirPass之前进行的，经过AutoTVM后生成优化的PrinFunc，可以理解成到tirPass之后就不再进行高层优化了，只是针对硬件做一些特殊处理？</p><h4 id="Target-转换">Target 转换</h4><p>这一阶段将 tir 的 IRModule 转换为相应硬件的可执行形式。对于 x86 和 ARM 等后端，使用 LLVM IRBuilder 来构建内存中的 LLVM IR。还可以生成源代码级语言，例如 CUDA C 和 OpenCL。最后，还支持通过外部代码生成器将 Relay 函数（子图）直接转换为特定 target 。</p><p><strong>重要的是，这一阶段的转换要尽可能轻量级，因为绝大多数转换和降级都在之前的阶段完成</strong></p><h4 id="Runtime-执行">Runtime 执行</h4><blockquote><p><em>TVM runtime 的主要目标是提供一个最小的 API，从而能以选择的语言（包括 Python、C++、Rust、Go、Java 和 JavaScript）加载和执行编译好的工件</em></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tvm<br><span class="hljs-comment"># Python 中 runtime 执行程序示例，带有类型注释</span><br>mod: tvm.runtime.Module = tvm.runtime.load_module(<span class="hljs-string">&quot;compiled_artifact.so&quot;</span>)<br>arr: tvm.runtime.NDArray = tvm.nd.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], device=tvm.cuda(<span class="hljs-number">0</span>))<br>fun: tvm.runtime.PackedFunc = mod[<span class="hljs-string">&quot;addone&quot;</span>]<br>fun(a)<br><span class="hljs-built_in">print</span>(a.numpy())<br></code></pre></td></tr></table></figure><p>tvm.runtime.Module 封装了编译的结果。runtime.Module 包含一个 GetFunction 方法，用于按名称获取 PackedFuncs。</p><p>tvm.runtime.PackedFunc 是一种为各种构造函数消解类型的函数接口。runtime.PackedFunc 的参数和返回值的类型如下：POD 类型（int, float）、string、runtime.PackedFunc、runtime.Module、runtime.NDArray 和 runtime.Object 的其他子类。</p><p>tvm.runtime.Module 和 tvm.runtime.PackedFunc 是模块化 runtime 的强大机制。例如，要在 CUDA 上获取上述 addone 函数，可以用 LLVM 生成主机端代码来计算启动参数（例如线程组的大小），然后用 CUDA 驱动程序 API 支持的 CUDAModule 调用另一个 PackedFunc。OpenCL 内核也有相同的机制。</p><p>下面的代码片段给出了用相同接口执行端到端模型的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tvm<br><span class="hljs-comment"># python 中 runtime 执行程序的示例，带有类型注释</span><br>factory: tvm.runtime.Module = tvm.runtime.load_module(<span class="hljs-string">&quot;resnet18.so&quot;</span>)<br><span class="hljs-comment"># 在 cuda(0) 上为 resnet18 创建一个有状态的图执行模块</span><br>gmod: tvm.runtime.Module = factory[<span class="hljs-string">&quot;resnet18&quot;</span>](tvm.cuda(<span class="hljs-number">0</span>))<br>data: tvm.runtime.NDArray = get_input_data()<br><span class="hljs-comment"># 设置输入</span><br>gmod[<span class="hljs-string">&quot;set_input&quot;</span>](<span class="hljs-number">0</span>, data)<br><span class="hljs-comment"># 执行模型</span><br>gmod[<span class="hljs-string">&quot;run&quot;</span>]()<br><span class="hljs-comment"># 得到输出</span><br>result = gmod[<span class="hljs-string">&quot;get_output&quot;</span>](<span class="hljs-number">0</span>).numpy()<br></code></pre></td></tr></table></figure><p>主要的结论是 runtime.Module 和 runtime.PackedFunc 可以封装算子级别的程序（例如 addone），以及端到端模型。</p><h3 id="逻辑架构组件">逻辑架构组件</h3><p><img src="image2.png" alt="Alt text"></p><p>先告一段落，看看自动调优方向</p><h2 id="3-MLIR">3. MLIR</h2><p>参考<br><a href="https://www.lei.chat/zh/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/">某大佬博客</a></p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLVM</tag>
      
      <tag>MLIR</tag>
      
      <tag>TVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MLC学习</title>
    <link href="/2023/09/21/MLC%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/09/21/MLC%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<p>陈天奇的MLC课程，参考</p><p><a href="https://github.com/BBuf/tvm_mlir_learn">TVM学习仓库</a></p><p><a href="https://mlc.ai/zh/index.html">MLC官方课程文档</a></p><p><a href="https://space.bilibili.com/1663273796/channel/collectiondetail?sid=499979">MLC课程视频</a></p><span id="more"></span><h2 id="1-概述">1.概述</h2><p>定义：将机器学习的算法（模型）从开发形式（如pytorch、tf等通用框架编写的模型描述以及相关权重），通过变换和优化，转化为部署形式（如模型支撑代码、内存控制、接口等）<br>即，将神经网络模型转变成在特定硬件上运行的张量函数代码</p><p>机器学习编译目标：</p><ol><li>集成和最小化依赖</li><li>利用硬件加速：利用到每个部署环境的原生加速技术</li><li>通用优化</li></ol><h2 id="2-张量程序抽象">2. 张量程序抽象</h2><p>元张量函数：机器学习模型执行中的每一个步骤（或者说算子？），如linear、relu、softmax</p><p>许多不同的抽象可以表达同一种元张量函数，如torch.add和numpy.add，同时，有些机器学习框架也提供模型的编译过程优化，将元张量函数转变成更专门的、针对性的函数</p><p>张量程序抽象：一个典型的元张量函数实现包括：</p><ol><li>存储数据的多维数组</li><li>驱动张量计算的循环嵌套</li><li>计算语句</li></ol><p>根据抽象出来的共同特征，元张量函数因此可以被一系列有效的程序变换所改变，即优化。<br>一般情况下，我们感兴趣的大部分元张量函数都具有良好的可变换属性。</p><h3 id="TensorIR：TVM使用的张量程序抽象">TensorIR：TVM使用的张量程序抽象</h3><p>前提：大多数的机器学习编译可以视为张量函数之间的变换</p><h4 id="示例：一个经典的点积-relu-网络">示例：一个经典的点积 + relu 网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dtype = <span class="hljs-string">&quot;float32&quot;</span><br>a_np = np.random.rand(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>).astype(dtype)<br>b_np = np.random.rand(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>).astype(dtype)<br>c_mm_relu = np.maximum(a_np @ b_np, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>在底层，numpy可能使用循环和算术运算实现上述操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mm_relu</span>(<span class="hljs-params">A: np.ndarray, B: np.ndarray, C: np.ndarray</span>):<br>    <span class="hljs-comment"># 存储数据的多维数组</span><br>    Y = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                <span class="hljs-comment"># 计算语句</span><br>                Y[i, j] = Y[i, j] + A[i, k] * B[k, j]<br>    <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            C[i, j] = <span class="hljs-built_in">max</span>(Y[i, j], <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="TensorIR实现：">TensorIR实现：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@tvm.script.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">                B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">                C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># 存储数据的多维数组（缓冲区）</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j)<br>                vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j)<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p>块是tensorIR的基本计算单位。定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[block_axis] = T.axis.[axis_type]([axis_range], [mapped_value])<br></code></pre></td></tr></table></figure><p>如<code>vi = T.axis.spatial(128, i)</code> 即表示vi为i的映射，范围为(0,128)，且该块轴属性为spatial（空间轴），而vk的属性则为reduce规约轴。（可以理解为空间轴是原本就在的，规约轴是在上面做滑动的）</p><p>块轴加属性的好处是使得vi，vj，vk独立于外部的循环嵌套i，j，k，同时也对外部循环正确性做了二次验证。同时这些附加信息也有助于机器学习编译分析，比如说，我们总是可以在空间轴上做并行化，但在规约轴上做并行化则需要特定的策略</p><pre><code class="hljs">如果觉得自定义属性比较麻烦也可以一键绑定</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># SSR means the properties of each axes are &quot;spatial&quot;, &quot;spatial&quot;, &quot;reduce&quot;</span><br>vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br></code></pre></td></tr></table></figure><h4 id="tensorIR的元张量函数变换">tensorIR的元张量函数变换</h4><p>tensorIR引入了名为Schedule的辅助结构，允许我们进行方便的元张量函数变换</p><p>这是原来的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> IPython<br>IPython.display.Code(MyModule.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-comment"># from tvm.script import ir as I</span><br><span class="hljs-comment"># from tvm.script import tir as T</span><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br>                T.reads(A[vi, vk], B[vk, vj])<br>                T.writes(Y[vi, vj])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                T.reads(Y[vi, vj])<br>                T.writes(C[vi, vj])<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p>使用Schedule进行变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 以给定的module作为输入的辅助Schedule类</span><br>sch = tvm.tir.Schedule(MyModule)<br><span class="hljs-comment"># 获取对应的块及相应循环的引用</span><br>block_Y = sch.get_block(<span class="hljs-string">&quot;Y&quot;</span>, func_name=<span class="hljs-string">&quot;mm_relu&quot;</span>)<br>i, j, k = sch.get_loops(block_Y)<br><span class="hljs-comment"># 变换：将原有的j循环拆分成两个循环（4表示内部循环长度）</span><br>j0, j1 = sch.split(j, factors=[<span class="hljs-literal">None</span>, <span class="hljs-number">4</span>])<br><span class="hljs-comment"># 再次检查结果</span><br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j_0, j_1, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">32</span>, <span class="hljs-number">4</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + j_1)<br>                vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                T.reads(A[vi, vk], B[vk, vj])<br>                T.writes(Y[vi, vj])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                T.reads(Y[vi, vj])<br>                T.writes(C[vi, vj])<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br><span class="hljs-comment"># 还可以更换循环次序</span><br><span class="hljs-comment"># sch.reorder(j0, k, j1)</span><br></code></pre></td></tr></table></figure><p>此外，块之间也可以通过变换完成组合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将块C放到Y的内循环中</span><br>block_C = sch.get_block(<span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;mm_relu&quot;</span>)<br><span class="hljs-comment"># 感觉意思是将块C与j0循环绑定，及j0这个空间轴变换时，原本只有Y有动作，现在C也有动作</span><br>sch.reverse_compute_at(block_C, j0)<br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j_0 <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">32</span>):<br>            <span class="hljs-keyword">for</span> k, j_1 <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">4</span>):<br>                <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                    vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                    vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + j_1)<br>                    vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                    T.reads(A[vi, vk], B[vk, vj])<br>                    T.writes(Y[vi, vj])<br>                    <span class="hljs-keyword">with</span> T.init():<br>                        Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>            <span class="hljs-keyword">for</span> ax0 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                    vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                    <span class="hljs-comment"># 注意这里vj的变化，原本vj = j = j_0 * 4 + j_1，现在变成了j_0 * 4 + ax0</span><br>                    <span class="hljs-comment"># 感觉是因为上面 reverse_compute_at 只是将C与j0绑定，所以j_1这个循环还是在Y中，C里还需要单独循环ax0</span><br>                    vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + ax0)<br>                    T.reads(Y[vi, vj])<br>                    T.writes(C[vi, vj])<br>                    C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br></code></pre></td></tr></table></figure><p>此外还介绍了另一种原语decompose_reduction，用于将语块中元素的初始化与规约更新分开：<br>这也是 TVM 在以后编译的时候隐式做的，所以这一步的主要目的是让它显式，看看最终效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将块Y中的初始化与循环k无关(k是规约轴)</span><br>sch.decompose_reduction(block_Y, k)<br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mm_relu_v3</span>(<span class="hljs-params">A: np.ndarray, B: np.ndarray, C: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j0 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>):<br>            <span class="hljs-comment"># Y_init</span><br>            <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                j = j0 * <span class="hljs-number">4</span> + j1<br>                <span class="hljs-comment"># 此时初始化在k循环之前就已经做好</span><br>                Y[i, j] = <span class="hljs-number">0</span><br>            <span class="hljs-comment"># Y_update</span><br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                    j = j0 * <span class="hljs-number">4</span> + j1<br>                    Y[i, j] = Y[i, j] + A[i, k] * B[k, j]<br>            <span class="hljs-comment"># C</span><br>            <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                j = j0 * <span class="hljs-number">4</span> + j1<br>                C[i, j] = <span class="hljs-built_in">max</span>(Y[i, j], <span class="hljs-number">0</span>)<br><br>c_np = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=dtype)<br>lnumpy_mm_relu_v3(a_np, b_np, c_np)<br>np.testing.assert_allclose(c_mm_relu, c_np, rtol=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure><h4 id="构建与运行">构建与运行</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用llvm将模型编译到本机平台</span><br>rt_lib = tvm.build(MyModule, target=<span class="hljs-string">&quot;llvm&quot;</span>)<br><br><span class="hljs-comment"># 用于存储输入和输出的TVM NDArray</span><br>a_nd = tvm.nd.array(a_np)<br>b_nd = tvm.nd.array(b_np)<br>c_nd = tvm.nd.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br><br><span class="hljs-comment"># 调用编译好的函数</span><br>func_mm_relu = rt_lib[<span class="hljs-string">&quot;mm_relu&quot;</span>]<br>func_mm_relu(a_nd, b_nd, c_nd)<br><span class="hljs-comment"># 将TVM与numpy的结果进行比较</span><br>np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=<span class="hljs-number">1e-5</span>)<br><br><span class="hljs-comment"># 调用TVM变换后的函数，继续比较</span><br>rt_lib_after = tvm.build(sch.mod, target=<span class="hljs-string">&quot;llvm&quot;</span>)<br>rt_lib_after[<span class="hljs-string">&quot;mm_relu&quot;</span>](a_nd, b_nd, c_nd)<br>np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure><p>在最后的结果中，TVM变换后的函数运行时间相比原先的TVM函数大幅缩短，为什么不同的循环变体会导致不同的时间性能呢？</p><p>关键在于CPU的访存策略，由于局部性原理，CPU在读取内存某元素时会尝试将该元素附近的元素一起获取到缓存中（cache块？特么OS快忘干净了😅）。因此具有连续内存访问的代码通常比随机访问内存不同部分的代码更快。</p><h2 id="3-端到端的模型执行">3. 端到端的模型执行</h2><p>现在考虑一个基础的两层神经网络，由2个MLP和1个relu组成（简化问题，删除最后的softmax）</p><p>numpy实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numpy_mlp</span>(<span class="hljs-params">data, w0, b0, w1, b1</span>):<br>    lv0 = data @ w0.T + b0<br>    lv1 = np.maximum(lv0, <span class="hljs-number">0</span>)<br>    lv2 = lv1 @ w1.T + b1<br>    <span class="hljs-keyword">return</span> lv2<br><br>res = numpy_mlp(img.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">784</span>),<br>                mlp_params[<span class="hljs-string">&quot;w0&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;b0&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;w1&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;b1&quot;</span>])<br></code></pre></td></tr></table></figure><p>底层实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_linear0</span>(<span class="hljs-params">X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">784</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            Z[i, j] = Y[i, j] + B[j]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_relu0</span>(<span class="hljs-params">X: np.ndarray, Y: np.ndarray</span>):<br>     <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            Y[i, j] = np.maximum(X[i, j], <span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_linear1</span>(<span class="hljs-params">X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>            Z[i, j] = Y[i, j] + B[j]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mlp</span>(<span class="hljs-params">data, w0, b0, w1, b1</span>):<br>    lv0 = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_linear0(data, w0, b0, lv0)<br><br>    lv1 = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_relu0(lv0, lv1)<br><br>    out = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_linear1(lv1, w1, b1, out)<br>    <span class="hljs-keyword">return</span> out<br><br>result =lnumpy_mlp(<br>    img.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">784</span>),<br>    mlp_params[<span class="hljs-string">&quot;w0&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;b0&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;w1&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;b1&quot;</span>])<br><br>pred_kind = result.argmax(axis=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Low-level Numpy MLP Prediction:&quot;</span>, class_names[pred_kind[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure><p>该模型的TVMScript实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@tvm.script.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">relu0</span>(<span class="hljs-params">x: T.handle, y: T.handle</span>):<br>        n = T.int64()<br>        X = T.match_buffer(x, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Y = T.match_buffer(y, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                Y[vi, vj] = T.<span class="hljs-built_in">max</span>(X[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">linear0</span>(<span class="hljs-params">x: T.handle,</span><br><span class="hljs-params">                w: T.handle,</span><br><span class="hljs-params">                b: T.handle,</span><br><span class="hljs-params">                z: T.handle</span>):<br>        m, n, k = T.int64(), T.int64(), T.int64()<br>        X = T.match_buffer(x, (<span class="hljs-number">1</span>, m), <span class="hljs-string">&quot;float32&quot;</span>)<br>        W = T.match_buffer(w, (n, m), <span class="hljs-string">&quot;float32&quot;</span>)<br>        B = T.match_buffer(b, (n, ), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Z = T.match_buffer(z, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Y = T.alloc_buffer((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n, m):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Z&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                Z[vi, vj] = Y[vi, vj] + B[vj]<br><br><span class="hljs-meta">    @R.function</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">x: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">1</span>, <span class="hljs-string">&quot;m&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             w0: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;m&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             b0: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;n&quot;</span>, </span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             w1: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;k&quot;</span>, <span class="hljs-string">&quot;n&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             b1: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;k&quot;</span>, </span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        m, n, k = T.int64(), T.int64(), T.int64()<br>        <span class="hljs-keyword">with</span> R.dataflow():<br>            lv0 = R.call_dps_packed(<span class="hljs-string">&quot;linear0&quot;</span>, (x, w0, b0), R.Tensor((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>))<br>            lv1 = R.call_dps_packed(<span class="hljs-string">&quot;relu0&quot;</span>, (lv0, ), R.Tensor((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>))<br>            out = R.call_dps_packed(<span class="hljs-string">&quot;linear0&quot;</span>, (lv1, w1, b1), R.Tensor((<span class="hljs-number">1</span>, k), <span class="hljs-string">&quot;float32&quot;</span>))<br>            R.output(out)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>引入了一个新的 <code>@R.function</code> 即Relex函数，是一种表示上层神经网络执行的全新抽象</p><p><img src="image.png" alt="Alt text"></p><p>注意到，其中<code>call_dps_packed</code>将我们的元函数嵌入到计算图中，其主要作用是满足<strong>目标传递</strong>的调用约定，即 pure 或 side-effect free ，函数只从其输入中读取数据并输出返回结果，而不改变程序的其他部分，这可以方便我们隐藏调用底层元函数的细节</p><p>如果只是像numpy实现中那样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lnumpy_linear0(data, w0, b0, lv0)<br>lnumpy_relu0(lv0, lv1)<br>lnumpy_linear1(lv1, w1, b1, out)<br></code></pre></td></tr></table></figure><p>计算图可能会变成这样：lv0既是<code>lnumpy_linear0</code>的入参，也是<code>lnumpy_relu0</code>的入参，其余同理<br><img src="image-1.png" alt="Alt text"></p><blockquote><p>计算图通常具有以下性质：</p><ul><li>框的每个输入边对应于操作的输入</li><li>每个出边对应于操作的输出</li><li>每个操作可以任意重新排序，直到边缘的拓扑顺序</li></ul></blockquote><p>当然，numpy的底层同样也使用了如<code>lnumpy_call_dps_packed</code>的类似调用</p><p>此外，注意<code>with R.dataflow():</code> 是一个帮助我们标注程序计算图范围的方式，后面的构建运行就不多说了</p><h2 id="4-自动程序优化">4. 自动程序优化</h2><p>这一章主要讲随机调度变换，当我们无法决定原张量函数优化的每一个细节时，可以使用机器的一些<strong>随机变换</strong>做法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">stochastic_schedule_mm</span>(<span class="hljs-params">sch: tvm.tir.Schedule</span>):<br>    block_C = sch.get_block(<span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;main&quot;</span>)<br>    i, j, k = sch.get_loops(block=block_C)<br>    <span class="hljs-comment"># 注意 j_factors 没有使用固定的[none,4]，而是采用随机值</span><br>    j_factors = sch.sample_perfect_tile(loop=j, n=<span class="hljs-number">2</span>)<br>    j_0, j_1 = sch.split(loop=j, factors=j_factors)<br>    sch.reorder(i, j_0, k, j_1)<br>    sch.decompose_reduction(block_C, k)<br>    <span class="hljs-keyword">return</span> sch<br></code></pre></td></tr></table></figure><p>上述代码中，用到了 sch.sample_perfect_tile 来随机拆分循环。它会将输入的循环的长度进行随机分割，例如原始j =128 时，就可以分割为 [8,16]、[32,4]、[2,64] 等等，可以发现，每次运行时该函数的采样都不一样</p><p>此外还讲了一些随机搜索的东西，大概类似超参数的网格搜索之类的，在TVM里叫<code>meta_schedule</code>，主要还做了以下事情：</p><ol><li>跨多个进程的并行基准测试</li><li>使用代价模型<code>cost model</code>进行代价评估，这样可以避免每组都进行基准测试</li><li>根据历史轨迹来进行遗传搜索，而不是每次都随机采样</li></ol><p>关键思想就是使用随机变换来指定好的程序的搜索空间，使用 <code>tune_tir</code> API 帮助在搜索空间内搜索并找到最优的调度变换</p><blockquote><p><strong>前面几章内容总结，就是为什么通过编译可以使模型运行更快（cache空间局部性），以及怎么样编译可以更快（元张量函数变换），同时也介绍了一些随机变换的方法（网格搜索），感觉随机变换的算法才是MLC性能的核心，也就是自动调优，TVM后面似乎用到了一些 autoTVM、autoSchedule 之类的方法进行 auto tune，这也是我需要重点关注的部分</strong></p></blockquote><h2 id="5-与机器学习框架的整合">5. 与机器学习框架的整合</h2><p>如何将机器学习模型从现有框架引入MLC，一些API的基础教程，参考 <a href="https://mlc.ai/zh/chapter_integration/index.html">https://mlc.ai/zh/chapter_integration/index.html</a></p><h2 id="6-GPU硬件加速">6. GPU硬件加速</h2><p>在GPU环境下的MLC流程，第一部分主要讨论CUDA，第二部分讨论专门的GPU环境，后面再看吧</p><h2 id="7-计算图优化">7. 计算图优化</h2><p>提供了一些算子融合的基础代码，也不太想看</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TVM</tag>
      
      <tag>MLC</tag>
      
      <tag>课程笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网络状态检查</title>
    <link href="/2023/09/16/%E7%BD%91%E7%BB%9C%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5/"/>
    <url>/2023/09/16/%E7%BD%91%E7%BB%9C%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5/</url>
    
    <content type="html"><![CDATA[<p>最近fps老是延迟+掉包（把把被一枪头想必不是我的问题），加上网口接触不良一直不好确定源头在哪，在此记录一下问题定位的过程</p><span id="more"></span><h2 id="原因分析：">原因分析：</h2><ul><li>数据传输流程：<br>本地主机 — 本地路由（如果有） — 网关 — 外部网络中转 — 目标主机</li></ul><p>很显然突然的延迟和掉包几乎不可能是外部网络中转和目标主机的问题，加上目前是直连宽带，问题只可能出在主机 &lt;—&gt; 网关上</p><h2 id="问题排查：">问题排查：</h2><h3 id="1-查看自身网络配置：">1. 查看自身网络配置：</h3><p><img src="0.png" alt=""></p><p>本地主机有两条连接，网关分别为192.168.14.1（网口）和172.168.15.1(wifi)</p><h3 id="2-运行任一出现掉包的应用">2. 运行任一出现掉包的应用</h3><h3 id="3-任务管理器中的资源监视器，找到进程PID以及通信的外部IP">3. 任务管理器中的资源监视器，找到进程PID以及通信的外部IP</h3><p><img src="1.png" alt=""></p><p>我也不知道这么多IP哪一个是导致掉包和延迟的。。先找收发高的吧 180.102.211.22 121.229.89.178</p><h3 id="4-netstat查看该进程的网络相关信息">4. netstat查看该进程的网络相关信息</h3><p><img src="2.png" alt=""></p><p>很显然主机是用的192.1168.14.1上的某个端口向外部通信，并且问题大概率出现在与121.229与180.102的通信上</p><h3 id="5-tracert查看路由（pathping也行）">5. tracert查看路由（pathping也行）</h3><table><thead><tr><th style="text-align:center"><img src="4.png" alt=""></th><th style="text-align:center"><img src="5.png" alt=""></th></tr></thead></table><p>两次在走过网口后，都又经过了一个本地IP 100.69.0.1 ，估计是一整栋楼或者本层局域网统一之后的二级网关，经过这个才到外部网络</p><h3 id="6-ping">6. ping</h3><p>分别 ping -t 了一下网关和100.69两个ip，发现到网关基本上没问题，也就是主机到网口的全过程流畅</p><p>每次出现掉包时，到100.69也会出现请求超时，问题就在网口到100.69这段，分析有两种可能：</p><ol><li>网口到100.69线路导致丢包</li><li>100.69负载过高，收到数据后转发丢包</li></ol><h2 id="问题解决">问题解决</h2><p><s>不信邪，直接去楼上的两间空房和本层的另一间空房试了一下，楼上经网关后统一发向100.65.0.1，楼下统一发向100.69.0.1，且本层其他房间ping 100.69也会出现超时或延迟过高问题，服了，确实是层级网关负载过高的原因</s></p><p>并不是，过几天发现有时网络也会走100.65，而且同样会有丢包现象。</p><p>于是偷偷溜进弱点机房观察了一下，一层大概二十个房间，使用两组交换机，每个交换机的24号口连接主路由，主路由的WAN口再连接光猫。主路由还有一个口用来给wifi的路由器。</p><p>这个路由器应该就是100.65.0.1，但是为什么之前测试走的100.69，可能有负载均衡吧。</p><p>实测不管换线还是换接口，延迟和丢包依旧在。看到主路由还有一个闲置LAN口，索性把自己房间直接跳过交换机接到主路由上，发现延迟还是有些不稳定，但丢包彻底解决。公寓该换交换机了。</p><p>本来之后还想着直接接到光猫的闲置口上，但无法识别，可能端口没开启或者是手动分配ip的吧。</p><p>1Gbps速率测速能跑到七八十M/s，应该没啥问题，但wifi延迟和带宽还是很拉跨，应该也是wifi路由器的问题。</p><h2 id="有线和wifi双网叠加">有线和wifi双网叠加</h2><p>参考：<a href="https://www.zhihu.com/question/294289602/answer/2912972037">https://www.zhihu.com/question/294289602/answer/2912972037</a></p><p>系统会优先选择跃点数小的网络进行传输， route print查看连接网络跃点数，之后将两个网络跃点数设置为相同即可</p><p>感觉很扯，实测下载速度不变，上传速度翻倍。。。牛皮</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>记录</tag>
      
      <tag>网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GPGPA阅读</title>
    <link href="/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/"/>
    <url>/2023/09/14/GPGPA%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1>General-Purpose Graphics Processor Architectures</h1><h2 id="Abstract-Preface">Abstract &amp;&amp; Preface</h2><p>与CPU相比，GPU可以更加聚焦与计算，因此性能和效率更高。——通用可编程GPU</p><p>章节介绍：GPU基本结构与历史 —— GPU编程模型 —— GPU计算核心的体系结构 —— 计算核心与内存系统的交叉研究</p><h2 id="Chapter-1-Intro">Chapter 1: Intro</h2><h3 id="1-1-计算加速器的前景">1.1 计算加速器的前景</h3><p>过去，计算系统的性能提升大部分依赖于工艺的进步，使得晶体管尺寸缩小，从而提升集成度，使得运行速度更快。</p><p>Dennard Scaling：从05年开始晶体管缩放规则失效。因此为了提高性能，需要找到更高效的硬件架构。</p><p>hardware specialization：定制化硬件，可以使能效比大幅提高，两大方向：</p><ul><li>硬件向量化，消除指令处理的开销</li><li>优化计算过程，减少数据运输开销</li></ul><p>计算架构的关键：专业化硬件带来的收益与支持广泛程序所需的灵活性之间的平衡。相比于专用加速器（例如google的TPU），仍然需要GPU这种较为通用的计算硬件。</p><p>Turing-complete：图灵完备，只要给足够的时间与内存，GPU可以完成一切运算。</p><h3 id="1-2-GPU硬件基础">1.2 GPU硬件基础</h3><p>GPU不会完全取代CPU：GPU不是独立的计算设备，通常来说，CPU负责在GPU上启动计算并负责GPU上的数据传输。当前访问I/O设备或提供OS服务的软件主要还是运行在CPU上（这些软件缺乏大规模并行性），因此，需要首先考虑GPU与CPU的交互。</p><ul><li>独立GPU：两个U各有各的mem，同时核心通过PCIE总线进行数据传输。注意对于独显来说，两个U的mem的DRAM技术通常是不一样的，CPU的DRAM通常针对低延迟访问进行优化（DDR），而GPU的DRAM通常针对高吞吐进行优化（GDDR）。</li><li>集成GPU：两个U共享一个cache，cache与一个内存进行数据交互。由于共享内存所以只能采取单一技术，集成式GPU通常搭载在低功耗设备上，因此DRAM通常针对低功耗进行优化（LPDDR）。</li></ul><p>一个GPU计算应用会从CPU上开始，通常，该应用程序的CPU部分负责分配和初始化一些数据结构。在旧的N卡和A卡上，CPU需要为CPU和GPU内存中的数据结构分配空间，并协调数据从CPUmem到GPUmem的移动。在新的N卡（Pascal，10系）上的软硬件支持数据从Cmem到Gmem的自动传输，这项技术通过利用虚拟内存支持来实现，NV称之为unified memory。对于集显来说不存在数据mem传输的问题，但是由于两个U共享cache并且有些cache可能是私有的，因此也需要关注缓存一致性问题 (cache-coherence) 。</p><p>启动GPU运算一般需要驱动程序完成，在GPU启动运算前，CPU通过驱动程序指定GPU运行哪些代码，这些代码称为内核（kernel），同时，CPU还需要指定线程数、每个线程的数据位置等等。配置完毕后，CPU向GPU发出信号，GPU开始运算。</p><p>现代GPU由许多核心（SIMT Core）组成，NV称之为流式多处理器(Streaming Multiprocessor, SM)，AMD称之为计算单元(compute unit)，每个核心都执行一个与此时运行的内核相关的单指令多线程程序，一个核心可以运行上千个线程，这些线程通过暂存区mem进行通信，并使用快速屏障技术（fast barrier operations）进行同步。每个核心同时还有一级指令和一级缓存，这些缓存可以充当带宽过滤器，减少向低级别内存的流量，当拥有大量线程时，可以隐藏由于有时某线程的缓存未命中而访问内存带来的的性能下滑。</p><p>高计算吞吐需要高内存带宽的支持，这又对内存系统的并行性提出要求。这种并行性又多通道内存实现，每个通道与内存分区中的最后一级缓存（LLC）相连，GPU核心通过片上互连网络与内存分区相连。也有一些替代的方案，例如Intel的Xeon Phi，就是将LLC直接交由GPU核心分配</p><p>对于高并发任务来说，GPU相对超标量无序CPU拥有更高的单位面积性能，因为GPU可以将其芯片面积的大部分专用于算术逻辑单元，并相应的减小控制逻辑的面积。</p><p>09年出来一个性能随线程变化的分析模型。模型显示：</p><ol><li>当少量线程共享大缓存时（如多核CPU），性能会随着线程数量的增加而提高。</li><li>当线程数增加到缓存无法容纳整个工作集时，性能反而会随着线程数量增加而下降。</li><li>但是随着线程数量的进一步增加，性能会随着多线程隐藏片外延迟的能力而提高。<br>GPU就是通过采用多线程来容忍频繁的缓存未命中，提高运算性能。</li></ol><p>内存访问不仅降低性能，同时也会提高能耗。新的GPGPU架构的重点是改善内存访问。</p><h3 id="1-3-GPU简史">1.3 GPU简史</h3><h3 id="1-4-书籍大纲">1.4 书籍大纲</h3><ul><li>第二章：编程模型、代码开发过程、编译流程</li><li>第三章：单个GPU核心（SM）的体系结构</li><li>第四章：内存系统</li><li>第五章：其他研究</li></ul><h2 id="Chapter-2：编程模型">Chapter 2：编程模型</h2><p>现代GPU广泛采用SIMD硬件来利用数据级并行，但GPU的计算API（如NV的cuda和AMD的opencl）并不向程序员暴露SIMD的硬件，而是采取类似MIMD的编程模型，允许程序员在GPU上启动大量标量线程。其中的每一个标量线程都有自己独特的执行路径，并都可以访问内存。运行时，GPU上的的SIMD硬件利用线程的规律性和空间局部性，同步启动这些标量线程组，称为SIMT（单指令多线程）</p><ul><li>SIMD：例如两个向量，对两个向量的每一个分量进行相同的op操作，输出为一个向量（在一个线程里，受ALU宽度限制）</li><li>SIMT：与SIMD在一个线程公用一个ALU不同，SIMT有多个线程，每个线程各有各的ALU和自己的数据，但执行的指令相同（但是由于数据不同，执行指令时的控制分支可能会不一样）</li></ul><h3 id="2-1-运行模型">2.1 运行模型</h3><p>为GPU优化的代码很可能在CPU架构上表现不佳。假定一个 单精度标量A * 向量X + 向量Y 的函数实现：</p><p>CPU实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-type">void</span> <span class="hljs-title function_">saxpy_serial</span><span class="hljs-params">(<span class="hljs-type">int</span> n, <span class="hljs-type">float</span> a, <span class="hljs-type">float</span> *x, <span class="hljs-type">float</span> *y)</span><br>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; ++i)<br>        y[i] = a * x[i] + y[i];<br>&#125;<br>main()<br>&#123;<br>    <span class="hljs-type">float</span> *x, *y;<br>    <span class="hljs-type">int</span> n;<br>    <span class="hljs-comment">// 省略*x、*y的赋值操作</span><br>    saxpy_serial(n, <span class="hljs-number">2.0</span>, x, y);<br>    <span class="hljs-comment">// 省略内存释放操作</span><br>&#125;<br></code></pre></td></tr></table></figure><p>GPU-CUDA实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs C">__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">saxpy</span><span class="hljs-params">(<span class="hljs-type">int</span> n, <span class="hljs-type">float</span> a, <span class="hljs-type">float</span> *x, <span class="hljs-type">float</span> *y)</span> <span class="hljs-comment">// __global__代表函数在GPU上运行</span><br>&#123;<br>   <span class="hljs-comment">// 每个线程都有各自的blockIdx.x、blockDim.x、threadIdx.x</span><br>   <span class="hljs-type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;<br>   <span class="hljs-comment">// threadIdx.x代表线程在线程块中的x坐标</span><br>   <span class="hljs-comment">// blockIdx.x代表线程所属的线程块在grid中的x坐标</span><br>   <span class="hljs-comment">// blockDim.x一个线程块在x维度的最大线程数</span><br>   <span class="hljs-comment">// 一般来说线程在线程块中有xyz三个坐标，线程块在网格中也有xyz三个坐标，这里省略y，z</span><br>   <span class="hljs-keyword">if</span> (i &lt; n)<br>      y[i] = a * x[i] + y[i];<br>&#125;<br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>   <span class="hljs-comment">// 一般h_表示cpu的内存指针，d_表示gpu的内存指针</span><br>   <span class="hljs-type">float</span> *h_x, *h_y;<br>   <span class="hljs-type">int</span> n;<br>   <span class="hljs-comment">// 省略*h_x、*h_y的赋值操作</span><br>   <span class="hljs-type">float</span> *d_x, *d_y;<br>   <span class="hljs-type">int</span> nblocks = (n + <span class="hljs-number">255</span>) / <span class="hljs-number">256</span>;<br>   <span class="hljs-comment">// 调用GPU驱动程序并要求分配gpu内存，并将这一片内存的地址赋给&amp;d_x</span><br>   cudaMalloc(&amp;d_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));<br>   cudaMalloc(&amp;d_y, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>));<br>   <span class="hljs-comment">// 将h_x指向的内容赋值给d_x指向的区域</span><br>   cudaMemcpy(d_x, h_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>   cudaMemcpy(d_y, h_y, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice);<br>   <span class="hljs-comment">// 交由GPU，并启动nblocks个线程块（Thread Block，或CTA），每个线程块256个线程，所有的线程块组成一个grid，即本次内核的计算单元</span><br>   saxpy&lt;&lt;&lt;nblocks, <span class="hljs-number">256</span>&gt;&gt;&gt;(n, <span class="hljs-number">2.0</span>, d_x, d_y);<br>   <span class="hljs-comment">// 为了提高效率，每个线程块中，每32个线程以锁步形式组成一组warp，warp往上再组成线程块</span><br>   <span class="hljs-comment">// 一个warp包含多少线程是硬件概念，而一个线程块可以有多少线程则是软件概念（当然得是warp的整数倍）</span><br>   <span class="hljs-comment">// 将计算结果返回给CPU内存</span><br>   cudaMemcpy(h_x, d_x, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyDeviceToHost);<br>   <span class="hljs-comment">// 省略内存释放操作</span><br>&#125;<br></code></pre></td></tr></table></figure><p>硬件：一个GPU有多个SM，每个SM包含多个SP（Stream Processor）</p><p>软件：一个GPU内核对应一个grid，一个grid包含多个CTA。CTA中有多个warp，每个warp包含固定数量的线程数（与SM中SP数量相同）。</p><p>GPU在运算时，可能是一个grid独占GPU，也可以多个grid并行跑GPU；SM对应的工作单元是CTA，其中的基本执行单元是warp（即每个SP对应一个warp中的线程），在某个warp受阻时SM可以切换同一个CTA中的其他warp，但只有该CTA执行完，才切换其他CTA</p><p>CTA中的线程之间可以通过暂存器内存互相通信（NV称之为共享内存），同步也轻松，同时每个SM中也有一个共享内存，可以分配给在该SM上运行的所有CTA</p><p>不同CTA中的线程也可以通过所有线程都能访问的全局地址空间通信，但代价较高</p><h3 id="2-2-指令模型">2.2 指令模型</h3><p>NV的并行线程执行ISA：Parallel Thread eXecution，简称PTX （虚拟指令，类似汇编指令）</p><p>GPU运行PTX代码前，需要编译（汇编）成实际的机器指令，NV称此为SASS（Streaming ASSembler），该过程有NV的工具包完成，并没有开放，这使得NV可以在硬件级别提供向后兼容性，每一代都可以重新设计ISA架构</p><h2 id="Chapter-3：SIMT核心：指令和寄存器数据流">Chapter 3：SIMT核心：指令和寄存器数据流</h2><p>对传统图形渲染来说，GPU通常需要访问详细的纹理图，这样的数据集因为太大不可能完全缓存在芯片上，因此有必要采用能够维持大片外带宽的GPU架构。所以如今的GPU都往高并发线程发展（大概意思是线程越多越能够隐藏访存损失）。并且，尽管每个线程的片上缓存很小，但因为局部性原理，仍然可以有效减少大量的片外存储访问。</p><p>SM的微体系结构，流水线分为SIMT前端和SIMD后端，共3个循环：</p><ol><li>取值（fetch）循环：fetch、I-Cache、Decode和I-Buffer模块</li><li>发指（issue）循环：I-Buffer、Scoreboard、issue和SIMT stack模块</li><li>寄存器访问调度循环：Operand Collector、ALU和Memory模块</li></ol><p><img src="image.png" alt=" "></p><h3 id="3-1-单循环近似">3.1 单循环近似</h3><p>线程的调度单位是warp（AMD称之为wavefronts）。每一个周期，SM选择一个warp进行调度。</p><p>单循环中，warp的程序计数器（PC）用于访问指令存储器时查找为warp执行的下一条指令。获得指令后，对指令解码，并找到源操作数寄存器。与此同时，SIMT的执行掩码值也被确定。</p><p>在执行掩码与源寄存器可用后，执行以SIMD的方式进行。如果设置了SIMT执行掩码，则每个线程都在与通路关联的功能单元上执行。与现代CPU一样，功能单元通常异构，即不同的单元支持不同的指令运行。</p><p>每个功能单元在名义上包含的通路数与warp的线程数相同，但也有一些GPU使用不同的实现，使其中的warp在多个时钟周期内执行。</p><h4 id="3-1-1-SIMT执行掩码">3.1.1 SIMT执行掩码</h4><p>现代GPU的关键特性是SIMT执行模型，为程序员提供了单个线程完全独立执行的抽象，这是通过传统谓词（prediction）与SIMT谓词掩码堆栈结合实现的。</p><p>SIMT堆栈有助于处理线程可以独立执行时出现的两个关键问题：</p><ol><li>嵌套控制流</li><li>完全跳过计算</li></ol><table><thead><tr><th style="text-align:center"><img src="image-1.png" alt=" "></th><th style="text-align:center"><img src="image-2.png" alt=" "></th><th style="text-align:center"><img src="image-3.png" alt=" "></th></tr></thead></table><p>假设每个warp有四个线程，所有线程都执行了A基本块，之后遵循不同的控制流，有3个线程进入B，1个线程进入F。如此流动，最后所有线程统一到达G。</p><table><thead><tr><th style="text-align:center"><img src="image-4.png" alt=" "></th><th style="text-align:center"><img src="image-5.png" alt=" "></th></tr></thead></table><p>堆栈包括三项：重新收敛程序计数器(Reconvergence program counter, RPC)、要执行的下一条指令的地址(Next PC)和活跃掩码(active mask)。warp每次都执行栈顶指针指向的条目的nextPC指向的代码块</p><ol><li>开始时堆栈中只有一个条目“-，A，1111”。代表所有线程都将进入A。</li><li>所有4个线程在走完A之后进行分支，此时需要有3处修改：<ul><li>将原先条目的nextPC值修改成分支后的重新汇聚点，对于这次分支B和F，将在G重新汇聚，因此将第一条的nextPC由A修改为G</li><li>这次分支有3个进入B，1个进入F，因此在堆栈中压入关于B和F的两个条目</li></ul></li><li>线程执行栈顶条目“G，B，1110”，掩码是1110代表这行条目对前三个线程active，走完B后进行分支，同理修改原来条目的nextPC，改成最近的重新收敛点E，同时添加两个分支条目。<ul><li>一般改成最近的重新收敛点，是为了从该位置将之前发散的线程以锁步的方式继续执行，便于同步</li><li>通常来说，在分支过后，最好是先将最多活跃线程的条目先入栈，少活跃线程的条目后入栈，例如d部分，而c部分的例子相反</li></ul></li></ol><h4 id="3-1-2-SIMT死锁与无堆栈SIMT结构">3.1.2 SIMT死锁与无堆栈SIMT结构</h4><p>SIMT基于堆栈的实现可能导致死锁：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-comment">// 将mutex置0代表资源空闲</span><br>*mutex = <span class="hljs-number">0</span>;<br><span class="hljs-comment">// atomicCAS读取mutex，若为0，则置1（即若空闲，则访问），返回mutex原始值</span><br><span class="hljs-comment">// 一个warp中的所有线程都执行，因此只有一个线程看到mutex=0，其他都看到=1</span><br><span class="hljs-keyword">while</span>(!atomicCAS(mutex,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>));<br><span class="hljs-comment">// 释放mutex</span><br>atomicExch(mutex,<span class="hljs-number">0</span>);<br></code></pre></td></tr></table></figure><p>简而言之，对于一个互斥资源，当一个warp的所有线程同时执行互斥锁式访问时，只有一个线程拿到资源，其他线程陷入原地等待。但是，拿到资源的线程在执行完毕后，达到了上文中的重新收敛点，会等待其他所有线程一起到这个点，才能继续执行第三句释放锁。</p><p>无堆栈分支的重新收敛机制：warp收敛屏障</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文笔记</tag>
      
      <tag>TBC</tag>
      
      <tag>GPGPA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DianNao系列论文阅读</title>
    <link href="/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2023/09/14/DianNao%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning">DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</h2><h3 id="概述">概述</h3><p>DianNao：一个用于普遍机器学习的小规模高通量加速器。寒武纪开山之作</p><p>现阶段（2014）机器学习工作中（CNN、神经网络）CPU性能不够，GPU、FPGA等功耗过高。</p><p>当前的绝大多数acc的关注点都在于算法计算部分（efficiently implementing the computational part of the algorithms），然而CNN和DNN的特点是大尺寸、大计算量。在这种情况下，DMA的效果不太好。因此我们针对这一特性，设计了一个专门对访存做特别优化的加速器</p><p>对比常规SIMD处理器速度快117倍，能耗比提高21倍</p><h3 id="Intro">Intro</h3><p>加速器设计的权衡：灵活性和高性能。而由于当时机器学习的SOTA就是CNN和DNN，种类有限，所以可以这类计算设计出有针对性的加速器</p><p>当前针对CNN或MLP的加速器都专注于神经网络中计算原语如卷积的有效实现，如矩阵乘法、向量计算方面的优化。但是忽略了对性能影响同样巨大的访存部分</p><p>由于阿姆达尔定律，即使计算原件做大量优化，整体性能依旧会受制于内存传输部分，并且在机器学习领域，为了实现更高的精度和功能，有一个必然的趋势就是提高神经网络的规模，而这也证明了访存优化对于设计加速卡的重要性</p><p>主要贡献：高吞吐、高能耗比、侧重内存性能的加速卡设计</p><h3 id="最新机器学习技术入门">最新机器学习技术入门</h3><p>当前的市场来说，我们的加速器应该聚焦于网络的前馈而非反馈。这是由于在许多业务场景下离线学习都是主流，网路可以周期性的进行离线学习，而拿到客户手中只需要高效的前向推理。并且由于反向传播和正向路径原理类似，我们在之后也会针对反向传播做出优化工作</p><h3 id="基于处理器的（大型）神经网络实现">基于处理器的（大型）神经网络实现</h3><table><thead><tr><th style="text-align:center"><img src="image.png" alt=" "></th><th style="text-align:center"><img src="image-1.png" alt=" "></th><th style="text-align:center"><img src="image-2.png" alt=" "></th></tr></thead></table><ol><li><p>classifier层</p><ul><li>就是全连接层，在一般的理解中，每一个output相当于所有input的加权求和在Sigmoid出的概率值，如图所示，synapses矩阵对于每个output来说该input的权重</li><li>首先展示的是针对全连接层的优化，一般比较符合直观思维的模式是逐行运算，每次输出一个output。这种方式在遇到大规模神经网络（i/o神经元数量大）时有一个问题：对带宽要求高，内存运输总数= inputs loaded + synapses loaded + outputs written = Ni x Nn + Ni x Nn + Nn</li><li>改进方式：将输入神经元进行tile loop，在L1缓存不够大的情况下，平铺再分块，类似卷积层，这样做的好处是对input神经元的数据做了复用，大大降低了输入神经元的内存带宽需求，略微增加了输出带宽需求（因为不再是一次出结果，需要written多次）</li><li>同时如果将synapses矩阵存入L2缓存（对于当时的神经网络来说，权重总数在百万数量级，仍在L2的范围内），可以进一步减少所需带宽</li></ul></li><li><p>卷积层</p><ul><li>有两种数据复用可能：滑动窗口（就是卷积核权重，滑动过程中卷积核不变）以及跨通道的输出复用</li><li>简单来说就是每次不再都整个input feature map，而是在map上截取一片Tx*Ty，每次不同的卷积核在上面卷积运算之后送给output，再最后加权求和</li><li>无非是由于缓存空间限制而对feture maps、Input channel、output channel做出截取，让运算时数据都在缓存中，降低内存带宽</li><li>针对卷积核权值共享做了优化，文章倾向于共享卷积核</li></ul></li><li><p>池化层:</p><ul><li>重用机会少，对于增加Tx，Ty效果不显著</li></ul></li></ol><h3 id="小型神经网络加速器">小型神经网络加速器</h3><ul><li>对于小型网络，可假设所有神经元和突触都由硬件实现，内存仅用于I/O</li><li>对于小型神经网络可以大幅提高能耗，但随神经元数量增多面积、能量和延迟呈二次方增长</li></ul><h3 id="大型神经网络加速器">大型神经网络加速器</h3><p><img src="image-3.png" alt=" "></p><p>总共三大部件：</p><ul><li>运算部分：NFU</li><li>存储部分：输入缓存NBin、输出缓存NBout、突触权重SB</li><li>控制逻辑：CP</li></ul><p>NFU：</p><ul><li>按照第三节，将每一层分解为Ti和Tn的计算块</li><li>流水线：layer都可以分为若干个规范计算单元的组合，将整个计算规范化，流程化<ol><li>NFU-1：乘法单元</li><li>NFU-2：加法树</li><li>NFU-3：激活单元</li></ol></li></ul><p>全连接层：突出*输入；乘积求和；激活函数sigmoid；<br>卷积层：计算阶段相同，只是激活函数可能不同；<br>池化层：没有乘积的操作，可以是求最大池化和平均池化</p><h2 id="DaDianNao-A-Machine-Learning-Supercomputer">DaDianNao: A Machine-Learning Supercomputer</h2><h3 id="概述-2">概述</h3><p>在多核芯片中，由于CNNs和DNNs所需内存并未超过其片上存储空间，结合CNN/DNN算法自身特点，会导致高内部带宽和低外部通信这一情况，从而能在合理的区域成本下实现高并发。</p><h3 id="Intro-2">Intro</h3><p>前人工作缺陷：加速芯片要么有神经网络大小限制，要么对于大型网络，神经元和突触必须存储在内存中</p><p>神经网络性能瓶颈：内存访问</p><h3 id="The-GPU-Option-The-Accelerator-Option">The GPU Option/ The Accelerator Option</h3><p>CPU与GPU与DianNao的一些比较；DianNao的介绍，现有DianNao的不足</p><p>主要的限制来源于两种重要层的内存带宽需求：私有内核的卷积层(用于dnn)和全连接层</p><h3 id="A-Machine-Learning-Supercomputer">A Machine-Learning Supercomputer</h3><p>权重存储在将使用它们的神经元附近，最大限度地降低数据移动，节省时间和能量;架构是完全分布式的，没有主存</p><p>非对称的体系结构，其中每个节点占用的空间大量偏向于存储而不是计算</p><p>传递神经元值而不是权重，因为在两个典型层中，神经元值比权重数量级小，需要相对较少的外部(跨芯片)带宽</p><p>通过将本地存储分解成许多块来实现高的内部带宽</p><p>将SRAM更换为eDRAM，缩小面积，但需要周期性刷新、延迟高</p><p>NFU无法简单扩大规模：数据布线面积占用过多，可扩展性差</p><p>对NFU进行分片</p><h2 id="Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks">Cambricon: An Instruction Set Architecture for Neural Networks</h2><h3 id="概述-3">概述</h3><p>传统神经网络往往在CPU、GPGPU这样的的通用平台执行，通常来说不够节能，因为这种平台主要是为了灵活支持各类型工作</p><p>最近的一些硬件加速器，这类加速器通常采用高级指令直接控制高级功能块。但是，当需要灵活支持各种不同的NN时，这种直接控制块的方式就不太行</p><p>思路 a. 分解大块的操作成一个个小的指令，获得更大的灵活性。用户可以用低层次的操作组合成高层次的功能。b. 简单的短指令可以大幅降低设计验证的复查度和解码器的功耗和面积。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DianNao</tag>
      
      <tag>论文笔记</tag>
      
      <tag>TBC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>00：个人网站部署</title>
    <link href="/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/"/>
    <url>/2023/09/13/00%EF%BC%9A%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>花了两三天时间搭了一个用于笔记的个人博客，部署在了 github 上，这里记录一下部署的过程。</p><span id="more"></span><h2 id="选型">选型</h2><p>网站的建立主要是为了搭一个在公司和家里都能访问的博客环境，对工作和学习做一些记录，所以直接放弃传统的带前端后端的动态页面，时间成本太高，整一个可以一键上传 markdown 的静态页面就挺ok。前端框架采用 hexo，UI选择 fluid，代码放在 github 上，并使用 github action 进行持续集成，部署到 github pages 后，后续写作只需要一次 git push 就可以自动将文章更新到目标网站上。</p><h2 id="框架搭建">框架搭建</h2><p><a href="https://hexo.io/zh-cn/docs/">https://hexo.io/zh-cn/docs/</a></p><h2 id="UI设置">UI设置</h2><p><a href="https://hexo.fluid-dev.com/docs/guide/">https://hexo.fluid-dev.com/docs/guide/</a></p><h2 id="部署">部署</h2><h3 id="快速部署">快速部署</h3><p><a href="https://hexo.io/zh-cn/docs/one-command-deployment">https://hexo.io/zh-cn/docs/one-command-deployment</a></p><p>适用于希望源代码保存在本地而不用上传的情况，相当于本地构建完再将构建好的网页直接推给gh page</p><p>需要在 _config.yml 中配置 gh page 的仓库地址和分支，推送后，hexo 会将 public 目录中的文件推送至_config.yml 中指定的分支中，并且完全覆盖该分支下的已有内容。</p><p>这就导致了一个问题，由于是只传 public 目录，域名映射需要的CNAME文件只能放到 public 下，这样每次 hexo clean 后会清空 public，还得再编辑一次CNAME，但是好处在于刨除了云端构建的不稳定性，每次可以本地看看网站效果，再直接放到 gh page 中</p><h3 id="gh-actions-持续集成">gh actions 持续集成</h3><p><a href="https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html">https://easyhexo.com/1-Hexo-install-and-config/1-5-continuous-integration.html</a></p><p>源代码放到 <a href="http://user.github.io">user.github.io</a> 仓库中后（仓库名只能设为这个，否则生成网页会变成 <a href="http://user.github.io">user.github.io</a> 的子页），CNAME 放在 source 中，然后在 .github/workflws 中定义 gh actions 的详细配置</p><p>采用的hexo官方文档中的配置，最后一步使用 peaceiris/actions-gh-pages@v3 咱也不太懂，参考知乎上的其他配置，大概相当于安装 hexo 完了在将 main 分支的源码 deploy 到 gh-pages 分支上，之后在设置时选择这个分支即可</p><p>主要问题在于每次 push 都要重新 build，推测后期内容增多后网站更新会十分不及时，可能需要看看别人的追加更新是咋弄的</p><h3 id="CDN加速、">CDN加速、</h3><p>更换 Cloudflare 的 DNS，注意 SSL/TLS 加密模式设为严格</p><h2 id="Summary">Summary</h2><p>属于我的第0篇博客，大概，能在网站上正常显示，证明基本功能已经ok</p><h2 id="TODO">TODO</h2>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
    </categories>
    
    
    <tags>
      
      <tag>前端</tag>
      
      <tag>记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
