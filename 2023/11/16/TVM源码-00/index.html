

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.png">
  <link rel="icon" href="/img/logo.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhang Yix">
  <meta name="keywords" content="">
  
    <meta name="description" content="TVM v0.8 源码学习">
<meta property="og:type" content="article">
<meta property="og:title" content="TVM源码-00">
<meta property="og:url" content="http://example.com/2023/11/16/TVM%E6%BA%90%E7%A0%81-00/index.html">
<meta property="og:site_name" content="Zhyx&#39;s Blog">
<meta property="og:description" content="TVM v0.8 源码学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/11/16/TVM%E6%BA%90%E7%A0%81-00/image.png">
<meta property="article:published_time" content="2023-11-16T13:49:21.000Z">
<meta property="article:modified_time" content="2024-12-22T05:23:55.966Z">
<meta property="article:author" content="Zhang Yix">
<meta property="article:tag" content="TVM">
<meta property="article:tag" content="编译">
<meta property="article:tag" content="cmake">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2023/11/16/TVM%E6%BA%90%E7%A0%81-00/image.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>TVM源码-00 - Zhyx&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 40vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Zhyx&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/index.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.2)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="TVM源码-00"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-16 13:49" pubdate>
          2023年11月16日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          79 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">TVM源码-00</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2024年12月22日 凌晨
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p>TVM v0.8 源码学习</p>
<span id="more"></span>
<h2 id="代码拉取">代码拉取</h2>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --recursive https://github.com/apache/tvm.git<br>git checkout v0.8<br></code></pre></td></tr></table></figure>
<h2 id="编译安装">编译安装</h2>
<h3 id="编译常识">编译常识</h3>
<h4 id="GCC">GCC</h4>
<p>对于小项目来说，文件数量较少，使用GCC直接进行编译即可</p>
<ol>
<li>g++:将源文件编译成.out可执行文件</li>
<li>g++ -c:将源文件编译成中间文件 而不进行链接，即编译成.o文件</li>
<li>g++ -o:将源文件或者.o文件进行编译+链接或链接，生成.out文件</li>
<li>对于包含多个源文件的项目，可以将源文件分别gcc -c，再将产生的文件链接到一起，如<code>g++ -o myprogram file1.o file2.o</code></li>
</ol>
<h4 id="make-makefile">make &amp; makefile</h4>
<p>然而随着计算机的发展，一个软件工程包含的源文件越来越多，手动逐个编译完全不可行，于是有个make和makefile。</p>
<p>Make 是一个批处理工具，它根据 Makefile 文件中的规则来构建项目。Make 可以确定哪些文件需要重新编译，哪些文件已经是最新的，从而只编译需要编译的文件。</p>
<p>Makefile：Makefile 是 Make 的配置文件，它包含了一系列的规则，用于指定如何构建项目。Make 通过读取 Makefile 文件来构建项目。</p>
<p>在这一阶段，工程师可以手写项目的makefile文件，再使用make指令统一构建整个项目</p>
<h4 id="Cmake-CMakeLists">Cmake &amp; CMakeLists</h4>
<p>makefile在一些简单的工程下，完全可以人工手写，但是问题又来了，工程非常大的时候，连 makefile 的手写也非常麻烦，这时就需要一个工具可以自动生成 makefile ，这个工具就是cmake。</p>
<p>CMakeLists 是 Cmake 的配置文件。还是需要手写。</p>
<p>Cmake 会根据 CMakeLists 自动生成项目的 makefile 文件，然后再使用 make 构建项目。</p>
<p>Cmake 有不同的生成器，可以生成不同平台下的 makefile 文件，比如 Unix Makefile、Visual Studio、Ninja、Nmake等等，可以生成不同平台下的makefile，生成后再进行make就可以将项目构建在不同的平台下。</p>
<p>ninja是一种注重速度的生成器，使用ninja生成会产生一个build.ninja文件，然后使用ninja而非make进行构建</p>
<h3 id="编译TVM">编译TVM</h3>
<p>首先在 <a target="_blank" rel="noopener" href="https://winlibs.com/">https://winlibs.com/</a> 拿到带 LLVM 库的 GCC 包，安装并加入环境变量，在cmd中可以使用 <code>llvm-config --libdir</code> 验证</p>
<p>创建build目录并<code>cp cmake/config.cmake build</code>并自定义配置，把USE LLVM打开</p>
<p>Conda创建tvm的虚拟环境，可以直接 <code>conda env create --file conda/build-environment.yaml</code> 但是使用该环境 git 会莫名奇妙出bug，不如手动创环境下载该文件中提到的依赖包</p>
<p>安装visual studio，把桌面C++开发组件勾上，安装完成后在cmd中使用<code>cl</code>验证（有没有可能除了装一个vs上述步骤全部不需要，但是不管了，官方文档安装部分相当混乱）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> build<br><span class="hljs-built_in">cd</span> build<br>cmake -A x64 -Thost=x64 ..<br><span class="hljs-built_in">cd</span> ..<br>cmake --build build --config Release -- /m<br></code></pre></td></tr></table></figure>
<h3 id="安装python包">安装python包</h3>
<p>在import tvm时如果未找到包，vscode会自动在工作区下创建配置帮你把tvm的路径加到 <code>python.analysis.extraPaths</code> 但实测虽然变绿且鼠标可以左键跳转了，但解释器还是找不到，可能需要改全局的python路径啥的</p>
<p>不如直接安装，环境变量引入包的好处在于源码更改后，引入可以立即感知；而install的包在每次源码更改后要重新install才能生效。但是我使用的v0.8的源码，已经没有更新了，所以直接安装也ok</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> python<br>python setup.py install<br></code></pre></td></tr></table></figure>
<h2 id="用户手册">用户手册</h2>
<p>tvm/gallery</p>
<h3 id="introduction-py"><a target="_blank" rel="noopener" href="http://introduction.py">introduction.py</a></h3>
<p><img src="image.png" srcset="/img/loading.gif" lazyload alt="Alt text"></p>
<p>TVM编译步骤：</p>
<ol>
<li>
<p>从 TensorFlow、PyTorch 或 ONNX 等框架导入模型。在导入阶段中，TVM 可以从其他框架（如 TensorFlow、PyTorch 或 ONNX）中提取模型。 TVM 为前端提供的支持水平会随着我们不断改进这个开源项目而变化。如果在将模型导入 TVM 时遇到问题，可以将其转换为 ONNX。</p>
</li>
<li>
<p>翻译成 TVM 的高级模型语言 Relay。已导入 TVM 的模型在 Relay 中表示。Relay 是神经网络的功能语言和中间表示（IR）。Relay 应用图级优化 pass 来优化模型。</p>
</li>
<li>
<p>降级为张量表达式（TE）表示。降级是指将较高级的表示转换为较低级的表示。应用了高级优化之后，Relay 通过运行 FuseOps pass，把模型划分为许多小的子图，并将子图降级为 TE 表示。张量表达式（TE）是一种用于描述张量计算的领域特定语言。 TE 还提供了几个 schedule 原语来指定底层循环优化，例如循环切分、矢量化、并行化、循环展开和融合。为将 Relay 表示转换为 TE 表示，TVM 包含了一个张量算子清单（TOPI），其中包含常用张量算子的预定义模板（例如，conv2d、transpose）。</p>
</li>
<li>
<p>使用 auto-tuning 模块 AutoTVM 或 AutoScheduler 搜索最佳 schedule。schedule 为 TE 中定义的算子或子图指定底层循环优化。auto-tuning 模块搜索最佳 schedule，并将其与 cost model 和设备上的测量值进行比较。TVM 中有两个 auto-tuning 模块，AutoTVM（有模板）和Ansor（无模板）。</p>
</li>
<li>
<p>为模型编译选择最佳配置。调优后，auto-tuning 模块会生成 JSON 格式的调优记录。此步骤为每个子图选择最佳 schedule。</p>
</li>
<li>
<p>降级为张量中间表示（TIR，TVM 的底层中间表示）。基于调优步骤选择最佳配置后，所有 TE 子图降级为 TIR 并通过底层优化 pass 进行优化。接下来，优化的 TIR 降级为硬件平台的目标编译器。这是生成可部署到生产的优化模型的最终代码生成阶段。</p>
</li>
<li>
<p>编译成机器码。compiler-specific 的生成代码最终可降级为机器码。 TVM 可将模型编译为可链接对象模块，然后轻量级 TVM runtime 可以用 C 语言的 API 来动态加载模型，也可以为 Python 和 Rust 等其他语言提供入口点。或将 runtime 和模型放在同一个 package 里时，TVM 可以对其构建捆绑部署。</p>
</li>
</ol>
<h3 id="autotvm-relay-x86-py">autotvm_relay_x86.py</h3>
<p>使用TVM的Python API编译、训练和调优与训练的模型。</p>
<h4 id="导入依赖，并下载和加载ONNX模型">导入依赖，并下载和加载ONNX模型</h4>
<p>使用ResNet-50 v2，一个50层的用于图像分类的卷积神经网络，可以使用<a target="_blank" rel="noopener" href="https://netron.app/">Netron</a>检查模型结构。输入图像为224×224，模型已经预训练好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> onnx<br><span class="hljs-keyword">from</span> tvm.contrib.download <span class="hljs-keyword">import</span> download_testdata<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tvm.relay <span class="hljs-keyword">as</span> relay<br><span class="hljs-keyword">import</span> tvm<br><span class="hljs-keyword">from</span> tvm.contrib <span class="hljs-keyword">import</span> graph_executor<br><br>model_url = (<br>    <span class="hljs-string">&quot;https://github.com/onnx/models/raw/main/&quot;</span><br>    <span class="hljs-string">&quot;vision/classification/resnet/model/&quot;</span><br>    <span class="hljs-string">&quot;resnet50-v2-7.onnx&quot;</span><br>)<br><br>model_path = download_testdata(model_url, <span class="hljs-string">&quot;resnet50-v2-7.onnx&quot;</span>, module=<span class="hljs-string">&quot;onnx&quot;</span>)<br>onnx_model = onnx.load(model_path)<br><br><span class="hljs-comment"># 为 numpy 的 RNG 设置 seed，保证每次复现得到一致的结果</span><br>np.random.seed(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<h4 id="下载和预处理图像">下载和预处理图像</h4>
<p>将图像转为Numpy数组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">img_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&quot;</span><br>img_path = download_testdata(img_url, <span class="hljs-string">&quot;imagenet_cat.png&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-comment"># 重设大小为 224x224</span><br>resized_image = Image.<span class="hljs-built_in">open</span>(img_path).resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br>img_data = np.asarray(resized_image).astype(<span class="hljs-string">&quot;float32&quot;</span>)<br><br><span class="hljs-comment"># 输入图像是 HWC 布局，而 ONNX 需要 CHW 输入，所以转换数组</span><br>img_data = np.transpose(img_data, (<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br><br><span class="hljs-comment"># 根据 ImageNet 输入规范进行归一化</span><br>imagenet_mean = np.array([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>]).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>imagenet_stddev = np.array([<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>norm_img_data = (img_data / <span class="hljs-number">255</span> - imagenet_mean) / imagenet_stddev<br><br><span class="hljs-comment"># 添加 batch 维度，期望 4 维输入：NCHW。</span><br><span class="hljs-comment"># N:number of samples，即batch size C:channels H:height W:width</span><br>img_data = np.expand_dims(norm_img_data, axis=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<h4 id="使用Relay编译模型">使用Relay编译模型</h4>
<p>将模型转为Relay中间表示（IR）。首先用 from_onnx 导入器将模型导入到 Relay 中。然后，用标准优化，将模型构建到 TVM 库中，最后从库中创建一个 TVM 计算图 runtime 模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入名称可能因模型类型而异</span><br><span class="hljs-comment"># 可用 Netron 工具检查输入名称</span><br>input_name = <span class="hljs-string">&quot;data&quot;</span><br>target = <span class="hljs-string">&quot;llvm&quot;</span><br><br><span class="hljs-comment"># 定义输入的形状字典，键是模型固定的输入名称，值是输入形状，用于指定ONNX模型的输入形状</span><br>shape_dict = &#123;input_name: img_data.shape&#125;<br><br><span class="hljs-comment"># 将ONNX模型转换为Relay中间表示（IR）</span><br>mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)<br><br><span class="hljs-comment"># 构建和编译模型</span><br><span class="hljs-keyword">with</span> tvm.transform.PassContext(opt_level=<span class="hljs-number">3</span>):<br>    <span class="hljs-comment"># 使用Relay构建模型并编译为目标为&quot;llvm&quot;的库</span><br>    lib = relay.build(mod, target=target, params=params)<br><br><span class="hljs-comment"># 创建&quot;llvm&quot;设备</span><br>dev = tvm.device(target, <span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 创建GraphModule并加载编译好的模块</span><br>module = graph_executor.GraphModule(lib[<span class="hljs-string">&quot;default&quot;</span>](dev))<br></code></pre></td></tr></table></figure>
<p><code>relay.frontend.from_onnx</code>接受一个ONNX模型和一个形状字典作为输入，返回一个Relay模块（mod）和一个参数字典（params）</p>
<p>mod是整个模型的计算图，params是模型的参数字典，包含了模型的权重和偏置等参数</p>
<p><code>tvm.transform.PassContext</code>是TVM控制优化过程的上下文，<code>opt_level=3</code>代表启用所有推荐优化<br>
lib是编译后生成的模块库，包含模型计算图、参数和编译后的函数，它将可以被加载到一个GraphModule中，并在指定设备上执行</p>
<p><code>dev = tvm.device(target, 0)</code>和<code>module = graph_executor.GraphModule(lib[&quot;default&quot;](dev))</code> 分别创建了一个device对象和一个GraphModule对象（图执行模块）<br>
可以在上面运行上面编译好的lib库</p>
<h4 id="使用模型进行推理">使用模型进行推理</h4>
<p>直接运行模型进行预测，并将输出转为可读形式。并收集此时还未优化过的模型基本性能数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 运行模型</span><br>dtype = <span class="hljs-string">&quot;float32&quot;</span><br>module.set_input(input_name, img_data)<br>module.run()<br>output_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br>tvm_output = module.get_output(<span class="hljs-number">0</span>, tvm.nd.empty(output_shape)).numpy()<br><br><span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> softmax<br><br><span class="hljs-comment"># 下载标签列表</span><br>labels_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;</span><br>labels_path = download_testdata(labels_url, <span class="hljs-string">&quot;synset.txt&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(labels_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    labels = [l.rstrip() <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> f]<br><br><span class="hljs-comment"># 打开输出文件并读取输出张量</span><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br><br><span class="hljs-comment"># 运行时间评估</span><br><span class="hljs-keyword">import</span> timeit<br>timing_number = <span class="hljs-number">10</span><br>timing_repeat = <span class="hljs-number">10</span><br>unoptimized = (<br>    np.array(timeit.Timer(<span class="hljs-keyword">lambda</span>: module.run()).repeat(repeat=timing_repeat, number=timing_number))<br>    * <span class="hljs-number">1000</span><br>    / timing_number<br>)<br>unoptimized = &#123;<br>    <span class="hljs-string">&quot;mean&quot;</span>: np.mean(unoptimized),<br>    <span class="hljs-string">&quot;median&quot;</span>: np.median(unoptimized),<br>    <span class="hljs-string">&quot;std&quot;</span>: np.std(unoptimized),<br>&#125;<br></code></pre></td></tr></table></figure>
<h4 id="输出后处理">输出后处理</h4>
<p>用专为该模型提供的查找表，运行一些后处理（post-processing），从而使得 ResNet-50 v2 的输出形式更具有可读性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> softmax<br><br><span class="hljs-comment"># 下载标签列表</span><br>labels_url = <span class="hljs-string">&quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;</span><br>labels_path = download_testdata(labels_url, <span class="hljs-string">&quot;synset.txt&quot;</span>, module=<span class="hljs-string">&quot;data&quot;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(labels_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    labels = [l.rstrip() <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> f]<br><br><span class="hljs-comment"># 打开输出文件并读取输出张量</span><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br></code></pre></td></tr></table></figure>
<h4 id="进行模型调优">进行模型调优</h4>
<p>用编译的模块推理，有时可能无法获得预期的性能。在这种情况下，可用自动调优器更好地配置模型，从而提高性能。 TVM 中的调优是指，在给定 target 上优化模型，使其运行得更快。与训练或微调不同，它不会影响模型的准确性，而只会影响 runtime 性能。作为调优过程的一部分，TVM 实现并运行许多不同算子的变体，以查看哪个性能最佳。这些运行的结果存储在调优记录文件中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tvm.auto_scheduler <span class="hljs-keyword">as</span> auto_scheduler<br><span class="hljs-keyword">from</span> tvm.autotvm.tuner <span class="hljs-keyword">import</span> XGBTuner<br><span class="hljs-keyword">from</span> tvm <span class="hljs-keyword">import</span> autotvm<br><br>number = <span class="hljs-number">10</span><br>repeat = <span class="hljs-number">1</span><br>min_repeat_ms = <span class="hljs-number">0</span>  <span class="hljs-comment"># 调优 CPU 时设置为 0</span><br>timeout = <span class="hljs-number">10</span>  <span class="hljs-comment"># 秒</span><br><br><span class="hljs-comment"># 创建 TVM 运行器</span><br>runner = autotvm.LocalRunner(<br>    <span class="hljs-comment"># 将要测试的不同配置的数量</span><br>    number=number,<br>    <span class="hljs-comment"># 每个配置重复次数</span><br>    repeat=repeat,<br>    <span class="hljs-comment"># 每次测试运行时间上限</span><br>    timeout=timeout,<br>    <span class="hljs-comment"># 指定运行配置测试需要多长时间，如果重复次数低于此时间，则增加其值</span><br>    min_repeat_ms=min_repeat_ms,<br>    enable_cpu_cache_flush=<span class="hljs-literal">True</span>,<br>)<br><br>tuning_option = &#123;<br>    <span class="hljs-string">&quot;tuner&quot;</span>: <span class="hljs-string">&quot;xgb&quot;</span>,<br>    <span class="hljs-comment"># 试验次数，CPU上推荐1500，GPU推荐3000-4000，此处仅作展示用</span><br>    <span class="hljs-string">&quot;trials&quot;</span>: <span class="hljs-number">10</span>,<br>    <span class="hljs-comment"># 使搜索提前停止的实验最小值</span><br>    <span class="hljs-string">&quot;early_stopping&quot;</span>: <span class="hljs-number">100</span>,<br><br>    <span class="hljs-string">&quot;measure_option&quot;</span>: autotvm.measure_option(<br>        builder=autotvm.LocalBuilder(build_func=<span class="hljs-string">&quot;default&quot;</span>),<br>        runner=runner<br>    ),<br>    <span class="hljs-comment"># 调优数据保存的文件名</span><br>    <span class="hljs-string">&quot;tuning_records&quot;</span>: <span class="hljs-string">&quot;resnet-50-v2-autotuning.json&quot;</span>,<br>&#125;<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先从 onnx 模型中提取任务</span><br>tasks = autotvm.task.extract_from_program(mod[<span class="hljs-string">&quot;main&quot;</span>], target=target, params=params)<br><br><span class="hljs-keyword">for</span> i, task <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tasks):<br>    prefix = <span class="hljs-string">&quot;[Task %2d/%2d] &quot;</span> % (i + <span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(tasks))<br>    tuner_obj = XGBTuner(task, loss_type=<span class="hljs-string">&quot;rank&quot;</span>)<br>    tuner_obj.tune(<br>        n_trial=<span class="hljs-built_in">min</span>(tuning_option[<span class="hljs-string">&quot;trials&quot;</span>], <span class="hljs-built_in">len</span>(task.config_space)),<br>        early_stopping=tuning_option[<span class="hljs-string">&quot;early_stopping&quot;</span>],<br>        measure_option=tuning_option[<span class="hljs-string">&quot;measure_option&quot;</span>],<br>        callbacks=[<br>            autotvm.callback.progress_bar(tuning_option[<span class="hljs-string">&quot;trials&quot;</span>], prefix=prefix),<br>            autotvm.callback.log_to_file(tuning_option[<span class="hljs-string">&quot;tuning_records&quot;</span>]),<br>        ],<br>    )<br></code></pre></td></tr></table></figure>
<h4 id="使用生成的调优数据优化编译模型">使用生成的调优数据优化编译模型</h4>
<p>获取上述存储在<code>resnet-50-v2-autotuning.json</code> 中的调优记录，并使用该结果为指定target上的模型生成高性能代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> autotvm.apply_history_best(tuning_option[<span class="hljs-string">&quot;tuning_records&quot;</span>]):<br>    <span class="hljs-keyword">with</span> tvm.transform.PassContext(opt_level=<span class="hljs-number">3</span>, config=&#123;&#125;):<br>        lib = relay.build(mod, target=target, params=params)<br><br>dev = tvm.device(<span class="hljs-built_in">str</span>(target), <span class="hljs-number">0</span>)<br>module = graph_executor.GraphModule(lib[<span class="hljs-string">&quot;default&quot;</span>](dev))<br><br><span class="hljs-comment"># 验证模型是否产生相同的结果</span><br>dtype = <span class="hljs-string">&quot;float32&quot;</span><br>module.set_input(input_name, img_data)<br>module.run()<br>output_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br>tvm_output = module.get_output(<span class="hljs-number">0</span>, tvm.nd.empty(output_shape)).numpy()<br><br>scores = softmax(tvm_output)<br>scores = np.squeeze(scores)<br>ranks = np.argsort(scores)[::-<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> ranks[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class=&#x27;%s&#x27; with probability=%f&quot;</span> % (labels[rank], scores[rank]))<br></code></pre></td></tr></table></figure>
<h4 id="比较调优前后的模型性能">比较调优前后的模型性能</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> timeit<br><br>timing_number = <span class="hljs-number">10</span><br>timing_repeat = <span class="hljs-number">10</span><br>optimized = (<br>    np.array(timeit.Timer(<span class="hljs-keyword">lambda</span>: module.run()).repeat(repeat=timing_repeat, number=timing_number))<br>    * <span class="hljs-number">1000</span><br>    / timing_number<br>)<br>optimized = &#123;<span class="hljs-string">&quot;mean&quot;</span>: np.mean(optimized), <span class="hljs-string">&quot;median&quot;</span>: np.median(optimized), <span class="hljs-string">&quot;std&quot;</span>: np.std(optimized)&#125;<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;optimized: %s&quot;</span> % (optimized))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;unoptimized: %s&quot;</span> % (unoptimized))<br></code></pre></td></tr></table></figure>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/work/" class="category-chain-item">work</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/TVM/" class="print-no-link">#TVM</a>
      
        <a href="/tags/%E7%BC%96%E8%AF%91/" class="print-no-link">#编译</a>
      
        <a href="/tags/cmake/" class="print-no-link">#cmake</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>TVM源码-00</div>
      <div>http://example.com/2023/11/16/TVM源码-00/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Zhang Yix</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年11月16日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2024年12月22日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                    <i class="iconfont icon-nc"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <span class="hint--top hint--rounded" aria-label="ND - 禁止演绎">
                    <i class="iconfont icon-nd"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/12/31/Git-cheat-sheet/" title="Git cheat sheet">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Git cheat sheet</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/10/31/The-Deep-Learning-Compiler-A-Comprehensive-Survey/" title="The Deep Learning Compiler: A Comprehensive Survey">
                        <span class="hidden-mobile">The Deep Learning Compiler: A Comprehensive Survey</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"8xk2t3uTLFeKLsI2P2znQa5G-gzGzoHsz","appKey":"ECVg3w6koW7bZ0d5L1S0H1pH","path":"window.location.pathname","placeholder":"随便讲两句...","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":true},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> &amp;&amp; <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <i class="iconfont icon-love"></i> <a href="https://www.bilibili.com/video/BV1r54y1B7sT" target="_blank" rel="nofollow noopener"><span>RuiXin</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="/page.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
