

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.png">
  <link rel="icon" href="/img/logo.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhang Yix">
  <meta name="keywords" content="">
  
    <meta name="description" content="陈天奇的MLC课程，参考 TVM学习仓库 MLC官方课程文档 MLC课程视频">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Compilation">
<meta property="og:url" content="http://example.com/2023/09/21/Machine-Learning-Compilation/index.html">
<meta property="og:site_name" content="Zhyx&#39;s Blog">
<meta property="og:description" content="陈天奇的MLC课程，参考 TVM学习仓库 MLC官方课程文档 MLC课程视频">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/09/21/Machine-Learning-Compilation/image.png">
<meta property="og:image" content="http://example.com/2023/09/21/Machine-Learning-Compilation/image-1.png">
<meta property="article:published_time" content="2023-09-21T16:30:11.000Z">
<meta property="article:modified_time" content="2024-04-01T14:55:51.841Z">
<meta property="article:author" content="Zhang Yix">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="TVM">
<meta property="article:tag" content="课程笔记">
<meta property="article:tag" content="MLC">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2023/09/21/Machine-Learning-Compilation/image.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Machine Learning Compilation - Zhyx&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 40vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Zhyx&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/index.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.2)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Machine Learning Compilation"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-09-21 16:30" pubdate>
          2023年9月21日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          12k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          99 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Machine Learning Compilation</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2024年4月1日 下午
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p>陈天奇的MLC课程，参考</p>
<p><a target="_blank" rel="noopener" href="https://github.com/BBuf/tvm_mlir_learn">TVM学习仓库</a></p>
<p><a target="_blank" rel="noopener" href="https://mlc.ai/zh/index.html">MLC官方课程文档</a></p>
<p><a target="_blank" rel="noopener" href="https://space.bilibili.com/1663273796/channel/collectiondetail?sid=499979">MLC课程视频</a></p>
<span id="more"></span>
<h2 id="1-概述">1.概述</h2>
<p>定义：将机器学习的算法（模型）从开发形式（如pytorch、tf等通用框架编写的模型描述以及相关权重），通过变换和优化，转化为部署形式（如模型支撑代码、内存控制、接口等）<br>
即，将神经网络模型转变成在特定硬件上运行的张量函数代码</p>
<p>机器学习编译目标：</p>
<ol>
<li>集成和最小化依赖</li>
<li>利用硬件加速：利用到每个部署环境的原生加速技术</li>
<li>通用优化</li>
</ol>
<h2 id="2-张量程序抽象">2. 张量程序抽象</h2>
<p>元张量函数：机器学习模型执行中的每一个步骤（或者说算子？），如linear、relu、softmax</p>
<p>许多不同的抽象可以表达同一种元张量函数，如torch.add和numpy.add，同时，有些机器学习框架也提供模型的编译过程优化，将元张量函数转变成更专门的、针对性的函数</p>
<p>张量程序抽象：一个典型的元张量函数实现包括：</p>
<ol>
<li>存储数据的多维数组</li>
<li>驱动张量计算的循环嵌套</li>
<li>计算语句</li>
</ol>
<p>根据抽象出来的共同特征，元张量函数因此可以被一系列有效的程序变换所改变，即优化。<br>
一般情况下，我们感兴趣的大部分元张量函数都具有良好的可变换属性。</p>
<h3 id="TensorIR：TVM使用的张量程序抽象">TensorIR：TVM使用的张量程序抽象</h3>
<p>前提：大多数的机器学习编译可以视为张量函数之间的变换</p>
<h4 id="示例：一个经典的点积-relu-网络">示例：一个经典的点积 + relu 网络</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dtype = <span class="hljs-string">&quot;float32&quot;</span><br>a_np = np.random.rand(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>).astype(dtype)<br>b_np = np.random.rand(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>).astype(dtype)<br>c_mm_relu = np.maximum(a_np @ b_np, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<p>在底层，numpy可能使用循环和算术运算实现上述操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mm_relu</span>(<span class="hljs-params">A: np.ndarray, B: np.ndarray, C: np.ndarray</span>):<br>    <span class="hljs-comment"># 存储数据的多维数组</span><br>    Y = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                <span class="hljs-comment"># 计算语句</span><br>                Y[i, j] = Y[i, j] + A[i, k] * B[k, j]<br>    <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            C[i, j] = <span class="hljs-built_in">max</span>(Y[i, j], <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<h4 id="TensorIR实现：">TensorIR实现：</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@tvm.script.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">                B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">                C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># 存储数据的多维数组（缓冲区）</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j)<br>                vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-comment"># 驱动张量计算的循环嵌套</span><br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-comment"># 计算语句</span><br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j)<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure>
<p>ir_module是TVM编译的最小完整单元，在TVM前端，其通常包括一个或多个relay（一个relay通常对应一个端到端模型），在经过如autoTVM、tirPasses之后relay被分解成一个或多个primFunc</p>
<p>块是tensorIR的基本计算单位。定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[block_axis] = T.axis.[axis_type]([axis_range], [mapped_value])<br></code></pre></td></tr></table></figure>
<p>如<code>vi = T.axis.spatial(128, i)</code> 即表示vi为i的映射，范围为(0,128)，且该块轴属性为spatial（空间轴），而vk的属性则为reduce规约轴。（可以理解为空间轴是原本就在的，规约轴是在上面做滑动的）</p>
<p>块轴加属性的好处是使得vi，vj，vk独立于外部的循环嵌套i，j，k，同时也对外部循环正确性做了二次验证。同时这些附加信息也有助于机器学习编译分析，比如说，我们总是可以在空间轴上做并行化，但在规约轴上做并行化则需要特定的策略</p>
<pre><code class="hljs">如果觉得自定义属性比较麻烦也可以一键绑定
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># SSR means the properties of each axes are &quot;spatial&quot;, &quot;spatial&quot;, &quot;reduce&quot;</span><br>vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br></code></pre></td></tr></table></figure>
<h4 id="tensorIR的元张量函数变换">tensorIR的元张量函数变换</h4>
<p>tensorIR引入了名为Schedule的辅助结构，允许我们进行方便的元张量函数变换</p>
<p>这是原来的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> IPython<br>IPython.display.Code(MyModule.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-comment"># from tvm.script import ir as I</span><br><span class="hljs-comment"># from tvm.script import tir as T</span><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br>                T.reads(A[vi, vk], B[vk, vj])<br>                T.writes(Y[vi, vj])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                T.reads(Y[vi, vj])<br>                T.writes(C[vi, vj])<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure>
<p>使用Schedule进行变换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 以给定的module作为输入的辅助Schedule类</span><br>sch = tvm.tir.Schedule(MyModule)<br><span class="hljs-comment"># 获取对应的块及相应循环的引用</span><br>block_Y = sch.get_block(<span class="hljs-string">&quot;Y&quot;</span>, func_name=<span class="hljs-string">&quot;mm_relu&quot;</span>)<br>i, j, k = sch.get_loops(block_Y)<br><span class="hljs-comment"># 变换：将原有的j循环拆分成两个循环（4表示内部循环长度）</span><br>j0, j1 = sch.split(j, factors=[<span class="hljs-literal">None</span>, <span class="hljs-number">4</span>])<br><span class="hljs-comment"># 再次检查结果</span><br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j_0, j_1, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">32</span>, <span class="hljs-number">4</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + j_1)<br>                vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                T.reads(A[vi, vk], B[vk, vj])<br>                T.writes(Y[vi, vj])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                T.reads(Y[vi, vj])<br>                T.writes(C[vi, vj])<br>                C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br><span class="hljs-comment"># 还可以更换循环次序</span><br><span class="hljs-comment"># sch.reorder(j0, k, j1)</span><br></code></pre></td></tr></table></figure>
<p>此外，块之间也可以通过变换完成组合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将块C放到Y的内循环中</span><br>block_C = sch.get_block(<span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;mm_relu&quot;</span>)<br><span class="hljs-comment"># 感觉意思是将块C与j0循环绑定，及j0这个空间轴变换时，原本只有Y有动作，现在C也有动作</span><br>sch.reverse_compute_at(block_C, j0)<br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-meta">@I.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Module</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mm_relu</span>(<span class="hljs-params">A: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), B: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>), C: T.Buffer(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">128</span>, <span class="hljs-number">128</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        T.func_attr(&#123;<span class="hljs-string">&quot;global_symbol&quot;</span>: <span class="hljs-string">&quot;mm_relu&quot;</span>, <span class="hljs-string">&quot;tir.noalias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>        <span class="hljs-comment"># with T.block(&quot;root&quot;):</span><br>        Y = T.alloc_buffer((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br>        <span class="hljs-keyword">for</span> i, j_0 <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">32</span>):<br>            <span class="hljs-keyword">for</span> k, j_1 <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">128</span>, <span class="hljs-number">4</span>):<br>                <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                    vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                    vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + j_1)<br>                    vk = T.axis.reduce(<span class="hljs-number">128</span>, k)<br>                    T.reads(A[vi, vk], B[vk, vj])<br>                    T.writes(Y[vi, vj])<br>                    <span class="hljs-keyword">with</span> T.init():<br>                        Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]<br>            <span class="hljs-keyword">for</span> ax0 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;C&quot;</span>):<br>                    vi = T.axis.spatial(<span class="hljs-number">128</span>, i)<br>                    <span class="hljs-comment"># 注意这里vj的变化，原本vj = j = j_0 * 4 + j_1，现在变成了j_0 * 4 + ax0</span><br>                    <span class="hljs-comment"># 感觉是因为上面 reverse_compute_at 只是将C与j0绑定，所以j_1这个循环还是在Y中，C里还需要单独循环ax0</span><br>                    vj = T.axis.spatial(<span class="hljs-number">128</span>, j_0 * <span class="hljs-number">4</span> + ax0)<br>                    T.reads(Y[vi, vj])<br>                    T.writes(C[vi, vj])<br>                    C[vi, vj] = T.<span class="hljs-built_in">max</span>(Y[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br></code></pre></td></tr></table></figure>
<p>此外还介绍了另一种原语decompose_reduction，用于将语块中元素的初始化与规约更新分开：<br>
这也是 TVM 在以后编译的时候隐式做的，所以这一步的主要目的是让它显式，看看最终效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将块Y中的初始化与循环k无关(k是规约轴)</span><br>sch.decompose_reduction(block_Y, k)<br>IPython.display.Code(sch.mod.script(), language=<span class="hljs-string">&quot;python&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mm_relu_v3</span>(<span class="hljs-params">A: np.ndarray, B: np.ndarray, C: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>        <span class="hljs-keyword">for</span> j0 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>):<br>            <span class="hljs-comment"># Y_init</span><br>            <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                j = j0 * <span class="hljs-number">4</span> + j1<br>                <span class="hljs-comment"># 此时初始化在k循环之前就已经做好</span><br>                Y[i, j] = <span class="hljs-number">0</span><br>            <span class="hljs-comment"># Y_update</span><br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                    j = j0 * <span class="hljs-number">4</span> + j1<br>                    Y[i, j] = Y[i, j] + A[i, k] * B[k, j]<br>            <span class="hljs-comment"># C</span><br>            <span class="hljs-keyword">for</span> j1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                j = j0 * <span class="hljs-number">4</span> + j1<br>                C[i, j] = <span class="hljs-built_in">max</span>(Y[i, j], <span class="hljs-number">0</span>)<br><br>c_np = np.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=dtype)<br>lnumpy_mm_relu_v3(a_np, b_np, c_np)<br>np.testing.assert_allclose(c_mm_relu, c_np, rtol=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure>
<h4 id="构建与运行">构建与运行</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用llvm将模型编译到本机平台</span><br>rt_lib = tvm.build(MyModule, target=<span class="hljs-string">&quot;llvm&quot;</span>)<br><br><span class="hljs-comment"># 用于存储输入和输出的TVM NDArray</span><br>a_nd = tvm.nd.array(a_np)<br>b_nd = tvm.nd.array(b_np)<br>c_nd = tvm.nd.empty((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br><br><span class="hljs-comment"># 调用编译好的函数</span><br>func_mm_relu = rt_lib[<span class="hljs-string">&quot;mm_relu&quot;</span>]<br>func_mm_relu(a_nd, b_nd, c_nd)<br><span class="hljs-comment"># 将TVM与numpy的结果进行比较</span><br>np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=<span class="hljs-number">1e-5</span>)<br><br><span class="hljs-comment"># 调用TVM变换后的函数，继续比较</span><br>rt_lib_after = tvm.build(sch.mod, target=<span class="hljs-string">&quot;llvm&quot;</span>)<br>rt_lib_after[<span class="hljs-string">&quot;mm_relu&quot;</span>](a_nd, b_nd, c_nd)<br>np.testing.assert_allclose(c_mm_relu, c_nd.numpy(), rtol=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure>
<p>在最后的结果中，TVM变换后的函数运行时间相比原先的TVM函数大幅缩短，为什么不同的循环变体会导致不同的时间性能呢？</p>
<p>关键在于CPU的访存策略，由于局部性原理，CPU在读取内存某元素时会尝试将该元素附近的元素一起获取到缓存中（cache块？特么OS快忘干净了😅）。因此具有连续内存访问的代码通常比随机访问内存不同部分的代码更快。</p>
<h2 id="3-端到端的模型执行">3. 端到端的模型执行</h2>
<p>现在考虑一个基础的两层神经网络，由2个MLP和1个relu组成（简化问题，删除最后的softmax）</p>
<p>numpy实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numpy_mlp</span>(<span class="hljs-params">data, w0, b0, w1, b1</span>):<br>    lv0 = data @ w0.T + b0<br>    lv1 = np.maximum(lv0, <span class="hljs-number">0</span>)<br>    lv2 = lv1 @ w1.T + b1<br>    <span class="hljs-keyword">return</span> lv2<br><br>res = numpy_mlp(img.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">784</span>),<br>                mlp_params[<span class="hljs-string">&quot;w0&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;b0&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;w1&quot;</span>],<br>                mlp_params[<span class="hljs-string">&quot;b1&quot;</span>])<br></code></pre></td></tr></table></figure>
<p>底层实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_linear0</span>(<span class="hljs-params">X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">784</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            Z[i, j] = Y[i, j] + B[j]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_relu0</span>(<span class="hljs-params">X: np.ndarray, Y: np.ndarray</span>):<br>     <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>            Y[i, j] = np.maximum(X[i, j], <span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_linear1</span>(<span class="hljs-params">X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray</span>):<br>    Y = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>):<br>                <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>                    Y[i, j] = <span class="hljs-number">0</span><br>                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>            Z[i, j] = Y[i, j] + B[j]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lnumpy_mlp</span>(<span class="hljs-params">data, w0, b0, w1, b1</span>):<br>    lv0 = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_linear0(data, w0, b0, lv0)<br><br>    lv1 = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_relu0(lv0, lv1)<br><br>    out = np.empty((<span class="hljs-number">1</span>, <span class="hljs-number">10</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>    lnumpy_linear1(lv1, w1, b1, out)<br>    <span class="hljs-keyword">return</span> out<br><br>result =lnumpy_mlp(<br>    img.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">784</span>),<br>    mlp_params[<span class="hljs-string">&quot;w0&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;b0&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;w1&quot;</span>],<br>    mlp_params[<span class="hljs-string">&quot;b1&quot;</span>])<br><br>pred_kind = result.argmax(axis=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Low-level Numpy MLP Prediction:&quot;</span>, class_names[pred_kind[<span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure>
<p>该模型的TVMScript实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@tvm.script.ir_module</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>:<br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">relu0</span>(<span class="hljs-params">x: T.handle, y: T.handle</span>):<br>        n = T.int64()<br>        X = T.match_buffer(x, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Y = T.match_buffer(y, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                Y[vi, vj] = T.<span class="hljs-built_in">max</span>(X[vi, vj], T.float32(<span class="hljs-number">0</span>))<br><br><span class="hljs-meta">    @T.prim_func</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">linear0</span>(<span class="hljs-params">x: T.handle,</span><br><span class="hljs-params">                w: T.handle,</span><br><span class="hljs-params">                b: T.handle,</span><br><span class="hljs-params">                z: T.handle</span>):<br>        m, n, k = T.int64(), T.int64(), T.int64()<br>        X = T.match_buffer(x, (<span class="hljs-number">1</span>, m), <span class="hljs-string">&quot;float32&quot;</span>)<br>        W = T.match_buffer(w, (n, m), <span class="hljs-string">&quot;float32&quot;</span>)<br>        B = T.match_buffer(b, (n, ), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Z = T.match_buffer(z, (<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        Y = T.alloc_buffer((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">for</span> i, j, k <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n, m):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Y&quot;</span>):<br>                vi, vj, vk = T.axis.remap(<span class="hljs-string">&quot;SSR&quot;</span>, [i, j, k])<br>                <span class="hljs-keyword">with</span> T.init():<br>                    Y[vi, vj] = T.float32(<span class="hljs-number">0</span>)<br>                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]<br>        <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> T.grid(<span class="hljs-number">1</span>, n):<br>            <span class="hljs-keyword">with</span> T.block(<span class="hljs-string">&quot;Z&quot;</span>):<br>                vi, vj = T.axis.remap(<span class="hljs-string">&quot;SS&quot;</span>, [i, j])<br>                Z[vi, vj] = Y[vi, vj] + B[vj]<br><br><span class="hljs-meta">    @R.function</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">x: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-number">1</span>, <span class="hljs-string">&quot;m&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             w0: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;m&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             b0: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;n&quot;</span>, </span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             w1: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;k&quot;</span>, <span class="hljs-string">&quot;n&quot;</span></span>), <span class="hljs-string">&quot;float32&quot;</span></span>),</span><br><span class="hljs-params">             b1: R.Tensor(<span class="hljs-params">(<span class="hljs-params"><span class="hljs-string">&quot;k&quot;</span>, </span>), <span class="hljs-string">&quot;float32&quot;</span></span>)</span>):<br>        m, n, k = T.int64(), T.int64(), T.int64()<br>        <span class="hljs-keyword">with</span> R.dataflow():<br>            lv0 = R.call_dps_packed(<span class="hljs-string">&quot;linear0&quot;</span>, (x, w0, b0), R.Tensor((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>))<br>            lv1 = R.call_dps_packed(<span class="hljs-string">&quot;relu0&quot;</span>, (lv0, ), R.Tensor((<span class="hljs-number">1</span>, n), <span class="hljs-string">&quot;float32&quot;</span>))<br>            out = R.call_dps_packed(<span class="hljs-string">&quot;linear0&quot;</span>, (lv1, w1, b1), R.Tensor((<span class="hljs-number">1</span>, k), <span class="hljs-string">&quot;float32&quot;</span>))<br>            R.output(out)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>
<p>引入了一个新的 <code>@R.function</code> 即Relex函数，是一种表示上层神经网络执行的全新抽象</p>
<p><img src="image.png" srcset="/img/loading.gif" lazyload alt="Alt text"></p>
<p>注意到，其中<code>call_dps_packed</code>将我们的元函数嵌入到计算图中，其主要作用是满足<strong>目标传递</strong>的调用约定，即 pure 或 side-effect free ，函数只从其输入中读取数据并输出返回结果，而不改变程序的其他部分，这可以方便我们隐藏调用底层元函数的细节</p>
<p>如果只是像numpy实现中那样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lnumpy_linear0(data, w0, b0, lv0)<br>lnumpy_relu0(lv0, lv1)<br>lnumpy_linear1(lv1, w1, b1, out)<br></code></pre></td></tr></table></figure>
<p>计算图可能会变成这样：lv0既是<code>lnumpy_linear0</code>的入参，也是<code>lnumpy_relu0</code>的入参，其余同理<br>
<img src="image-1.png" srcset="/img/loading.gif" lazyload alt="Alt text"></p>
<blockquote>
<p>计算图通常具有以下性质：</p>
<ul>
<li>框的每个输入边对应于操作的输入</li>
<li>每个出边对应于操作的输出</li>
<li>每个操作可以任意重新排序，直到边缘的拓扑顺序</li>
</ul>
</blockquote>
<p>当然，numpy的底层同样也使用了如<code>lnumpy_call_dps_packed</code>的类似调用</p>
<p>此外，注意<code>with R.dataflow():</code> 是一个帮助我们标注程序计算图范围的方式，后面的构建运行就不多说了</p>
<h2 id="4-自动程序优化">4. 自动程序优化</h2>
<p>这一章主要讲随机调度变换，当我们无法决定原张量函数优化的每一个细节时，可以使用机器的一些<strong>随机变换</strong>做法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">stochastic_schedule_mm</span>(<span class="hljs-params">sch: tvm.tir.Schedule</span>):<br>    block_C = sch.get_block(<span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;main&quot;</span>)<br>    i, j, k = sch.get_loops(block=block_C)<br>    <span class="hljs-comment"># 注意 j_factors 没有使用固定的[none,4]，而是采用随机值</span><br>    j_factors = sch.sample_perfect_tile(loop=j, n=<span class="hljs-number">2</span>)<br>    j_0, j_1 = sch.split(loop=j, factors=j_factors)<br>    sch.reorder(i, j_0, k, j_1)<br>    sch.decompose_reduction(block_C, k)<br>    <span class="hljs-keyword">return</span> sch<br></code></pre></td></tr></table></figure>
<p>上述代码中，用到了 sch.sample_perfect_tile 来随机拆分循环。它会将输入的循环的长度进行随机分割，例如原始j =128 时，就可以分割为 [8,16]、[32,4]、[2,64] 等等，可以发现，每次运行时该函数的采样都不一样</p>
<p>此外还讲了一些随机搜索的东西，大概类似超参数的网格搜索之类的，在TVM里叫<code>meta_schedule</code>，主要还做了以下事情：</p>
<ol>
<li>跨多个进程的并行基准测试</li>
<li>使用代价模型<code>cost model</code>进行代价评估，这样可以避免每组都进行基准测试</li>
<li>根据历史轨迹来进行遗传搜索，而不是每次都随机采样</li>
</ol>
<p>关键思想就是使用随机变换来指定好的程序的搜索空间，使用 <code>tune_tir</code> API 帮助在搜索空间内搜索并找到最优的调度变换</p>
<blockquote>
<p><strong>前面几章内容总结，就是为什么通过编译可以使模型运行更快（cache空间局部性），以及怎么样编译可以更快（元张量函数变换），同时也介绍了一些随机变换的方法（网格搜索），感觉随机变换的算法才是MLC性能的核心，也就是自动调优，TVM后面似乎用到了一些 autoTVM、autoSchedule 之类的方法进行 auto tune，这也是我需要重点关注的部分</strong></p>
</blockquote>
<h2 id="5-与机器学习框架的整合">5. 与机器学习框架的整合</h2>
<p>如何将机器学习模型从现有框架引入MLC，一些API的基础教程，参考 <a target="_blank" rel="noopener" href="https://mlc.ai/zh/chapter_integration/index.html">https://mlc.ai/zh/chapter_integration/index.html</a></p>
<h2 id="6-GPU硬件加速">6. GPU硬件加速</h2>
<p>在GPU环境下的MLC流程，第一部分主要讨论CUDA，第二部分讨论专门的GPU环境，后面再看吧</p>
<h2 id="7-计算图优化">7. 计算图优化</h2>
<p>提供了一些算子融合的基础代码，也不太想看</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/work/" class="category-chain-item">work</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/TVM/" class="print-no-link">#TVM</a>
      
        <a href="/tags/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" class="print-no-link">#课程笔记</a>
      
        <a href="/tags/MLC/" class="print-no-link">#MLC</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Machine Learning Compilation</div>
      <div>http://example.com/2023/09/21/Machine-Learning-Compilation/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Zhang Yix</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年9月21日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2024年4月1日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                    <i class="iconfont icon-nc"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <span class="hint--top hint--rounded" aria-label="ND - 禁止演绎">
                    <i class="iconfont icon-nd"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/09/25/%E5%BE%B7%E5%B7%9E%E6%89%91%E5%85%8B%E5%AD%A6%E4%B9%A0/" title="Texas Hold&#39;em">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Texas Hold&#39;em</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/09/16/%E7%BD%91%E7%BB%9C%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5/" title="网络状态检查">
                        <span class="hidden-mobile">网络状态检查</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a>
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="/page.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
